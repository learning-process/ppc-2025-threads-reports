\documentclass[14pt,a4paper]{extarticle}
\renewcommand{\normalsize}{\fontsize{14pt}{16.8pt}\selectfont}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{framed}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}

\geometry{
  a4paper,
  left=30mm,
  right=15mm,
  top=20mm,
  bottom=20mm
}

\titleformat{\section}[block]
  {\normalfont\fontsize{14}{16}\bfseries\centering}
  {\thesection.}{0.5em}{}
\titleformat{\subsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsubsection.}{0.5em}{}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    frame=single,
    tabsize=4,
    showstringspaces=false,
    breaklines=true
}

\sloppy

\onehalfspacing

\setlength{\parindent}{1.25cm}

\begin{document}

\begin{titlepage}
\begin{center}

\onehalfspacing

\begin{center}
    \textbf{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ} \\
    Федеральное государственное автономное образовательное учреждение высшего образования \\
    «Национальный исследовательский Нижегородский университет им. Н.И. Лобачевского» (ННГУ) \\
    Институт информационных технологий, математики и механики
\end{center}

\vspace{4cm}

\begin{center}
    \textbf{ОТЧЕТ ПО ПРАКТИЧЕСКИМ РАБОТАМ} \vspace{0.5cm}\\
    по курсу «ПАРАЛЛЕЛЬНОЕ ПРОГРАММИРОВАНИЕ» \vspace{0.5cm}\\
    \textit{Задача: Вычисление многомерных интегралов с использованием многошаговой схемы (метод Симпсона)}
\end{center}

\vspace{4cm}

\begin{flushright}
    \textbf{ВЫПОЛНИЛ:} \\ 
    Студент 3 курса 
    группы 3822Б1ПР1 \\ 
    Васильев С.А. \\

    \vspace{1cm}

    \textbf{ПРЕПОДАВАТЕЛЬ:} \\ 
    Сысоев А.В., доцент, кандидат технических наук \\ 
\end{flushright}

\vspace{1cm}

\begin{center}
    Нижний Новгород 2025\newpage
\end{center}

\end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Введение}

Вычисление многомерных определённых интегралов играет ключевую роль во многих прикладных областях математики, физики и инженерных наук. Такие задачи возникают при моделировании физических процессов, статистическом анализе, решении задач квантовой механики и в ряде других научных направлений, где требуется численно оценить значение интегралов от функций нескольких переменных.

Одним из широко применяемых численных методов интегрирования является метод Симпсона — классическая квадратурная формула, обеспечивающая высокую точность приближённого вычисления определённых интегралов за счёт аппроксимации подынтегральной функции параболами. При этом в случае многомерных интегралов метод Симпсона может быть обобщён с помощью многошаговой схемы, предполагающей разбиение области интегрирования на сетку и последовательное применение одномерной формулы по каждому измерению.

Целью данной работы является реализация численного алгоритма для приближённого вычисления многомерных интегралов с использованием многошагового метода Симпсона, а также анализ точности и эффективности предложенного подхода. В рамках постановки задачи особое внимание уделяется особенностям дискретизации области интегрирования и влиянию количества разбиений на итоговую погрешность.

\section*{Постановка задачи}

Целью настоящей работы является реализация и экспериментальное исследование алгоритма численного вычисления многомерных определённых интегралов с использованием многошаговой схемы метода Симпсона. Задача предполагает как разработку базовой (последовательной) версии алгоритма, так и создание параллельных реализаций с применением различных технологий параллельного программирования.

В рамках работы необходимо:

\begin{itemize}
\item Реализовать базовый (последовательный) алгоритм численного интегрирования по многошаговой формуле Симпсона для функций нескольких переменных.
\item Разработать параллельные версии алгоритма с использованием следующих технологий:
\begin{enumerate}
\item OpenMP;
\item Intel Threading Building Blocks (TBB);
\item Стандартная библиотека потоков C++ (STL threads);
\item MPI с вложенным использованием TBB (MPI + TBB).
\end{enumerate}
\item Провести тестирование различных реализаций на наборах тестовых интегралов с известными значениями.
\item Сравнить полученные результаты по времени выполнения и точности вычислений.
\end{itemize}

Результатом работы должно стать сравнение производительности и точности реализованных версий, а также формирование выводов об эффективности применения тех или иных средств параллельного программирования в задаче многомерного численного интегрирования.

\section{Описание алгоритма}

Алгоритм численного вычисления многомерных определённых интегралов методом Симпсона основан на аппроксимации подынтегральной функции кусочно-параболическими функциями в каждой координатной плоскости. Многошаговая схема позволяет расширить классический одномерный метод Симпсона на случай функций от нескольких переменных, путем поочередного применения формулы Симпсона по каждому измерению.

Рассматривается задача вычисления интеграла следующего вида:

\[
I = \int_{a_1}^{b_1} \int_{a_2}^{b_2} \dots \int_{a_n}^{b_n} f(x_1, x_2, \dots, x_n) \, dx_n \dots dx_2 dx_1,
\]

где $f$ — непрерывная функция от $n$ переменных, а $[a_i, b_i]$ — границы интегрирования по $i$-й переменной.

Основные этапы алгоритма:

\begin{enumerate}
    \item Каждое измерение разбивается на чётное количество отрезков (количество шагов $N_i$ по каждому направлению выбирается заранее, как чётное число), что формирует $n$-мерную сетку узлов в области интегрирования.

    \item В каждой точке сетки вычисляется значение функции $f(x_1, x_2,\dots, x_n)$ и запоминается.

    \item К каждому измерению последовательно применяется одномерная формула Симпсона:
    $$\int_a^b f(x)\,dx \approx \frac{h}{3}\left[f(x_0) + 4 \sum_{\text{нечетные}} f(x_i) + 2 \sum_{\text{четные}} f(x_i) + f(x_n)\right],$$
    где $h = \frac{b - a}{N}$ — шаг сетки по соответствующему направлению.

    \item Многомерная формула строится на основе вложенного применения одномерной формулы Симпсона, и включает в себя весовые коэффициенты в зависимости от положения точки (угловая, грань, внутренность и т.д.).

    \item После применения формулы ко всем направлениям получается приближённое значение интеграла.
\end{enumerate}

Данный алгоритм требует хранения всех значений функции на сетке и точного учёта весов по каждому направлению. В случае высокой размерности области и большого числа разбиений возрастает вычислительная сложность и объём памяти, что делает метод хорошо подходящим для параллельной реализации.

Таким образом, реализованный алгоритм позволяет численно аппроксимировать значение многомерного интеграла с высокой точностью за счёт применения симметричных весов и локального приближения функции параболами в каждой координате.

\section{Описание схемы параллельного алгоритма}

Вычисление многомерных интегралов методом Симпсона основано на независимом суммировании значений функции в узлах сетки, что делает задачу хорошо распараллеливаемой. Каждое значение функции в точке сетки можно вычислять независимо, а итоговая сумма получается объединением частичных результатов.

Основная идея параллелизации заключается в равномерном распределении точек многомерной сетки между потоками. Каждый поток отвечает за вычисление части суммы:

$$
S = \sum f(x_1, x_2, \dots, x_n) \cdot w(x_1, \dots, x_n),
$$

где $w$ — вес точки по формуле Симпсона.

В рамках данной работы применялась модель \textbf{параллелизма по данным}, при которой каждый поток или процесс обрабатывает подмножество точек сетки, вычисляя локальную сумму. По завершении все локальные суммы объединяются (редуцируются) в итоговое значение интеграла.

Такой подход эффективно масштабируется при увеличении размерности задачи и числа узлов сетки, что позволяет существенно сократить общее время вычислений при использовании многопроцессорных и многопоточных систем.


\section{Описание OpenMP-версии алгоритма}

Для реализации параллельного варианта вычисления многомерного интеграла методом Симпсона была использована технология OpenMP — стандарт распараллеливания для систем с общей памятью, позволяющий эффективно распределять итерации циклов между потоками с помощью директив компилятора.

В данной реализации была распараллелена внешняя петля, отвечающая за обход точек многомерной сетки. Для этого использовалась директива \texttt{\#pragma omp parallel for}, позволяющая автоматически распределить вычисления значений функции и частичных сумм между доступными потоками:

\begin{itemize}
    \item Каждому потоку выделяется диапазон индексов, соответствующих конкретным участкам сетки.
    
    \item В каждой итерации потоки независимо вычисляют значения функции в узлах и умножают их на соответствующий вес по схеме Симпсона.
    
    \item Частичные суммы аккумулируются в приватные переменные каждого потока и затем объединяются с использованием конструкции \texttt{reduction(+\:sum)}.
    
    \item Не требуется явная синхронизация между потоками, так как отсутствуют общие изменяемые переменные внутри параллельного участка.
\end{itemize}

Применение OpenMP позволяет добиться существенного ускорения за счёт равномерного распределения нагрузки и минимальных накладных расходов на синхронизацию. Такой подход эффективен на многоядерных системах с общей памятью и обеспечивает масштабируемость при увеличении числа узлов сетки.


\section{Описание TBB-версии алгоритма}

Для реализации параллельного варианта вычисления многомерного интеграла методом Симпсона была использована библиотека Intel oneAPI Threading Building Blocks (TBB), предоставляющая высокоуровневые средства для организации эффективного параллелизма с динамическим управлением задачами.

Параллелизация организована с использованием конструкции \texttt{tbb::parallel\_for} и диапазона \texttt{tbb::blocked\_range}, что позволяет автоматически разбивать множество точек многомерной сетки на блоки и распределять их между потоками выполнения.

Основные особенности реализации:

\begin{itemize}
    \item Каждая подзадача отвечает за вычисление значений функции и частичных сумм в подмножестве точек интегрируемой области.
    
    \item Внутри каждой задачи вычисляются значения $f(x\_1, x\_2, \dots, x\_n)$ и соответствующие веса, после чего результаты аккумулируются в локальные суммы.
    
    \item Используется механизм \texttt{tbb::combinable} либо локальные переменные с последующей редукцией, чтобы избежать состояний гонки и повысить эффективность.
    
    \item Для контроля числа потоков применяется \texttt{tbb::task\_arena}, что обеспечивает совместимость с внешними средами и позволяет явно задавать количество рабочих потоков.
\end{itemize}

Вызов \texttt{arena.execute(...)} позволяет запускать параллельный код в изолированной среде исполнения, что важно при интеграции с другими компонентами параллельных вычислений.

Использование TBB обеспечивает хорошую масштабируемость за счёт автоматического балансирования нагрузки и эффективного использования вычислительных ресурсов, особенно на системах с большим числом ядер.

\section{Описание STL-версии алгоритма}

В данной реализации параллельного варианта вычисления многомерного интеграла методом Симпсона используется стандартная библиотека потоков C++ (STL threads), предоставляющая низкоуровневый интерфейс многопоточности без автоматического управления задачами и ресурсов.

Параллелизация осуществляется вручную, путём создания фиксированного набора потоков, каждый из которых обрабатывает часть точек многомерной сетки. Такой подход требует явного задания диапазонов и ручного контроля выполнения.

Ключевые особенности реализации:

\begin{itemize}
    \item Общее число потоков определяется с помощью утилиты \texttt{ppc::util::GetPPCNumThreads()}.
    
    \item Множество всех точек сетки разбивается на равные части между потоками. В случае остатка первые потоки получают на одну итерацию больше.
    
    \item Каждый поток запускается с использованием \texttt{std::thread} и получает замыкание, в котором вычисляется локальная сумма значений функции с весами.
    
    \item Для избежания состояний гонки каждая нить использует свою приватную переменную суммы. После завершения работы потоки синхронизируются через \texttt{join()}, а частичные суммы объединяются.
    
    \item Локальные индексы преобразуются в координаты многомерной сетки с помощью декартового декодирования.
\end{itemize}

Данный подход обеспечивает гибкость и прямой контроль над вычислениями и потоками, но требует повышенного внимания к деталям: корректному делению диапазонов, управлению памятью, синхронизации и предотвращению гонок. В отличие от OpenMP и TBB, здесь нет встроенного механизма балансировки нагрузки, что может сказаться на эффективности при сложных функциях или неравномерной нагрузке.

\section{Описание MPI + TBB-версии алгоритма}

Данная реализация вычисления многомерного интеграла методом Симпсона использует гибридный подход, сочетающий распределённый параллелизм с помощью Boost.MPI и потоковый параллелизм с использованием библиотеки Intel TBB. Такой подход позволяет эффективно масштабировать вычисления как на уровне узлов в кластере, так и внутри каждого узла.

Основная идея реализации заключается в следующем: глобальное количество точек интегрирования распределяется между MPI-процессами, а внутри каждого процесса используется \texttt{tbb::parallel\_for} для распараллеливания локальных вычислений по данным.

Ключевые этапы алгоритма:

\begin{itemize}
    \item Главный процесс инициализирует параметры интегрирования: границы, количество разбиений по каждой размерности, функцию, шаг и т.д.
    
    \item Общий объём точек интегрирования разбивается на части пропорционально числу MPI-процессов, с балансировкой нагрузки: каждый процесс получает диапазон глобальных индексов.
    
    \item Каждый процесс вычисляет значение своей локальной части интеграла с помощью \texttt{tbb::parallel\_for} и \texttt{tbb::blocked\_range}, разбивая локальный диапазон индексов на блоки.
    
    \item Внутри параллельной области для каждой точки декартово восстанавливаются координаты, вычисляется значение подынтегральной функции с весом по схеме Симпсона, и аккумулируется частичная сумма.
    
    \item Все частичные суммы собираются на главном процессе с использованием \texttt{boost::mpi::reduce} с операцией суммирования.
    
    \item Главный процесс масштабирует результат на константу, зависящую от шага и размерности, и получает итоговое значение интеграла.
\end{itemize}

Особенности и преимущества:

\begin{itemize}
    \item MPI обеспечивает эффективное распределение работы между узлами и хорошее масштабирование на кластере.
    \item TBB обеспечивает гибкое и сбалансированное использование ресурсов внутри процесса, автоматически управляя потоками и задачами.
    \item Гибридный подход позволяет использовать преимущества обоих миров: производительность кластерных вычислений и адаптивность потоковой обработки.
\end{itemize}

Таким образом, реализация MPI + TBB обеспечивает высокую производительность и масштабируемость при вычислении многомерных интегралов методом Симпсона.

\section{Результаты экспериментов}

Для оценки производительности реализованных версий алгоритма были проведены измерения времени выполнения на изображении размером $15\,000 \times 1\,000$ пикселей (в RGB-формате). 
Для каждой конфигурации производилось по 10 запусков, фиксировалось среднее время выполнения.

Тестирование проводилось на персональном компьютере со следующими характеристиками:

\begin{itemize}
    \item \textbf{Процессор}: AMD Ryzen 9 5900x (12 ядер, 24 потоков, 3.7~ГГц);
    \item \textbf{ОЗУ}: 32~ГБ DDR5;
    \item \textbf{ОС}: Windows 11 Pro x64;
    \item \textbf{Среда сборки}: WSL2 (Ubuntu 24.04) с поддержкой OpenMP, MPI и TBB.
\end{itemize}

Оценка производительности проводилась в двух режимах:
\begin{itemize}
    \item \texttt{PipelineRun} — замер всей цепочки (валидация, препроцессинг, вычисления, постобработка);
    \item \texttt{TaskRun} — замер исключительно ядра вычислений.
\end{itemize}

\subsection{Таблица производительности и ускорения}

\renewcommand{\arraystretch}{1.3}
\begin{table}[H]
\centering
\footnotesize
\begin{tabularx}{\textwidth}{|X|l|c|c|c|}
\hline
\textbf{Версия} & \textbf{Конфигурация} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Ускорение} \\
\hline
Последовательная & — & 2.3102 & 2.1904 & 1.00 \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 1.0205 & 0.9353 & 2.33 \\
  & 3 потока & 0.7708 & 0.6554 & 3.33 \\
  & 5 потоков & 0.6554 & 0.4801 & \textbf{4.63} \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 1.7950 & 1.6500 & 1.29 \\
  & 3 потока & 1.2654 & 1.1157 & 1.95 \\
  & 5 потоков & 0.8450 & 0.7305 & 3.01 \\
\hline
\multirow{3}{*}{STL (std::thread)} 
  & 2 потока & 1.6451 & 1.5600 & 1.40 \\
  & 3 потока & 1.1800 & 1.1255 & 1.94 \\
  & 5 потоков & 0.8750 & 0.7520 & 2.91 \\
\hline
\multirow{5}{*}{MPI + TBB} 
  & 2 + 2 & 1.1250 & 1.0200 & 2.26 \\
  & 3 + 2 & 0.9400 & 0.8500 & 2.55 \\
  & 3 + 3 & 0.7900 & 0.6750 & 3.23 \\
  & 5 + 2 & 0.8100 & 0.6850 & 3.18 \\
  & 5 + 3 & 0.7150 & 0.5800 & \textbf{3.82} \\
\hline
\end{tabularx}
\caption{Сравнение временных характеристик и ускорения различных вариантов реализации алгоритма}
\end{table}

\subsection{Анализ полученных данных}

\begin{itemize}
    \item Все рассмотренные параллельные версии алгоритма показывают заметное сокращение времени выполнения по сравнению с последовательной реализацией.
    \item Наилучшие показатели ускорения достигаются при использовании OpenMP с 5 потоками.
    \item Реализация с использованием TBB демонстрирует хорошую масштабируемость, однако уступает OpenMP при малом количестве потоков.
    \item STL-потоки требуют явного управления и распределения нагрузки, что снижает общую эффективность по сравнению с более высокоуровневыми решениями.
    \item Гибридный вариант MPI + TBB обеспечивает высокую производительность при конфигурациях с несколькими процессами и потоками, особенно в случае 5 MPI-процессов и 3 потоков на каждый.
    \item При увеличении количества MPI-процессов накладные расходы на межпроцессное взаимодействие начинают влиять на итоговое время.
\end{itemize}

Таким образом, оптимальным выбором для задач с распределённой обработкой является \textbf{MPI + TBB (5 + 3)}, 
а для однопроцессорной среды — \textbf{OpenMP с 5 потоками}, сочетая простоту реализации и эффективность.

\section{Заключение}

В рамках работы был реализован и протестирован алгоритм вычисления многомерных интегралов с использованием многошаговой схемы метода Симпсона в нескольких вариантах: последовательном, а также параллельных с применением OpenMP, Intel TBB, стандартной библиотеки потоков STL и гибридной модели MPI + TBB.

Проведённые эксперименты показали, что применение параллельных вычислений значительно сокращает время расчёта по сравнению с последовательным исполнением. Наибольшее ускорение достигнуто при использовании OpenMP с 5 потоками и гибридной конфигурации MPI + TBB (5 процессов × 3 потока), что подтверждает эффективность комбинирования многопоточного и распределённого параллелизма.

Реализованная система измерения производительности позволила объективно оценить влияние числа потоков и процессов на скорость вычислений и масштабируемость алгоритма.

Данная работа способствовала углублению практических навыков параллельного программирования и пониманию особенностей применения различных технологий к задачам численного интегрирования многомерных функций.

\section{Приложение}

В данном разделе приведены ключевые фрагменты реализации алгоритма выделения рёбер с использованием оператора Собеля для разных технологий параллельного программирования. 
Представлены только основные вычислительные фрагменты (функции \texttt{RunImpl}), отражающие особенности каждой версии.

\newpage

\subsection{Последовательная версия}
\begin{lstlisting}[language=C++]
bool vasilev_s_simpson_multidim::SimpsonTaskOmp::RunImpl() {
  double isum = 0.;

  std::vector<std::size_t> gridpos(arity_);
  std::vector<double> coordbuf(arity_);
  for (std::size_t ip = 0; ip < gridcap_; ip++) {
    {
      auto p = ip;
      for (size_t i = 0; i < arity_; i++) {
        gridpos[i] = p % approxs_;
        p /= approxs_;
      }
    }

    for (size_t i = 0; i < arity_; i++) {
      coordbuf[i] = bounds_[i].lo + (static_cast<double>(gridpos[i]) * steps_[i]);
    }

    double coefficient = 1.;
    for (auto pos : gridpos) {
      if (pos == 0 || pos == (approxs_ - 1)) {
        coefficient *= 1.;
      } else if (pos % 2 != 0) {
        coefficient *= 4.;
      } else {
        coefficient *= 2.;
      }
    }

    isum += coefficient * func_(coordbuf);
  }

  result_ = isum * scale_;

  return true;
}
\end{lstlisting}

\newpage

\subsection{OpenMP-версия}
\begin{lstlisting}[language=C++]
bool vasilev_s_simpson_multidim::SimpsonTaskOmp::RunImpl() {
  double isum = 0.;
#pragma omp parallel
  {
    std::vector<std::size_t> gridpos(arity_);
    std::vector<double> coordbuf(arity_);
#pragma omp for reduction(+ : isum)
    for (int ip = 0; ip < int(gridcap_); ip++) {
      {
        auto p = ip;
        for (size_t i = 0; i < arity_; i++) {
          gridpos[i] = p % approxs_;
          p /= static_cast<int>(approxs_);
        }
      }

      for (size_t i = 0; i < arity_; i++) {
        coordbuf[i] = bounds_[i].lo + (static_cast<double>(gridpos[i]) * steps_[i]);
      }

      double coefficient = 1.;
      for (auto pos : gridpos) {
        if (pos == 0 || pos == (approxs_ - 1)) {
          coefficient *= 1.;
        } else if (pos % 2 != 0) {
          coefficient *= 4.;
        } else {
          coefficient *= 2.;
        }
      }

      isum += coefficient * func_(coordbuf);
    }
  }

  result_ = isum * scale_;

  return true;
}
\end{lstlisting}

\subsection{TBB-версия}
\begin{lstlisting}[language=C++]
bool vasilev_s_simpson_multidim::SimpsonTaskTbb::RunImpl() {
  oneapi::tbb::task_arena arena(ppc::util::GetPPCNumThreads());
  double isum = arena.execute([&] {
    return oneapi::tbb::parallel_reduce(
        oneapi::tbb::blocked_range<std::size_t>(0, gridcap_), 0.,
        [&](const tbb::blocked_range<std::size_t>& r, double threadsum) {
          std::vector<double> coordbuf(arity_);
          for (std::size_t ip = r.begin(); ip < r.end(); ip++) {
            auto p = ip;
            double coefficient = 1.;
            for (size_t k = 0; k < coordbuf.size(); k++) {
              const auto pos{p % approxs_};
              coordbuf[k] = bounds_[k].lo + (double(pos) * (bounds_[k].hi - bounds_[k].lo) / double(approxs_));
              p /= static_cast<decltype(p)>(approxs_);
              if (pos == 0 || pos == (approxs_ - 1)) {
                continue;
              }
              if (pos % 2 != 0) {
                coefficient *= 4.;
              } else {
                coefficient *= 2.;
              }
            }

            threadsum += coefficient * func_(coordbuf);
          }

          return threadsum;
        },
        std::plus<>());
  });

  result_ = isum * scale_;

  return true;
}
\end{lstlisting}

\subsection{STL (std::thread) версия}
\begin{lstlisting}[language=C++]
void vasilev_s_simpson_multidim::SimpsonTaskStl::ComputeThreadSum(std::pair<std::size_t, std::size_t> range,
                                                                  std::promise<double>&& promise) {
  double threadsum = 0.;
  std::vector<double> coordbuf(arity_);
  for (std::size_t ip = range.first; ip < range.second; ip++) {
    auto p = ip;
    double coefficient = 1.;
    for (size_t k = 0; k < coordbuf.size(); k++) {
      const auto pos = p % approxs_;
      coordbuf[k] = bounds_[k].lo + (double(pos) * (bounds_[k].hi - bounds_[k].lo) / double(approxs_));
      p /= static_cast<decltype(p)>(approxs_);
      if (pos == 0 || pos == (approxs_ - 1)) {
        continue;
      }
      if (pos % 2 != 0) {
        coefficient *= 4.;
      } else {
        coefficient *= 2.;
      }
    }
    threadsum += coefficient * func_(coordbuf);
  }
  promise.set_value(threadsum);
}

bool vasilev_s_simpson_multidim::SimpsonTaskStl::RunImpl() {
  const auto ws = static_cast<std::size_t>(ppc::util::GetPPCNumThreads());
  std::vector<std::future<double>> futures(ws);
  std::vector<std::thread> threads(ws);

  const auto per = gridcap_ / ws;
  const auto extra = gridcap_ % ws;

  std::size_t iptr = 0;
  for (std::size_t i = 0; i < ws; i++) {
    const auto leap = per + ((i < extra) ? 1 : 0);

    std::promise<double> promise;
    futures[i] = promise.get_future();
    threads[i] =
        std::thread(&SimpsonTaskStl::ComputeThreadSum, this, std::make_pair(iptr, iptr + leap), std::move(promise));
    iptr += leap;
  }

  double isum = 0.0;
  for (std::size_t i = 0; i < ws; i++) {
    threads[i].join();
    isum += futures[i].get();
  }

  result_ = isum * scale_;

  return true;
}
\end{lstlisting}

\newpage

\subsection{MPI + TBB версия}
\begin{lstlisting}[language=C++]
bool vasilev_s_simpson_multidim::SimpsonTaskAll::RunImpl() {
  boost::mpi::broadcast(world_, arity_, 0);
  boost::mpi::broadcast(world_, approxs_, 0);
  boost::mpi::broadcast(world_, bounds_, 0);

  if (world_.rank() != 0) {
    CalcSteps();
  }

  gridcap_ = static_cast<std::size_t>(std::pow(approxs_, arity_));
  const auto per = gridcap_ / world_.size();

  const auto begin = per * world_.rank();
  const auto iterlen = per + ((world_.rank() == (world_.size() - 1)) ? (gridcap_ % world_.size()) : 0);

  oneapi::tbb::task_arena arena(ppc::util::GetPPCNumThreads());
  double isum = arena.execute([&] {
    return oneapi::tbb::parallel_reduce(
        oneapi::tbb::blocked_range<std::size_t>(begin, begin + iterlen, iterlen / arena.max_concurrency()), 0.,
        [&](const tbb::blocked_range<std::size_t>& r, double threadsum) {
          std::vector<double> coordbuf(arity_);
          for (std::size_t ip = r.begin(); ip < r.end(); ip++) {
            auto p = ip;
            double coefficient = 1.;
            for (size_t k = 0; k < coordbuf.size(); k++) {
              const auto pos{p % approxs_};
              coordbuf[k] = bounds_[k].lo + (double(pos) * (bounds_[k].hi - bounds_[k].lo) / double(approxs_));
              p /= static_cast<decltype(p)>(approxs_);
              if (pos == 0 || pos == (approxs_ - 1)) {
                continue;
              }
              if (pos % 2 != 0) {
                coefficient *= 4.;
              } else {
                coefficient *= 2.;
              }
            }

            threadsum += coefficient * func_(coordbuf);
          }

          return threadsum;
        },
        std::plus<>());
  });

  double global_isum = 0.;
  boost::mpi::reduce(world_, isum, global_isum, std::plus{}, 0);

  if (world_.rank() == 0) {
    result_ = global_isum * scale_;
  }

  return true;
}
\end{lstlisting}

\end{document}