\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{tocloft}
\usepackage{xcolor} 
\usepackage{listings}
\usepackage{float}

\geometry{a4paper, left=25mm, right=15mm, top=20mm, bottom=20mm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    pdftitle={Отчет по проекту},
    pdfauthor={Дурыничев Дмитрий},
    pdfsubject={Вычисление многомерных интегралов с использованием многошаговой схемы (метод Симпсона).}
}

\lstset{
  language=C++,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{green},
  breaklines=true,
  numbers=left,
  numberstyle=\tiny,
  literate={_}{\_}{1}
}

\renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}

\begin{document}
\begin{titlepage}
    \centering
    \large
    Министерство науки и высшего образования Российской Федерации\\[0.5cm]
    Федеральное государственное автономное образовательное учреждение высшего образования\\[0.5cm]
    \textbf{«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»}\\
    (ННГУ)\\[1cm]
    Институт информационных технологий, математики и механики\\[0.5cm]
    Направление подготовки: \textbf{«Программная инженерия»}\\[2cm]

    \vfill
    {\LARGE \textbf{ОТЧЕТ}}\\[0.5cm]
    {\Large по задаче}\\[0.5cm]
    {\LARGE \textbf{«Вычисление многомерных интегралов с использованием многошаговой схемы (метод Симпсона)»}}\\[0.5cm]
    {\Large \textbf{Вариант №11}}\\[2.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Выполнил:} \\
        студент группы 3822Б1ПР3 \\
        \textbf{Дурыничев Дмитрий}
    }\\[0.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Преподаватели:} \\
        Нестеров А.Ю.
        Оболенский А.А.

    }\\[2cm]

    Нижний Новгород\\
    2025
\end{titlepage}


\thispagestyle{empty}
\clearpage
\pagenumbering{arabic} 
\setcounter{page}{2} 
\tableofcontents
\clearpage
\setcounter{page}{3} 
\section{Введение}

\hspace*{1.25em}Вычисление многомерных интегралов является одной из фундаментальных задач вычислительной математики, находящей применение в таких областях, как физика, инженерия, экономика и машинное обучение. Точные и эффективные методы численного интегрирования позволяют решать сложные задачи моделирования, оптимизации и анализа данных. Однако при увеличении размерности интеграла и объема вычислений последовательные подходы часто становятся недостаточно производительными, что делает актуальным использование параллельных вычислений.

Метод Симпсона — один из классических методов численного интегрирования, который обеспечивает высокую точность благодаря использованию квадратичной аппроксимации функции на малых отрезках. Его многошаговая схема позволяет разбить область интегрирования на множество подобластей, что создает естественные возможности для параллелизации вычислений. Современные вычислительные системы предоставляют широкий спектр технологий для реализации параллельных алгоритмов: от многоядерных процессоров до распределенных кластеров.

В данной работе рассматривается реализация метода Симпсона для вычисления многомерных интегралов с использованием последовательного подхода (SEQ), а также технологий параллельного программирования: OpenMP (OMP), Intel Threading Building Blocks (TBB), стандартной библиотеки потоков C++ (STL) и комбинации MPI с STL (MPI+STL). Основное внимание уделяется анализу производительности и масштабируемости этих подходов при обработке многомерных интегралов.

Цель работы — разработать и сравнить различные реализации многошагового метода Симпсона, определить их преимущества и ограничения, а также выявить оптимальную технологию для эффективного вычисления многомерных интегралов в многопоточной и распределенной среде.

\section{Постановка задачи}

\hspace*{1.25em}Целью данной работы является разработка и исследование различных реализаций многошагового метода Симпсона для вычисления многомерных интегралов с использованием последовательного и параллельных подходов.

Для достижения этой цели необходимо решить следующие задачи:

\begin{itemize}
\item Реализовать последовательную версию метода Симпсона (SEQ) для вычисления многомерных интегралов;
\item Разработать параллельные версии алгоритма с использованием следующих технологий:
\begin{itemize}
\item OpenMP (OMP) — стандарт для параллельного программирования на системах с общей памятью;
\item Intel TBB — библиотека для потокового параллелизма с автоматическим управлением задачами;
\item std::thread (STL) — инструменты стандартной библиотеки C++ для ручного управления потоками;
\item MPI + STL — гибридный подход, объединяющий распределенные вычисления и многопоточность.
\end{itemize}
\item Провести тестирование и проверку корректности всех реализованных версий алгоритма;
\item Оценить производительность последовательной и параллельных реализаций по времени выполнения и масштабируемости при увеличении размерности интеграла и числа подобластей.
\end{itemize}

Ожидаемым результатом работы является определение преимуществ и ограничений каждого из подходов к параллельной реализации метода Симпсона, а также выбор оптимальной технологии для эффективного вычисления многомерных интегралов в зависимости от характеристик задачи и вычислительной системы.

\section{Описание алгоритма}
Метод Симпсона представляет собой численный метод интегрирования, основанный на аппроксимации подынтегральной функции параболами на малых отрезках. Его многошаговая схема позволяет эффективно вычислять интегралы в многомерном пространстве, разбивая область интегрирования на множество подобластей. Основная идея заключается в вычислении взвешенной суммы значений функции в узловых точках с использованием коэффициентов, обеспечивающих высокую точность.
Принцип работы алгоритма для одномерного случая можно описать следующим образом:
\begin{enumerate}
\item Область интегрирования $[a, b]$ разбивается на $n$ равных отрезков длиной $h = (b - a) / n$, где $n$ — четное число.
\item Значения подынтегральной функции $f(x)$ вычисляются в узловых точках $x_i = a + i \cdot h$, где $i = 0, 1, \dots, n$.
\item Интеграл приближенно вычисляется по формуле Симпсона:
$$\int_a^b f(x) \, dx \approx \frac{h}{3} \left[ f(a) + 4 \sum_{\text{нечетные } i} f(x_i) + 2 \sum_{\text{четные } i} f(x_i) + f(b) \right].$$
\end{enumerate}
Для многомерного случая (например, двумерного) метод обобщается путем последовательного применения формулы Симпсона по каждому измерению:
\begin{itemize}
\item Область интегрирования $[x_0, x_1] \times [y_0, y_1]$ разбивается на сетку с шагом $h_x = (x_1 - x_0) / n$ по оси $x$ и $h_y = (y_1 - y_0) / n$ по оси $y$.
\item Для каждой точки $(x_i, y_j)$ вычисляется значение функции $f(x_i, y_j)$.
\item Итоговый интеграл вычисляется как взвешенная сумма значений функции с коэффициентами, соответствующими формуле Симпсона, умноженная на $h_x \cdot h_y / 9$.
\end{itemize}
В данной работе алгоритм реализован с учетом возможности параллельного выполнения:
\begin{itemize}
\item Для параллельных реализаций область интегрирования делится между потоками или процессами, каждый из которых вычисляет интеграл на своей подобласти.
\item После вычисления локальных сумм результаты объединяются в глобальную сумму, что минимизирует накладные расходы на синхронизацию.
\end{itemize}

\section{Описание параллельных реализаций алгоритма}

\subsection{OpenMP}

Параллельная реализация метода Симпсона с использованием технологии OpenMP выполнена в классе \texttt{SimpsonIntegralOpenMP}, определённом в пространстве имён \texttt{durynichev\_d\_integrals\_simpson\_method\_omp}. Основной целью данной реализации является распараллеливание вычислений интегралов в одномерном, двумерном и трёхмерном пространствах, минимизируя накладные расходы на синхронизацию и обеспечивая эффективное использование многоядерных процессоров.

\textbf{Структура реализации:}

\begin{enumerate}
  \item \textbf{Предобработка (\texttt{PreProcessingImpl}).}
  На этапе предобработки входные данные (границы интегрирования, число разбиений $n$ и тип функции) извлекаются из структуры \texttt{task\_data}. Границы сохраняются в векторе \texttt{boundaries\_}, а размерность \texttt{dim\_} определяется как половина числа границ. Тип функции (\texttt{func\_type\_}) задаётся последним элементом входного массива, если он указан, иначе используется функция по умолчанию (\texttt{kSquare}). Этот этап выполняется последовательно, так как требует минимальных вычислительных ресурсов.

  \item \textbf{Валидация (\texttt{ValidationImpl}).}
  Проверяется корректность входных данных: количество входных параметров должно быть не менее 3, выходной массив должен содержать ровно один элемент, а число разбиений $n$ должно быть чётным (требование метода Симпсона). Этот этап также последовательный из-за простоты операций.

  \item \textbf{Вычисление интеграла (\texttt{RunImpl}).}
  Основной этап вычислений зависит от размерности задачи:
  \begin{itemize}
    \item Для одномерного случая (\texttt{Simpson1D}) интеграл вычисляется по формуле Симпсона. Параллелизация осуществляется с помощью двух циклов:
    \begin{itemize}
      \item Первый цикл (\texttt{\#pragma omp for reduction(+ : sum\_odd)}) вычисляет сумму значений функции в нечётных узлах с коэффициентом 4.
      \item Второй цикл (\texttt{\#pragma omp for reduction(+ : sum\_even)}) вычисляет сумму в чётных узлах с коэффициентом 2.
      \item Директива \texttt{reduction} обеспечивает безопасное накопление сумм, минимизируя синхронизацию.
    \end{itemize}
    \item Для двумерного случая (\texttt{Simpson2D}) внешний цикл по переменной $x$ распараллеливается с помощью \texttt{\#pragma omp parallel for reduction(+ : sum)}. Каждый поток вычисляет локальную сумму по переменной $y$ для фиксированного $x$, умножая значения функции на соответствующие коэффициенты Симпсона (1, 2 или 4). Итоговая сумма умножается на $h_x \cdot h_y / 9$.
    \item Для трёхмерного случая (\texttt{Simpson3D}) итерации по переменным $x$ и $y$ распределяются между потоками с использованием одномерного индекса (\texttt{idx}), вычисляемого как $i \cdot (n + 1) + j$. Каждый поток вызывает \texttt{ComputeZIntegral} для интегрирования по $z$, а результаты суммируются с учётом коэффициентов Симпсона. Директива \texttt{\#pragma omp parallel for reduction(+ : sum)} управляет параллельным выполнением.
  \end{itemize}

  \item \textbf{Оценка функции (\texttt{Evaluate1D}, \texttt{Evaluate2D}, \texttt{Evaluate3D}).}
  Для каждой размерности реализованы различные подынтегральные функции (\texttt{kSquare}, \texttt{kSin}, \texttt{kCos}, \texttt{kExp}, \texttt{kLog}, \texttt{kCombined}). Выбор функции осуществляется через перечисление \texttt{FunctionType}. Для функций \texttt{kExp} и \texttt{kLog} предусмотрены проверки на переполнение и деление на ноль.

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}).}
  Итоговое значение интеграла (\texttt{result\_}) записывается в выходной буфер задачи. Этот этап выполняется последовательно, так как объём операций минимален.
\end{enumerate}

\textbf{Преимущества реализации:}
\begin{itemize}
  \item Использование директивы \texttt{reduction} в методах \texttt{Simpson1D}, \texttt{Simpson2D} и \texttt{Simpson3D} устраняет необходимость в критических секциях, снижая накладные расходы на синхронизацию.
  \item Статическое планирование по умолчанию в директивах \texttt{\#pragma omp parallel for} обеспечивает равномерное распределение нагрузки между потоками.
  \item Поддержка различных подынтегральных функций расширяет применимость реализации для разнообразных задач.
  \item Эффективное масштабирование на многоядерных системах, особенно для больших $n$ и высокой размерности, за счёт параллельного вычисления сумм.
\end{itemize}

\textbf{Пример:}
Для двумерного интеграла с $n = 100$ и 4 потоками каждый поток обрабатывает примерно 25 итераций внешнего цикла по $x$. Локальные суммы по $y$ вычисляются независимо, а затем объединяются в глобальную сумму с помощью \texttt{reduction}. Это позволяет эффективно использовать ресурсы многоядерного процессора, сокращая время выполнения по сравнению с последовательной реализацией.

\subsection{Intel TBB}

Реализация метода Симпсона с использованием библиотеки Intel oneAPI Threading Building Blocks (TBB) основана на модели task-based параллелизма, которая эффективно распределяет вычислительные задачи между потоками на многоядерных процессорах. Параллелизм применяется на этапах вычисления сумм значений подынтегральной функции в узловых точках, что ускоряет обработку многомерных интегралов.

\textbf{Ключевая функция реализации:} \texttt{SimpsonIntegralTBB::RunImpl}.

\textbf{Основные этапы:}
\begin{enumerate}
  \item \textbf{Инициализация (\texttt{PreProcessingImpl}):}
  \begin{itemize}
    \item Входные данные (границы интегрирования и число разбиений \texttt{n\_}) загружаются из структуры \texttt{task\_data} в вектор \texttt{boundaries\_}.
    \item Размерность задачи определяется как \texttt{dim\_} = \texttt{boundaries\_}.size() / 2.
    \item Число разбиений \texttt{n\_} извлекается из последнего элемента входного массива.
    \item Инициализируется переменная результата \texttt{result\_} значением 0.0.
  \end{itemize}

  \item \textbf{Валидация (\texttt{ValidationImpl}):}
  Проверяется корректность входных данных: количество входных параметров должно быть не менее 3, выходной массив должен содержать ровно один элемент, а число разбиений \texttt{n\_} должно быть чётным. Этот этап выполняется последовательно из-за минимальных вычислительных затрат.

  \item \textbf{Параллельное вычисление интеграла (\texttt{RunImpl}):}
  Вычисления выполняются внутри \texttt{tbb::task\_arena} с автоматическим определением числа потоков. Используется \texttt{tbb::task\_group} для управления задачами:
  \begin{itemize}
    \item Для одномерного случая (\texttt{Simpson1D}) применяется \texttt{tbb::parallel\_for} для параллельного вычисления сумм значений функции в узловых точках:
    \begin{lstlisting}
tbb::parallel_for(tbb::blocked_range<int>(1, n_), [&](const tbb::blocked_range<int>& r) {
  double local_sum = 0.0;
  for (int i = r.begin(); i < r.end(); ++i) {
    double coef = (i % 2 == 0) ? 2.0 : 4.0;
    local_sum += coef * Func1D(a + (i * h));
  }
  tbb::mutex::scoped_lock lock(mutex_);
  inner_sum += local_sum;
});
    \end{lstlisting}
    Локальные суммы накапливаются в переменной \texttt{inner\_sum} с использованием мьютекса \texttt{tbb::mutex} для синхронизации. Итоговый результат вычисляется как $\text{sum} \cdot h / 3$, где $\text{sum} = f(a) + f(b) + \texttt{inner\_sum}$.

    \item Для двумерного случая (\texttt{Simpson2D}) внешний цикл по $x$ распараллеливается с помощью \texttt{tbb::parallel\_for}:
    \begin{lstlisting}
tbb::parallel_for(tbb::blocked_range<int>(0, n_ + 1), [&](const tbb::blocked_range<int>& r) {
  double local_sum = 0.0;
  for (int i = r.begin(); i < r.end(); ++i) {
    double x = x0 + (i * hx);
    double coef_x = (i == 0 || i == n_) ? 1.0 : (i % 2 != 0) ? 4.0 : 2.0;
    for (int j = 0; j <= n_; ++j) {
      double y = y0 + (j * hy);
      double coef_y = (j == 0 || j == n_) ? 1.0 : (j % 2 != 0) ? 4.0 : 2.0;
      local_sum += coef_x * coef_y * Func2D(x, y);
    }
  }
  tbb::mutex::scoped_lock lock(mutex_);
  sum += local_sum;
});
    \end{lstlisting}
    Каждый поток вычисляет локальную сумму для своего диапазона $i$, а результаты объединяются в глобальную сумму \texttt{sum} с использованием мьютекса. Итоговый интеграл вычисляется как $\text{sum} \cdot h_x \cdot h_y / 9$.
  \end{itemize}

  \item \textbf{Оценка функции (\texttt{Func1D}, \texttt{Func2D}):}
  Реализованы подынтегральные функции: $f(x) = x^2$ для одномерного случая и $f(x, y) = x^2 + y^2$ для двумерного. Эти функции вычисляются последовательно внутри параллельных итераций.

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}):}
  Итоговое значение интеграла \texttt{result\_} записывается в выходной буфер задачи \texttt{task\_data}. Этот этап выполняется последовательно, так как требует минимальных ресурсов.
\end{enumerate}

\textbf{Поддерживающие методы:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} --- загружает границы и параметры задачи в вектор \texttt{boundaries\_}.
  \item \texttt{ValidationImpl()} --- проверяет корректность входных данных и чётность \texttt{n\_}.
  \item \texttt{RunImpl()} --- выполняет вычисления внутри \texttt{tbb::task\_arena} с использованием \texttt{tbb::task\_group}.
  \item \texttt{PostProcessingImpl()} --- сохраняет результат в выходной буфер \texttt{task\_data}.
\end{itemize}

\textbf{Преимущества реализации TBB:}
\begin{itemize}
  \item Использование \texttt{tbb::parallel\_for} обеспечивает автоматическое распределение итераций между потоками, упрощая управление параллелизмом.
  \item \texttt{tbb::task\_arena} позволяет гибко настраивать число потоков и эффективно использовать ресурсы процессора.
  \item Мьютексы (\texttt{tbb::mutex}) обеспечивают безопасное накопление сумм, хотя их использование может ограничивать масштабируемость при большом числе потоков.
  \item Поддержка блочного разбиения (\texttt{tbb::blocked\_range}) оптимизирует кэш-память, снижая накладные расходы.
  \item Высокая производительность на многоядерных системах, особенно для больших \texttt{n\_} и двумерных интегралов.
\end{itemize}

\textbf{Итог:} Реализация метода Симпсона с использованием Intel TBB обеспечивает высокую производительность за счёт эффективного параллельного вычисления сумм и гибкого управления задачами через \texttt{tbb::task\_arena}. Однако использование мьютексов для синхронизации может ограничивать масштабируемость при увеличении числа потоков, что требует дальнейшей оптимизации, например, через редукцию.

\subsection{STL (std::thread)}

Реализация метода Симпсона с использованием стандартной библиотеки потоков \texttt{std::thread} основана на явном управлении потоками для параллельного вычисления многомерных интегралов. Параллелизм достигается путём разбиения области интегрирования на подобласти, каждая из которых обрабатывается отдельным потоком, что ускоряет вычисления на многоядерных системах.

\textbf{Ключевая функция реализации:} \texttt{SimpsonIntegralSTL::RunImpl}.

\textbf{Основные этапы:}
\begin{enumerate}
  \item \textbf{Инициализация (\texttt{PreProcessingImpl}):}
  \begin{itemize}
    \item Входные данные (границы интегрирования и число разбиений \texttt{n\_}) загружаются из структуры \texttt{task\_data} в вектор \texttt{boundaries\_}.
    \item Размерность задачи определяется как \texttt{dim\_} = \texttt{boundaries\_}.size() / 2.
    \item Число разбиений \texttt{n\_} извлекается из последнего элемента входного массива.
    \item Создаётся вектор \texttt{results\_} размером, равным числу потоков (\texttt{ppc::util::GetPPCNumThreads()}), для хранения локальных результатов каждого потока.
  \end{itemize}

  \item \textbf{Валидация (\texttt{ValidationImpl}):}
  Проверяется корректность входных данных: количество входных параметров должно быть не менее 3, выходной массив должен содержать ровно один элемент, а число разбиений \texttt{n\_} должно быть чётным. Этот этап выполняется последовательно из-за минимальных вычислительных затрат.

  \item \textbf{Параллельное вычисление интеграла (\texttt{RunImpl}):}
  Область интегрирования разбивается на части, и для каждой части создаётся поток, выполняющий вычисление интеграла:
  \begin{itemize}
    \item Для одномерного случая (\texttt{Simpson1D}) интервал $[a, b]$ делится на \texttt{num\_threads} подобластей. Каждый поток вычисляет интеграл на своей подобласти $[sub_a, sub_b]$:
    \begin{lstlisting}
for (int i = 0; i < num_threads; ++i) {
  double sub_a = a + (i * interval_size);
  double sub_b = (i == num_threads - 1) ? b : sub_a + interval_size;
  threads[i] = std::thread(&SimpsonIntegralSTL::Simpson1D, this, sub_a, sub_b, std::ref(results_[i]));
}
    \end{lstlisting}
    Локальный результат сохраняется в \texttt{results\_[i]}. Итоговый интеграл получается суммированием всех локальных результатов.

    \item Для двумерного случая (\texttt{Simpson2D}) область $[x_0, x_1] \times [y_0, y_1]$ делится по оси $x$ на \texttt{num\_threads} подобластей:
    \begin{lstlisting}
for (int i = 0; i < num_threads; ++i) {
  double sub_x0 = x0 + (i * interval_size_x);
  double sub_x1 = (i == num_threads - 1) ? x1 : sub_x0 + interval_size_x;
  threads[i] = std::thread(&SimpsonIntegralSTL::Simpson2D, this, sub_x0, sub_x1, y0, y1, std::ref(results_[i]));
}
    \end{lstlisting}
    Каждый поток вычисляет интеграл на своей подобласти $[sub_x0, sub_x1] \times [y_0, y_1]$, а результаты сохраняются в \texttt{results\_[i]}.

    \item После завершения всех потоков (\texttt{thread.join()}) локальные результаты суммируются в \texttt{PostProcessingImpl}.
  \end{itemize}

  \item \textbf{Оценка функции (\texttt{Func1D}, \texttt{Func2D}):}
  Реализованы подынтегральные функции: $f(x) = x^2$ для одномерного случая и $f(x, y) = x^2 + y^2$ для двумерного. Эти функции вычисляются последовательно внутри каждого потока.

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}):}
  Локальные результаты из вектора \texttt{results\_} суммируются в итоговое значение \texttt{final\_result}, которое записывается в выходной буфер \texttt{task\_data}. Этот этап выполняется последовательно.
\end{enumerate}

\textbf{Поддерживающие методы:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} --- загружает границы и параметры задачи в вектор \texttt{boundaries\_} и инициализирует \texttt{results\_}.
  \item \texttt{ValidationImpl()} --- проверяет корректность входных данных и чётность \texttt{n\_}.
  \item \texttt{RunImpl()} --- создаёт и управляет потоками для параллельного вычисления интегралов.
  \item \texttt{PostProcessingImpl()} --- суммирует локальные результаты и сохраняет итог в \texttt{task\_data}.
\end{itemize}

\textbf{Преимущества реализации:}
\begin{itemize}
  \item Полный контроль над созданием и синхронизацией потоков с использованием \texttt{std::thread}, что позволяет глубоко изучить принципы параллельного программирования.
  \item Отсутствие внешних зависимостей, так как используется только стандартная библиотека C++, что упрощает переносимость кода.
  \item Гибкое разбиение области интегрирования на подобласти, что обеспечивает эффективное распределение нагрузки между потоками.
\end{itemize}

\textbf{Итог:} Реализация метода Симпсона с использованием \texttt{std::thread} подходит для образовательных целей и небольших проектов, предоставляя прозрачный контроль над потоками. Однако явное управление потоками и синхронизация через \texttt{thread.join()} могут быть менее эффективны по сравнению с высокоуровневыми средствами, такими как OpenMP или Intel TBB, особенно для больших задач, где требуется оптимизация масштабируемости.


\subsection{MPI + STL}

Гибридная реализация метода Симпсона с использованием MPI и \texttt{std::thread} сочетает распределённые вычисления и многопоточность для эффективного вычисления многомерных интегралов. Каждый MPI-процесс обрабатывает часть области интегрирования, используя потоки STL для параллельного вычисления локальных сумм, а результаты собираются на корневом процессе с помощью MPI-редукции.

\textbf{Ключевая функция реализации:} \texttt{SimpsonIntegralSTLMPI::RunImpl}.

\textbf{Основные этапы реализации:}
\begin{enumerate}
  \item \textbf{Предобработка (\texttt{PreProcessingImpl}):}
  \begin{itemize}
    \item На процессе с рангом 0 входные данные (границы интегрирования и число разбиений \texttt{n\_}) загружаются из структуры \texttt{task\_data} в вектор \texttt{boundaries\_}.
    \item Размерность задачи определяется как \texttt{dim\_} = \texttt{boundaries\_}.size() / 2.
    \item Создаётся вектор \texttt{results\_} размером, равным числу потоков (\texttt{ppc::util::GetPPCNumThreads()}), для хранения локальных результатов.
    \item Данные (\texttt{boundaries\_}, \texttt{dim\_}, \texttt{n\_}) транслируются всем процессам с помощью \texttt{boost::mpi::broadcast}.
  \end{itemize}

  \item \textbf{Параллельное вычисление интеграла (\texttt{RunImpl}):}
  \begin{itemize}
    \item Каждый процесс использует \texttt{std::thread} для параллельного вычисления интеграла на своей части области:
    \begin{itemize}
      \item Для одномерного случая (\texttt{Simpson1D}) интервал $[a, b]$ делится на \texttt{total\_workers} = \texttt{num\_threads} $\cdot$ \texttt{world\_.size()} частей. Каждый поток обрабатывает свою часть интервала, вычисляя сумму значений функции:
      \begin{lstlisting}
for (int i = 0; i < num_threads; ++i) {
  threads[i] = std::thread(&SimpsonIntegralSTLMPI::Simpson1D, this, h, a, b, 
                           std::ref(results_[i]), overall_rank_offset + i, total_workers);
}
      \end{lstlisting}
      На корневом процессе добавляется вклад граничных точек $f(a) + f(b)$. Локальные суммы сохраняются в \texttt{results\_[i]}.

      \item Для двумерного случая (\texttt{Simpson2D}) область $[x_0, x_1] \times [y_0, y_1]$ делится по оси $x$ на \texttt{total\_workers} частей:
      \begin{lstlisting}
for (int i = 0; i < num_threads; ++i) {
  threads[i] = std::thread(&SimpsonIntegralSTLMPI::Simpson2D, this, hx, hy, x0, x1, y0, y1, 
                           std::ref(results_[i]), overall_rank_offset + i, total_workers);
}
      \end{lstlisting}
      Каждый поток вычисляет сумму для своей части по $x$, а результаты сохраняются в \texttt{results\_[i]}.
    \end{itemize}
    \item Функция \texttt{CalcRange} вычисляет диапазон итераций для каждого потока на основе общего ранга (\texttt{overall\_rank}) и общего числа рабочих единиц (\texttt{total\_workers}).
    \item После завершения потоков (\texttt{thread.join()}) локальные результаты суммируются в \texttt{preres}.
    \item Глобальная сумма \texttt{total\_res\_} получается с помощью \texttt{boost::mpi::reduce} с операцией сложения на корневом процессе.
  \end{itemize}

  \item \textbf{Оценка функции (\texttt{Func1D}, \texttt{Func2D}):}
  Реализованы подынтегральные функции: $f(x) = x^2$ для одномерного случая и $f(x, y) = x^2 + y^2$ для двумерного. Эти функции вычисляются последовательно внутри каждого потока.

  \item \textbf{Постобработка (\texttt{PostProcessingImpl}):}
  \begin{itemize}
    \item На корневом процессе (ранг 0) глобальная сумма \texttt{total\_res\_} умножается на коэффициент \texttt{rescoeff\_} (равный $h / 3$ для одномерного случая или $h_x \cdot h_y / 9$ для двумерного) и записывается в выходной буфер \texttt{task\_data}.
  \end{itemize}
\end{enumerate}

\textbf{Функции реализации:}
\begin{itemize}
  \item \texttt{PreProcessingImpl()} --- загружает входные данные на процессе 0 и транслирует их всем процессам.
  \item \texttt{RunImpl()} --- координирует распределение вычислений между MPI-процессами и потоками STL.
  \item \texttt{PostProcessingImpl()} --- умножает глобальную сумму на \texttt{rescoeff\_} и сохраняет результат в \texttt{task\_data}.
  \item \texttt{ValidationImpl()} --- проверяет корректность входных данных и чётность \texttt{n\_} на процессе 0.
  \item \texttt{CalcRange()} --- вычисляет диапазон итераций для каждого потока.
  \item \texttt{Simpson1D()} --- вычисляет локальную сумму для одномерного интеграла.
  \item \texttt{Simpson2D()} --- вычисляет локальную сумму для двумерного интеграла.
\end{itemize}

\textbf{Преимущества:}
\begin{itemize}
  \item Комбинация MPI и \texttt{std::thread} обеспечивает масштабируемость на кластерах и эффективное использование многоядерных процессоров внутри узлов.
  \item Использование \texttt{boost::mpi::reduce} для сбора результатов минимизирует коммуникационные задержки.
  \item Гибкое разбиение области интегрирования между процессами и потоками позволяет эффективно распределять нагрузку.
  \item Отсутствие внешних зависимостей, кроме Boost.MPI, упрощает переносимость кода.
\end{itemize}

\textbf{Итог:} Гибридная реализация MPI + STL обеспечивает высокую производительность и масштабируемость за счёт распределения области интегрирования между MPI-процессами и параллельного вычисления внутри каждого процесса с использованием \texttt{std::thread}. Подход подходит для высокопроизводительных вычислений на кластерах, но требует тщательной настройки числа потоков и процессов для оптимальной производительности.

\section{Результаты экспериментов}

\hspace*{1.25em}Для оценки производительности реализованных версий метода Симпсона были проведены замеры времени выполнения в конфигурациях с различным числом потоков и процессов. В таблице ниже представлены усреднённые значения времени выполнения (в секундах) для двух режимов тестирования --- \texttt{PipelineRun} и \texttt{TaskRun}, а также рассчитано ускорение (\textit{Speedup}) относительно последовательной версии, определённое как отношение времени \texttt{TaskRun} последовательной реализации к времени \texttt{TaskRun} параллельной реализации.

\subsection{Таблица производительности}

\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Версия} & \textbf{Конфигурация} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Speedup} \\
\hline
\textbf{Последовательная} & --- & 1.093 & 2.145 & 1.00 \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 8.602 & 9.375 & 0.23 \\
  & 3 потока & 6.132 & 6.792 & 0.32 \\
  & 4 потока & 4.861 & 5.474 & 0.39 \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 0.546 & 1.019 & 2.11 \\
  & 3 потока & 0.349 & 0.724 & 2.96 \\
  & 4 потока & 0.316 & 0.585 & 3.67 \\
\hline
\multirow{3}{*}{STL (std::thread)} 
  & 2 потока & 9.613 & 9.796 & 0.22 \\
  & 3 потока & 10.444 & 11.647 & 0.18 \\
  & 4 потока & 11.699 & 13.054 & 0.16 \\
\hline
\multirow{4}{*}{MPI + STL} 
  & 2 потока + 2 процесса & 3.482 & 3.934 & 0.55 \\
  & 2 потока + 4 процесса & 2.694 & 2.914 & 0.74 \\
  & 4 потока + 2 процесса & 2.691 & 2.904 & 0.74 \\
  & 4 потока + 4 процесса & 2.751 & 2.972 & 0.72 \\
\hline
\end{tabular}
\caption{Производительность различных реализаций метода Симпсона}
\label{tab:parallel_perf}
\end{table}

\subsection*{Анализ}

\hspace*{1.25em}На основе таблицы видно, что наибольшее ускорение достигается в реализации \textbf{TBB} с конфигурацией \textbf{4 потока}, где \textit{Speedup} составляет \textbf{3.67}. Это указывает на высокую эффективность TBB благодаря автоматическому управлению задачами и оптимизации использования кэш-памяти через \texttt{tbb::blocked\_range}. Реализация \textbf{TBB} также демонстрирует минимальное время \texttt{PipelineRun} (0.316 с при 4 потоках), что говорит о низких накладных расходах на предобработку и постобработку.

\hspace*{1.25em}Реализация \textbf{OpenMP} показывает значительно меньшее ускорение, с максимальным \textit{Speedup} 0.39 при 4 потоках. Более того, время выполнения (\texttt{TaskRun} и \texttt{PipelineRun}) увеличивается по сравнению с последовательной версией, что указывает на высокие накладные расходы на синхронизацию (вероятно, из-за использования \texttt{reduction} или других директив). Это может быть связано с неоптимальным распределением задач или недостаточной оптимизацией для данной задачи.

\hspace*{1.25em}Реализация \textbf{STL (std::thread)} демонстрирует худшие результаты, с \textit{Speedup} менее 1 (максимум 0.22 при 2 потоках). Время выполнения (\texttt{TaskRun} до 13.054 с при 4 потоках) значительно выше, чем у последовательной версии, что, вероятно, обусловлено высокими накладными расходами на явное управление потоками через \texttt{std::thread} и синхронизацию с помощью \texttt{thread.join()}.

\hspace*{1.25em}Гибридная реализация \textbf{MPI + STL} показывает умеренное ускорение, с максимальным \textit{Speedup} 0.74 при конфигурациях 2 потока + 4 процесса и 4 потока + 2 процесса. Хотя время выполнения (\texttt{TaskRun} около 2.9--3.9 с) лучше, чем у OpenMP и STL, оно всё ещё выше последовательной версии. Это может быть связано с коммуникационными накладными расходами в MPI (например, \texttt{boost::mpi::reduce}) и ограниченной эффективностью разбиения области интегрирования для данного набора данных.

\hspace*{1.25em}Сравнение \texttt{PipelineRun} и \texttt{TaskRun} показывает, что \texttt{TaskRun} обычно занимает больше времени, что может быть связано с дополнительными вычислениями в основном этапе интегрирования. Однако в TBB разница между \texttt{PipelineRun} и \texttt{TaskRun} минимальна, что подчёркивает эффективность этой реализации.

\subsection*{Выводы}

\hspace*{1.25em}Реализация на основе \textbf{TBB} оказалась наиболее эффективной для локальной многопоточной обработки, обеспечивая максимальное ускорение (\textit{Speedup} 3.67) благодаря автоматическому управлению задачами и минимальным накладным расходам. \textbf{TBB} является оптимальным выбором для систем с общей памятью, особенно для задач с большим числом разбиений (\texttt{n\_}).

\hspace*{1.25em}\textbf{OpenMP} и \textbf{STL (std::thread)} показали неожиданно низкую производительность, с \textit{Speedup} менее 1, что указывает на неэффективность этих реализаций для данной задачи. OpenMP, вероятно, страдает от высоких накладных расходов на синхронизацию, а STL --- от сложностей с управлением потоками. Эти подходы могут быть полезны в образовательных целях, но не подходят для высокопроизводительных вычислений в данном контексте.

\hspace*{1.25em}Гибридная реализация \textbf{MPI + STL} демонстрирует умеренную производительность, но её \textit{Speedup} также ниже 1. Для улучшения результатов рекомендуется оптимизировать коммуникации в MPI (например, минимизировать вызовы \texttt{boost::mpi::reduce}) и использовать более крупные наборы данных, где распределённые вычисления могут дать больший выигрыш.

\hspace*{1.25em}Таким образом, для вычисления многомерных интегралов методом Симпсона на локальных системах оптимальным выбором является \textbf{TBB}. Для распределённых систем \textbf{MPI + STL} требует дальнейшей оптимизации, возможно, с увеличением размера задачи или использованием более эффективных стратегий распределения. В целом, результаты подчёркивают важность выбора подходящей технологии параллелизации в зависимости от характеристик задачи и вычислительной системы.

\section{Заключение}

\hspace*{1.25em}В данной работе была реализована и проанализирована численная реализация метода Симпсона для вычисления одномерных и двумерных интегралов. Базовая последовательная реализация была дополнена четырьмя параллельными версиями с использованием современных технологий: OpenMP, Intel TBB, стандартной библиотеки потоков C++ (STL) и гибридной модели MPI + STL.

\hspace*{1.25em}Каждая технология была выбрана для изучения её эффективности и особенностей при реализации параллельных алгоритмов для численного интегрирования. Были решены задачи распределения области интегрирования между потоками и процессами, параллельного вычисления сумм значений подынтегральной функции и синхронизации результатов. Экспериментальные замеры позволили сравнить производительность всех реализаций по времени выполнения (\texttt{PipelineRun} и \texttt{TaskRun}) и ускорению (\textit{Speedup}).

\hspace*{1.25em}Наивысшее ускорение продемонстрировала реализация \textbf{TBB} в конфигурации с 4 потоками, достигнув \textit{Speedup} 3.67 относительно последовательной версии. Это подчёркивает эффективность TBB благодаря автоматическому управлению задачами через \texttt{tbb::task\_arena} и оптимизации кэш-памяти с использованием \texttt{tbb::blocked\_range}. Реализация \textbf{OpenMP}, напротив, показала низкую эффективность с максимальным \textit{Speedup} 0.39 при 4 потоках, что, вероятно, связано с высокими накладными расходами на синхронизацию (например, через директиву \texttt{reduction}). Реализация \textbf{STL (std::thread)} оказалась наименее эффективной, с \textit{Speedup} до 0.22 при 2 потоках, из-за значительных затрат на управление потоками и синхронизацию через \texttt{thread.join()}. Гибридная реализация \textbf{MPI + STL} показала умеренное ускорение (максимальный \textit{Speedup} 0.74 при 2 потоках + 4 процессах или 4 потоках + 2 процессах), что может быть обусловлено коммуникационными накладными расходами в MPI (например, \texttt{boost::mpi::reduce}).

\hspace*{1.25em}Таким образом, все поставленные цели работы были достигнуты:
\begin{itemize}
  \item Разработан и протестирован алгоритм численного интегрирования методом Симпсона для одномерных и двумерных функций;
  \item Реализованы параллельные версии с использованием OpenMP, TBB, STL и MPI + STL;
  \item Проведён экспериментальный анализ производительности всех реализаций;
  \item Выявлены преимущества и ограничения каждой технологии в контексте масштабируемости и производительности.
\end{itemize}

\hspace*{1.25em}Результаты подчёркивают высокую эффективность локальной параллельной реализации \textbf{TBB} для задач численного интегрирования на многоядерных системах. \textbf{OpenMP} и \textbf{STL} показали низкую производительность, что указывает на необходимость оптимизации синхронизации и управления потоками для данной задачи. Гибридная модель \textbf{MPI + STL} требует дальнейшей оптимизации, возможно, с использованием более крупных областей интегрирования или улучшенных стратегий распределения данных, чтобы реализовать её потенциал в распределённых системах. В перспективе возможно исследование адаптивных алгоритмов разбиения области и оптимизации коммуникаций в MPI для повышения производительности гибридных реализаций.

\section{Список литературы}
\begin{enumerate}
    \item Bertsekas, D. P., \& Tsitsiklis, J. N. (1989). \textit{Parallel and Distributed Computation: Numerical Methods}. Athena Scientific.
    \item Chapman, B., Jost, G., \& van der Pas, R. (2008). \textit{Using OpenMP: Portable Shared Memory Parallel Programming}. MIT Press.
    \item Reinders, J. (2007). \textit{Intel Threading Building Blocks: Outfitting C++ for Multi-core Processor Parallelism}. O'Reilly Media.
    \item Williams, A. (2012). \textit{C++ Concurrency in Action: Practical Multithreading}. Manning Publications.
    \item Gropp, W., Lusk, E., \& Skjellum, A. (1999). \textit{Using MPI: Portable Parallel Programming with the Message-Passing Interface}. MIT Press.
    \item Davis, P. J., \& Rabinowitz, P. (1984). \textit{Methods of Numerical Integration}. Academic Press.
    \item Pacheco, P. (1997). \textit{Parallel Programming with MPI}. Morgan Kaufmann.
    \item Trobec, R., Slanička, P., \& Robic, B. (Eds.). (2009). \textit{Parallel Computing: Numerics, Applications, and Trends}. Springer.
\end{enumerate}
\appendix
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

В этом разделе приведены ключевые фрагменты реализации алгоритма вычисления многомерных интегралов с использованием многошаговой схемы.

\subsection*{OpenMP:}
\begin{lstlisting}[language=C++]
double durynichev_d_integrals_simpson_method_omp::SimpsonIntegralOpenMP::GetSimpsonCoefficient(int index, int n) {
  if (index == 0 || index == n) {
    return 1.0;
  }
  return (index % 2 == 0) ? 2.0 : 4.0;
}

double durynichev_d_integrals_simpson_method_omp::SimpsonIntegralOpenMP::ComputeZIntegral(double x, double y, double z0,
                                                                                          double z1, int n, double hz) {
  double local_sum = 0.0;
  for (int k = 0; k <= n; k++) {
    double z = z0 + (k * hz);
    double coef_z = GetSimpsonCoefficient(k, n);
    local_sum += coef_z * Evaluate3D(x, y, z);
  }
  return local_sum;
}

// Simpson integration methods
double durynichev_d_integrals_simpson_method_omp::SimpsonIntegralOpenMP::Simpson1D(double a, double b) const {
  double h = (b - a) / n_;
  double sum = Evaluate1D(a) + Evaluate1D(b);
  double sum_odd = 0.0;
  double sum_even = 0.0;

#pragma omp parallel
  {
#pragma omp for reduction(+ : sum_odd)
    for (int i = 1; i < n_; i += 2) {
      sum_odd += Evaluate1D(a + (i * h));
    }

#pragma omp for reduction(+ : sum_even)
    for (int i = 2; i < n_ - 1; i += 2) {
      sum_even += Evaluate1D(a + (i * h));
    }
  }

  sum += 4 * sum_odd + 2 * sum_even;
  return sum * h / 3.0;
}

double durynichev_d_integrals_simpson_method_omp::SimpsonIntegralOpenMP::Simpson2D(double x0, double x1, double y0,
                                                                                   double y1) {
  double hx = (x1 - x0) / n_;
  double hy = (y1 - y0) / n_;
  double sum = 0.0;

#pragma omp parallel for reduction(+ : sum)
  for (int i = 0; i <= n_; i++) {
    double x = x0 + (i * hx);
    double coef_x = GetSimpsonCoefficient(i, n_);
    double local_sum = 0.0;

    for (int j = 0; j <= n_; j++) {
      double y = y0 + (j * hy);
      double coef_y = GetSimpsonCoefficient(j, n_);
      local_sum += coef_y * Evaluate2D(x, y);
    }
    sum += coef_x * local_sum;
  }

  return sum * hx * hy / 9.0;
}

double durynichev_d_integrals_simpson_method_omp::SimpsonIntegralOpenMP::Simpson3D(double x0, double x1, double y0,
                                                                                   double y1, double z0, double z1) {
  double hx = (x1 - x0) / n_;
  double hy = (y1 - y0) / n_;
  double hz = (z1 - z0) / n_;
  double sum = 0.0;
  int total_iterations = (n_ + 1) * (n_ + 1);

#pragma omp parallel for reduction(+ : sum)
  for (int idx = 0; idx < total_iterations; idx++) {
    int i = idx / (n_ + 1);
    int j = idx % (n_ + 1);
    double x = x0 + (i * hx);
    double y = y0 + (j * hy);
    double coef_x = GetSimpsonCoefficient(i, n_);
    double coef_y = GetSimpsonCoefficient(j, n_);
    double local_sum = ComputeZIntegral(x, y, z0, z1, n_, hz);
    sum += coef_x * coef_y * local_sum;
  }

  return sum * hx * hy * hz / 27.0;
}
\end{lstlisting}

\subsection*{TBB:}
\begin{lstlisting}[language=C++]
double SimpsonIntegralTBB::Simpson1D(double a, double b) const {
  double h = (b - a) / n_;
  double sum = Func1D(a) + Func1D(b);

  double inner_sum = 0.0;
  tbb::parallel_for(tbb::blocked_range<int>(1, n_), [&](const tbb::blocked_range<int>& r) {
    double local_sum = 0.0;
    for (int i = r.begin(); i < r.end(); ++i) {
      double coef = (i % 2 == 0) ? 2.0 : 4.0;
      local_sum += coef * Func1D(a + (i * h));
    }
    tbb::mutex::scoped_lock lock(mutex_);
    inner_sum += local_sum;
  });

  sum += inner_sum;
  return sum * h / 3.0;
}

double SimpsonIntegralTBB::Simpson2D(double x0, double x1, double y0, double y1) const {
  double hx = (x1 - x0) / n_;
  double hy = (y1 - y0) / n_;
  double sum = 0.0;

  tbb::parallel_for(tbb::blocked_range<int>(0, n_ + 1), [&](const tbb::blocked_range<int>& r) {
    double local_sum = 0.0;
    for (int i = r.begin(); i < r.end(); ++i) {
      double x = x0 + (i * hx);
      double coef_x = 0;
      if (i == 0 || i == n_) {
        coef_x = 1.0;
      } else if (i % 2 != 0) {
        coef_x = 4.0;
      } else {
        coef_x = 2.0;
      }

      for (int j = 0; j <= n_; ++j) {
        double y = y0 + (j * hy);
        double coef_y = 0;
        if (j == 0 || j == n_) {
          coef_y = 1.0;
        } else if (j % 2 != 0) {
          coef_y = 4.0;
        } else {
          coef_y = 2.0;
        }
        local_sum += coef_x * coef_y * Func2D(x, y);
      }
    }
    tbb::mutex::scoped_lock lock(mutex_);
    sum += local_sum;
  });

  return sum * hx * hy / 9.0;
}
\end{lstlisting}

\subsection*{STL:}
\begin{lstlisting}[language=C++]
bool SimpsonIntegralSTL::RunImpl() {
  const int num_threads = ppc::util::GetPPCNumThreads();
  std::vector<std::thread> threads(num_threads);

  if (dim_ == 1) {
    double a = boundaries_[0];
    double b = boundaries_[1];
    double interval_size = (b - a) / num_threads;

    for (int i = 0; i < num_threads; ++i) {
      double sub_a = a + (i * interval_size);
      double sub_b = (i == num_threads - 1) ? b : sub_a + interval_size;
      threads[i] = std::thread(&SimpsonIntegralSTL::Simpson1D, this, sub_a, sub_b, std::ref(results_[i]));
    }
  } else if (dim_ == 2) {
    double x0 = boundaries_[0];
    double x1 = boundaries_[1];
    double y0 = boundaries_[2];
    double y1 = boundaries_[3];
    double interval_size_x = (x1 - x0) / num_threads;

    for (int i = 0; i < num_threads; ++i) {
      double sub_x0 = x0 + (i * interval_size_x);
      double sub_x1 = (i == num_threads - 1) ? x1 : sub_x0 + interval_size_x;
      threads[i] = std::thread(&SimpsonIntegralSTL::Simpson2D, this, sub_x0, sub_x1, y0, y1, std::ref(results_[i]));
    }
  }

  for (auto& thread : threads) {
    thread.join();
  }

  return true;
}

void SimpsonIntegralSTL::Simpson1D(double a, double b, double& result) const {
  double h = (b - a) / n_;
  double sum = Func1D(a) + Func1D(b);

  for (int i = 1; i < n_; i += 2) {
    sum += 4 * Func1D(a + (i * h));
  }
  for (int i = 2; i < n_ - 1; i += 2) {
    sum += 2 * Func1D(a + (i * h));
  }

  result = sum * h / 3.0;
}

void SimpsonIntegralSTL::Simpson2D(double x0, double x1, double y0, double y1, double& result) const {
  double hx = (x1 - x0) / n_;
  double hy = (y1 - y0) / n_;
  double sum = 0.0;

  for (int i = 0; i <= n_; i++) {
    double x = x0 + (i * hx);
    double coef_x = 0.0;
    if (i == 0 || i == n_) {
      coef_x = 1;
    } else if (i % 2 != 0) {
      coef_x = 4;
    } else {
      coef_x = 2;
    }

    for (int j = 0; j <= n_; j++) {
      double y = y0 + (j * hy);
      double coef_y = 0.0;
      if (j == 0 || j == n_) {
        coef_y = 1;
      } else if (j % 2 != 0) {
        coef_y = 4;
      } else {
        coef_y = 2;
      }
      sum += coef_x * coef_y * Func2D(x, y);
    }
  }

  result = sum * hx * hy / 9.0;
}
\end{lstlisting}

\subsection*{MPI + STL:}
\begin{lstlisting}[language=C++]
bool SimpsonIntegralSTLMPI::RunImpl() {
  boost::mpi::broadcast(world_, boundaries_, 0);
  boost::mpi::broadcast(world_, dim_, 0);
  boost::mpi::broadcast(world_, n_, 0);

  const int num_threads = ppc::util::GetPPCNumThreads();
  std::vector<std::thread> threads(num_threads);

  const int overall_rank_offset = num_threads * world_.rank();
  const int total_workers = num_threads * world_.size();

  double preres{};
  if (dim_ == 1) {
    const double a = boundaries_[0];
    const double b = boundaries_[1];

    const double h = (b - a) / n_;

    rescoeff_ = h / 3.0;
    preres = world_.rank() == 0 ? (Func1D(a) + Func1D(b)) : 0;

    for (int i = 0; i < num_threads; ++i) {
      threads[i] = std::thread(&SimpsonIntegralSTLMPI::Simpson1D, this, h, a, b, std::ref(results_[i]),
                               overall_rank_offset + i, total_workers);
    }
  } else if (dim_ == 2) {
    const double x0 = boundaries_[0];
    const double x1 = boundaries_[1];
    const double y0 = boundaries_[2];
    const double y1 = boundaries_[3];

    const double hx = (x1 - x0) / n_;
    const double hy = (y1 - y0) / n_;

    rescoeff_ = hx * hy / 9.0;
    preres = 0.0;

    for (int i = 0; i < num_threads; ++i) {
      threads[i] = std::thread(&SimpsonIntegralSTLMPI::Simpson2D, this, hx, hy, x0, x1, y0, y1, std::ref(results_[i]),
                               overall_rank_offset + i, total_workers);
    }
  }

  for (auto& thread : threads) {
    thread.join();
  }

  for (double res : results_) {
    preres += res;
  }
  boost::mpi::reduce(world_, preres, total_res_, std::plus{}, 0);

  return true;
}

double SimpsonIntegralSTLMPI::Func1D(double x) { return x * x; }

double SimpsonIntegralSTLMPI::Func2D(double x, double y) { return (x * x) + (y * y); }

namespace {
std::pair<int, int> CalcRange(int space, int overall_rank, int total_workers) {
  const int base = space / total_workers;

  const int start = base * overall_rank;
  const int end = (overall_rank == (total_workers - 1)) ? space : (base * (overall_rank + 1));

  return std::make_pair(start, end);
}
}  // namespace

void SimpsonIntegralSTLMPI::Simpson1D(double h, double a, double b, double& result, int overall_rank,
                                      int total_workers) const {
  const auto r = CalcRange(n_ - 1, overall_rank, total_workers);
  double sum = 0.0;
  for (int i = 1 + r.first; i <= r.second; ++i) {
    double coef = (i % 2 == 0) ? 2.0 : 4.0;
    sum += coef * Func1D(a + (i * h));
  }
  result = sum;
}

void SimpsonIntegralSTLMPI::Simpson2D(double hx, double hy, double x0, double x1, double y0, double y1, double& result,
                                      int overall_rank, int total_workers) const {
  const auto r = CalcRange(n_ + 1, overall_rank, total_workers);

  double sum = 0.0;
  for (int i = r.first; i < r.second; ++i) {
    const double x = x0 + (i * hx);
    double coef_x = 0.0;
    if (i == 0 || i == n_) {
      coef_x = 1;
    } else if (i % 2 != 0) {
      coef_x = 4;
    } else {
      coef_x = 2;
    }

    for (int j = 0; j <= n_; j++) {
      const double y = y0 + (j * hy);
      double coef_y = 0.0;
      if (j == 0 || j == n_) {
        coef_y = 1;
      } else if (j % 2 != 0) {
        coef_y = 4;
      } else {
        coef_y = 2;
      }
      sum += coef_x * coef_y * Func2D(x, y);
    }
  }

  result = sum;
}
\end{lstlisting}
\end{document}