\documentclass[14pt, a4paper]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{tempora}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage[normalem]{ulem} 
\usepackage{amsmath}
\usepackage{listings} 
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{parskip}
\usepackage{makecell}

\titleformat{name=\section,numberless}{\centering\normalfont\fontsize{18pt}{22pt}\bfseries}{}{0pt}{}
\titlespacing*{name=\section,numberless}{0pt}{0pt}{24pt}

\titleformat{\section}{\normalfont\fontsize{16pt}{19pt}\bfseries}{\thesection.}{1em}{}
\titlespacing*{\section}{0pt}{0pt}{12pt}

\titleformat{\subsection}{\normalfont\fontsize{16pt}{19pt}\bfseries}{\thesubsection.}{1em}{}
\titlespacing*{\subsection}{0pt}{24pt}{12pt}

\lstset{
  basicstyle=\ttfamily\fontsize{12pt}{14pt}\selectfont,
  frame=single,
  breaklines=true,
  tabsize=2,
  language=C++,
  showstringspaces=false,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal}
}

\newcommand{\code}[1]{{\ttfamily\fontsize{14pt}{16pt}\selectfont #1}}

\geometry{top=2cm, bottom=2cm, left=2.5cm, right=1.5cm}
\setlength{\parindent}{1.25cm}

\titleformat{\section}{\normalfont\fontsize{18pt}{19pt}\bfseries}{\thesection.}{1em}{}
\titleformat{name=\section,numberless}{\centering\normalfont\fontsize{18pt}{19pt}\bfseries}{}{0pt}{}
\titleformat{\subsection}{\normalfont\fontsize{16pt}{19pt}\bfseries}{\thesubsection.}{1em}{}

\begin{document}
\onehalfspacing
\thispagestyle{empty}

\begin{spacing}{1.0}
\begin{center}
МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ \\

Федеральное государственное автономное образовательное учреждение высшего образования \\
\textbf{«Национальный исследовательский} \\
\textbf{Нижегородский государственный университет им. Н.И. Лобачевского»} \\

\textbf{(ННГУ)} \\[4ex]

\textbf{Институт информационных технологий, математики и механики} \\[18ex]

{\fontsize{18pt}{22pt}\selectfont\textbf{ОТЧЕТ}} \\[1ex]
по практическим работам \\[2ex]

на тему: \\[1ex]
{\fontsize{16pt}{20pt}\selectfont\textbf{Линейная фильтрация изображений (блочное разбиение) с ядром Гаусса 3x3}} \\[14ex]
\end{center}

\hspace*{0.45\textwidth}
\begin{minipage}{0.45\textwidth}
\textbf{Выполнил:} студент группы 3822Б1ФИ3 Созонов И.С.\\[2ex]

\textbf{Проверил:} к.т.н., доцент каф. ВВиСП Сысоев А.В.
\end{minipage}

\vfill

\begin{center}
Нижний Новгород \\
2025
\end{center}
\end{spacing}

\newpage

\begin{center}
\section*{Содержание}
\end{center}

\noindent
Введение \dotfill 3 \\
Основная часть \dotfill 4 \\
\hspace*{0.5cm}1.\hspace{0.5em}Постановка задачи \dotfill 4 \\
\hspace*{0.5cm}2.\hspace{0.5em}Описание алгоритма \dotfill 5 \\
\hspace*{0.5cm}3.\hspace{0.5em}Описание схемы параллельного алгоритма \dotfill 7 \\
\hspace*{1cm}3.1.\hspace{0.5em}Описание OpenMP-версии \dotfill 8 \\
\hspace*{1cm}3.2.\hspace{0.5em}Описание TBB-версии \dotfill 9 \\
\hspace*{1cm}3.3.\hspace{0.5em}Описание STL-версии \dotfill 10 \\
\hspace*{1cm}3.4.\hspace{0.5em}Описание MPI + OpenMP-версии \dotfill 11 \\
\hspace*{0.5cm}4.\hspace{0.5em}Результаты экспериментов \dotfill 13 \\
\hspace*{1cm}4.1.\hspace{0.5em}Тестовые данные и аппаратная среда \dotfill 13 \\
\hspace*{1cm}4.2.\hspace{0.5em}Ускорение и эффективность \dotfill 14 \\
\hspace*{1cm}4.3.\hspace{0.5em}Выводы из результатов \dotfill 16 \\
Заключение \dotfill 18 \\
Список литературы \dotfill 19 \\
Приложения \dotfill 20 \\
\hspace*{0.5cm}Приложение A.\hspace{0.5em}Реализация последовательной версии \dotfill 20 \\
\hspace*{0.5cm}Приложение B.\hspace{0.5em}Реализация OpenMP-версии \dotfill 21 \\
\hspace*{0.5cm}Приложение C.\hspace{0.5em}Реализация TBB-версии \dotfill 22 \\
\hspace*{0.5cm}Приложение D.\hspace{0.5em}Реализация STL-версии \dotfill 23 \\
\hspace*{0.5cm}Приложение E.\hspace{0.5em}Реализация MPI + OpenMP-версии \dotfill 24 \\

\newpage

\section*{Введение}

Обработка изображений играет ключевую роль в различных областях, включая компьютерное зрение, медицинскую диагностику, дистанционное зондирование и машинное обучение. Одним из важнейших этапов предварительной обработки изображений является линейная фильтрация – метод, позволяющий изменять значения пикселей изображения на основе значений их соседей по заранее определённому правилу.

Линейная фильтрация применяется для устранения шумов, сглаживания изображения, выделения краёв и других задач. Её основой является операция свёртки изображения с ядром – небольшой матрицей весов. При этом значение каждого пикселя в новом изображении вычисляется как взвешенная сумма значений его соседей. Такой подход позволяет эффективно подавлять мелкие шумовые колебания, сохраняя при этом общую структуру изображения.

В данной лабораторной работе используется ядро Гаусса 3×3, представляющее собой аппроксимацию двумерного нормального распределения. Оно обеспечивает плавное сглаживание изображения с минимальной потерей деталей, поскольку присваивает наибольший вес центральному пикселю и меньшие – окружающим.

Для повышения вычислительной эффективности применяется метод блочного разбиения изображения, при котором исходное изображение делится на неперекрывающиеся блоки, обрабатываемые независимо.

\newpage

\section*{Основная часть}

\section{Постановка задачи}

Целью данной лабораторной работы является реализация и сравнительный анализ производительности различных вариантов алгоритма линейной фильтрации изображений на основе ядра Гаусса 3×3, с применением блочного разбиения и технологий параллельного программирования. 

Для достижения поставленной цели необходимо решить следующие задачи:

\begin{enumerate}
  \item Реализовать последовательную версию алгоритма линейной фильтрации изображения с использованием ядра Гаусса 3×3 в качестве базового референса при оценке корректности и эффективности параллельных реализаций.
  
  \item Разработать параллельные версии алгоритма с использованием следующих технологий:
  \begin{itemize}
    \item Open Multi-Processing (OpenMP);
    \item Threading Building Blocks (TBB);
    \item Стандартная библиотека потоков C++ (std::thread);
    \item Message Passing Interface (MPI) с использованием OpenMP.
  \end{itemize}
  
  \item Организовать экспериментальное исследование:
  \begin{itemize}
    \item Замерить время выполнения каждого варианта алгоритма на изображениях различного размера;
    \item Выполнить сравнительный анализ производительности (ускорение и эффективность);
  \end{itemize}
  
  \item Подтвердить корректность параллельных реализаций путём:
  \begin{itemize}
    \item Сравнения результатов фильтрации с последовательной версией;
    \item Проверки устойчивости к различным размерам изображения и числу потоков или процессов.
  \end{itemize}
\end{enumerate}

\newpage

\section{Описание алгоритма}

Цифровое изображение представляется в виде двумерного массива пикселей (матрицы), где каждый элемент соответствует яркости точки изображения. Обычно значения пикселей находятся в диапазоне от 0 до 255 (в оттенках серого) или представляют собой векторы для цветных изображений (например, RGB-компоненты). В рамках данной лабораторной работы рассматривается одноканальное изображение (градации серого), представленное как матрица  $I$, где  $I_{i,j}$ – яркость пикселя в строке $i$, столбце $j$.

Необходимо построить новое изображение $G$, в котором каждый пиксель будет результатом применения линейного фильтра Гаусса к окрестности соответствующего пикселя исходного изображения.

Ключевым элементом линейной фильтрации является \textit{ядро свёртки} – небольшая матрица весов, которая используется для взвешенного суммирования окрестности пикселя. В данной лабораторной работе используется ядро Гаусса размером 3×3, которое моделирует двумерное гауссово распределение со средним квадратичным отклонением $\sigma \approx 1$. Такое ядро обеспечивает сглаживание изображения, т. е. удаление шумов при сохранении основных контуров и градиентов.

Ядро Гаусса 3×3 имеет следующий вид:
\[
K = \frac{1}{16}
\begin{bmatrix}
1 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 1
\end{bmatrix}
\]

Оно симметрично относительно центра и обеспечивает плавное влияние соседних пикселей: центр получает максимальный вес, ближние по горизонтали и вертикали – чуть меньший, а диагональные – минимальный. Сумма всех коэффициентов равна 1, что сохраняет общий уровень яркости изображения.

Суть линейной фильтрации заключается в том, что для каждого пикселя (за исключением краевых), мы вычисляем свёртку ядра с локальной окрестностью размером 3×3, взвешивая каждый пиксель на соответствующий коэффициент из ядра.

Для каждого внутреннего пикселя $G_{i,j}$ новое значение рассчитывается по формуле:
\[
G_{i,j} = \sum_{m=-1}^{1} \sum_{n=-1}^{1} K_{m+1,n+1} \cdot I_{i+m,j+n},
\]
где:
\begin{itemize}
  \item $m, n$ – смещения по строкам и столбцам от центра ядра,
  \item $K_{m+1,n+1}$ – соответствующий элемент ядра,
  \item $I_{i+m,j+n}$ – пиксель исходного изображения, попадающий в окно 3×3.
\end{itemize}

Иными словами, результат – это взвешенное среднее значений пикселя и его ближайших соседей, где веса определяются ядром $K$.

\newpage

\section{Описание схемы параллельного алгоритма}
Линейная фильтрация изображений – вычислительно затратная операция, особенно для изображений большого размера. Каждый пиксель требует обработки окрестности 3×3, что делает алгоритм естественным кандидатом для распараллеливания. Цель параллельного алгоритма – ускорить выполнение фильтрации за счёт распределения вычислений между несколькими потоками или процессами, избегая конфликтов и обеспечивая корректный доступ к необходимым данным.

В рамках данной реализации используется блочное разбиение изображения, т.е. оно делится на неперекрывающиеся прямоугольные блоки, каждый из которых обрабатывается независимо в своём потоке или процессе. Такой подход уменьшает накладные расходы на синхронизацию и эффективно использует многопроцессорные архитектуры.

Каждый поток или процесс выполняет линейную фильтрацию с помощью свертки ядра Гаусса 3×3 в пределах своего блока, игнорируя граничные пиксели, либо обрабатывая их с дополнительной буферизацией. Алгоритм внутри блока:

\begin{itemize}
  \item Пройти по каждой внутренней позиции блока.
  \item Рассчитать взвешенное среднее с использованием ядра Гаусса 3×3.
  \item Записать результат в выходную матрицу.
\end{itemize}

После завершения обработки всеми потоками в многопоточном приложении (OpenMP,  TBB, std::thread) все потоки записывают результат напрямую в общую выходную матрицу, при условии, что каждый отвечает за уникальный участок (без конфликтов).Тогда как в распределённой среде (MPI) каждый процесс возвращает свой обработанный фрагмент главному (root), который собирает итоговое изображение.

Далее будут подробно рассмотрены конкретные реализации данного алгоритма с использованием таких технологий, как OpenMP, Intel TBB, стандартные потоки C++ std::thread, а также гибридный подход с применением MPI совместно с OpenMP. Каждая из этих технологий обладает своими особенностями и преимуществами, позволяя выбрать наиболее подходящее решение для разных условий вычислительной среды и объёма обрабатываемых данных.

\subsection{Описание OpenMP-версии}
Для повышения производительности фильтрации изображений при больших размерах изображения была реализована параллельная версия алгоритма с использованием OpenMP (Open Multi-Processing), которая предоставляет набор директив компилятора, функций и переменных среды, упрощающих организацию многопоточной обработки в системах с общей памятью. В данной лабораторной работе OpenMP используется для одновременной обработки блоков изображения разными потоками, что позволяет существенно сократить общее время выполнения алгоритма свёртки.

Чтобы улучшить эффективность работы кэш-памяти и сбалансировать нагрузку между потоками, всё изображение предварительно разбивается на блоки фиксированного размера \code{block\_size~x~block\_size}. Число блоков по горизонтали \code{num\_blocks\_x} и вертикали \code{num\_blocks\_y} рассчитываются с округлением вверх, чтобы покрыть всё изображение даже при неполных крайних блоках.

Основной цикл обработки распараллеливается при помощи директивы \code{\#pragma omp parallel for}, которая сообщает компилятору, что итерации внешнего цикла по координате \code{block\_y} могут быть выполнены независимо и распределены между потоками. Схема планирования \code{schedule(dynamic, 1)} позволяет потокам динамически получать по одному блоку на выполнение, что особенно полезно при неоднородности нагрузки.

Внутри каждого блока производится линейная фильтрация на основе ядра Гаусса. Для каждого пикселя выполняется операция свёртки: из массива  \code{image\_} извлекаются значения пикселей из окрестности 3×3 вокруг текущей точки, умножаются на соответствующие элементы ядра  \code{kernel}, после чего результаты суммируются. Полученное значение записывается в соответствующую ячейку выходного массива  \code{filtered\_image\_}.

В результате, каждый блок изображения обрабатывается независимо и параллельно. Такой подход позволяет эффективно задействовать вычислительные ресурсы многопроцессорной системы, обеспечивая значительное ускорение по сравнению с последовательной реализацией. При этом использование OpenMP делает реализацию достаточно простой и удобной за счёт высокоуровневых средств управления параллелизмом.


\subsection{Описание TBB-версии}
Для повышения производительности алгоритма фильтрации изображений также была реализована параллельная версия с использованием TBB (Threading Building Blocks). Данная высокоуровневая библиотека предоставляет удобные абстракции для многопоточной обработки, автоматически управляя распределением задач и балансом нагрузки между потоками.

Изображение также разбивается на прямоугольные блоки фиксированного размера  \code{block\_size~x~block\_size}. Это разбиение позволяет локализовать доступ к памяти, улучшить работу кэша и упростить параллельную реализацию.

Каждого блок обрабатывается в отдельной функции \code{ProcessBlock}, которая принимает координаты левого верхнего угла блока, размеры изображения, а также ссылки на входной (\code{image}) и выходной (\code{filtered\_image}) массивы. Внутри этой функции для каждого пикселя (за исключением границ) выполняется операция свёртки с ядром Гаусса 3×3. Окрестность каждого пикселя умножается поэлементно на значения ядра, после чего сумма записывается в соответствующую позицию выходного изображения. Граничные пиксели (первый и последний ряды и столбцы) исключаются из обработки.

Для выполнения фильтрации в параллельном режиме используется объект \code{tbb::task\_arena}, инициализированный с числом потоков, определённым функцией \code{ppc::util::GetPPCNumThreads()}. Далее в пределах этой арены запускается набор задач через \code{tbb::task\_group}, в котором каждому блоку изображения соответствует своя задача. Они запускаются с помощью метода \code{run}, и после их постановки вызывается \code{wait}, обеспечивающий ожидание завершения всех задач перед выходом из арены.

Поскольку каждая задача обрабатывает отдельный блок изображения и не взаимодействует с другими, такая реализация обеспечивает безопасное и эффективное распараллеливание. Библиотека TBB автоматически управляет внутренним пулом потоков и распределяет задачи с учётом текущей загрузки, что обеспечивает хорошую масштабируемость и производительность на многопроцессорных системах. Таким образом, данная версия алгоритма сочетает в себе высокую эффективность, читаемость и удобство поддержки, благодаря гибким средствам, предоставляемым TBB.


\subsection{Описание STL-версии}
Для реализации параллельной обработки изображения была создана версия алгоритма на базе стандартной библиотеки потоков C++ std::thread, которая предоставляет низкоуровневый интерфейс управления потоками, позволяя вручную запускать, координировать и синхронизировать выполнение задач.

Подобно другим параллельным версиям, всё изображение предварительно разбивается на блоки фиксированного размера \code{block\_size~x~block\_size}. Для каждого блока сохраняются координаты его верхнего левого угла в структуре  \code{std::vector<std::pair<int, int>> blocks}, что позволяет последовательно или параллельно обрабатывать блоки по индексу.

Ключевая функция обработки \code{ProcessBlock} получает координаты начала блока, размеры изображения и ссылки на входной и выходной массивы. Для каждого пикселя блока (кроме граничных) выполняется операция свёртки с ядром Гаусса 3×3. Результат записывается в массив  \code{filtered\_image} по тем же координатам.

Параллельная обработка блоков реализована через пул потоков фиксированного размера  \code{num\_threads}. Для синхронизации работы между потоками используется переменная  \code{std::atomic<int> block\_id}, которая позволяет безопасно выбирать блоки для обработки без конфликтов. Каждый поток в цикле получает индекс очередного блока через  \code{fetch\_add} и затем вызывает  \code{ProcessBlock} с соответствующими параметрами. Таким образом, достигается динамическое распределение нагрузки, позволяющее потокам поочерёдно и независимо обрабатывать блоки до тех пор, пока не будут завершены все задачи.

По завершении запуска всех потоков основной поток вызывает  \code{join} для каждого из них, что обеспечивает корректное завершение работы и ожидание всех параллельных задач.

Подход с использованием std::thread обеспечивает полный контроль над параллелизмом, однако требует ручного управления синхронизацией и балансировкой нагрузки. Несмотря на это, данная реализация остаётся простой и эффективной для таких задач, как блочная фильтрация изображений ядром Гаусса 3×3, и позволяет добиться значительного ускорения по сравнению с последовательной версией при использовании многопроцессорной архитектуры.


\subsection{Описание MPI + OpenMP-версии}
Для обеспечения гибридного параллелизма в данной реализации использованы одновременно две технологии: MPI (Message Passing Interface) для межпроцессного взаимодействия и OpenMP для многопоточности внутри каждого процесса. Такой подход позволяет эффективно масштабировать алгоритм как на кластерах с несколькими узлами, так и на многоядерных системах.

На начальном этапе основным (нулевым) процессом выполняется рассылка размеров изображения (\code{width\_}, \code{height\_}) всем участникам с помощью коллективной функции широковещательного обмена \code{broadcast}. Затем изображение разбивается на блоки строк, количество которых зависит от числа доступных процессов \code{num\_procs}. Чтобы обеспечить равномерную загрузку, каждому процессу присваивается свой объём строк \code{block\_sizes}, при этом сохраняются смещения \code{displs}, необходимые для последующего сбора результата.

Каждый процесс получает только свою часть изображения через коллективную операцию рассылки данных \code{scatterv}, но для корректного выполнения свёртки с ядром Гаусса 3×3, дополнительно обменивается граничными halo-строками с соседними процессами:

\begin{itemize}
  \item Если процесс не первый, он получает верхнюю строку от предыдущего.
  \item Если процесс не последний, он получает нижнюю строку от следующего.
\end{itemize}

Обмен осуществляется через функции передачи сообщения \code{send} и их приема \code{recv}. Полученные дополнительные строки позволяют выполнять свёртку без выхода за границы локального фрагмента изображения.

Для распараллеливания используется директива \code{\#pragma omp parallel for schedule(static)}, которая обеспечивает равномерное распределение строк между потоками. Каждый поток обрабатывает свое локальное изображение в пределах основного блока. Расчёт нового значения пикселя производится путём взвешенного суммирования соседних значений согласно ядру Гаусса 3×3. 

После завершения фильтрации каждый процесс передаёт свою часть обработанного изображения обратно на основной процесс с помощью коллективной функции сбора данных \code{gatherv}, используя уже имеющиеся \code{block\_sizes} и \code{displs}. Это позволяет собрать финальный результат в массив \code{filtered\_image\_}.

Метод линейной фильтрации изображений с совместным использованием технологий MPI и OpenMP эффективно сочетает преимущества межпроцессного и многопоточного параллелизма. MPI позволяет распределить изображение по процессам, работающим на разных узлах, обеспечивая масштабируемость и ускорение при больших объёмах данных. Обмен граничными строками cохраняет корректность фильтрации на стыках блоков. Внутри каждого процесса используется OpenMP, что позволяет задействовать все доступные ядра узла без лишней синхронизации. Такой гибридный подход обеспечивает высокую производительность, хорошую масштабируемость и эффективное использование ресурсов как на уровне кластера, так и внутри отдельных узлов.


\newpage

\section{Результаты экспериментов}

\subsection{Тестовые данные и аппаратная среда}
Для оценки эффективности и корректности реализованных версий алгоритма линейной фильтрации изображений были проведены замеры производительности и точности результатов. Целью экспериментов являлось сравнение последовательной реализации с параллельными версиями, использующими все рассмотренные технологии.

Замеры производительности проводились на тестовом изображении размером 5~000~×~5~000 пикселей в формате RGB, предварительно преобразованное в градации серого. Для повышения точности оценки каждая конфигурация запускалась по 10 раз, после чего рассчитывалось среднее время выполнения.

Тестовая платформа имела следующие характеристики:
\begin{itemize}
  \item \textbf{Процессор:} 11th Gen Intel(R) Core(TM) i5-1135G7 (4 физических ядра, 8 потоков, 2.40 ГГц);
  \item \textbf{Оперативная память:} 8 ГБ DDR4 (3200 МГц);
  \item \textbf{Операционная система}: Windows 11 Pro x64;
  \item \textbf{Среда исполнения}: WSL2 (Ubuntu 24.04 LTS) с поддержкой OpenMP, MPI и TBB.
\end{itemize}

Измерения времени производились в двух режимах:
\begin{itemize}
  \item \textbf{PipelineRun} – полное время выполнения, включая валидацию данных, чтение, преобразование, вычисления и запись результата;
  \item \textbf{TaskRun} – только время выполнения ядра вычислений.
\end{itemize}

Помимо времени работы, также оценивалось качество фильтрации, заключающееся в сравнении выходных изображений, полученных различными параллельными реализациями и последовтальной версией. Это позволяло убедиться в корректности параллельных реализаций и идентичности их логики по  отношению к эталонной (последовательной) реализации.

\subsection{Ускорение и эффективность}
Для количественной оценки производительности параллельных реализаций алгоритма фильтрации изображений были рассчитаны следующие метрики:

\textbf{Ускорение (Speedup)} – показывает, во сколько раз параллельная версия работает быстрее последовательной:
\[
S = \frac{T_{seq}}{T_{par}}
\]

\textbf{Эффективность (Efficiency)} – отражает степень загрузки доступных вычислительных ресурсов ($N$ – количество задействованных потоков или процессов):
\[
E = \frac{S}{N} \times 100\%
\]

Была сформирована таблица для наглядного отображения производительности различных реализаций алгоритма параллельной фильтрации изображений. Она отражает влияние выбранной технологии параллелизма и числа задействованных потоков или процессов на общее время выполнения вычислительного ядра, а также количественную оценку эффективности использования ресурсов.

В таблице приведены результаты измерения времени вычисления (TaskRun) в секундах линейной фильтрации изображений с ядром Гаусса 3×3 для различных реализаций и конфигураций параллелизма. На основании этих данных рассчитаны средние значения ускорения (S) и эффективности (E).

\begin{center}
\renewcommand{\arraystretch}{2}
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Реализация} & \textbf{Конфигурация} & \textbf{TaskRun, с} & \textbf{S} & \textbf{E, \%} \\
\hline
Seq & – & 1.3942 & 1.0000 & 100.0000 \\
\hline
\multirow{3}{*}{OpenMP} & 3 потока & 0.7461 & 1.8687 & 62.2893 \\
& 5 потоков & 0.6014 & 2.3180 & 46.3609 \\
& 8 потоков & 0.6178 & 2.2567 & 28.2082 \\
\hline
\multirow{3}{*}{TBB} & 3 потока & 0.6747 & 2.0665 & 68.8830 \\
& 5 потоков & 0.5303 & 2.6290 & 52.5797 \\
& 8 потоков & 0.5829 & 2.3917 & 29.8969 \\
\hline
\multirow{3}{*}{std::thread} & 3 потока & 0.5867 & 2.3765 & 79.2150 \\
& 5 потоков & 0.4891 & 2.8503 & 57.0067 \\
& 8 потоков & 0.5793 & 2.4067 & 30.0839 \\
\hline
\multirow{9}{*}{\makecell{MPI\\+\\OpenMP}} & 4 процесса + 3 потока & 2.5593 & 0.5447 & 4.5396 \\
& 4 процесса + 5 потоков & 2.4878 & 0.5604 & 2.8021 \\
& 4 процесса + 8 потоков & 2.4932 & 0.5592 & 1.7474 \\
& 6 процессов + 3 потока & 2.6387 & 0.5284 & 2.9353 \\
& 6 процессов + 5 потоков & 2.5900 & 0.5383 & 1.7943 \\
& 6 процессов + 8 потоков & 2.6161 & 0.5329 & 1.1103 \\
& 8 процессов + 3 потока & 2.5528 & 0.5461 & 2.2756 \\
& 8 процессов + 5 потоков & 2.4797 & 0.5622 & 1.4056 \\
& 8 процессов + 8 потоков & 2.4586 & 0.5670 & 0.8860 \\
\hline
\end{tabular}
\caption{Сравнение производительности различных реализаций}
\end{table}

\end{center}

\newpage

\subsection{Выводы из результатов}
В ходе эксперимента были протестированы различные подходы к параллелизации: OpenMP, TBB, std::thread и гибридная модель MPI совместно OpenMP. Рассмотрим поведение каждого метода при увеличении числа потоков/процессов, с точки зрения ускорения (насколько быстрее работает параллельный код по сравнению с последовательным) и эффективности (насколько рационально используются доступные ресурсы).

\textbf{OpenMP} показал хорошую масштабируемость при умеренном числе потоков, однако его эффективность быстро снижается при увеличении количества потоков. Это обусловлено накладными расходами на синхронизацию и совместный доступ к памяти. OpenMP удобно использовать в задачах с простым разделением работы и минимальной межпоточной зависимостью. Тем не менее, при большом числе потоков отдача становится заметно ниже, что ограничивает применимость технологии для высокопараллельных задач на одном узле.

\textbf{TBB} также продемонстрировал устойчивое ускорение и приемлемую эффективность в различных конфигурациях. Особенно хорошо алгоритм работает при небольшом и среднем числе потоков, благодаря своей внутренней системе планирования задач, которая эффективно балансирует вычисления. Однако, как и другие средства, TBB сталкивается с падением эффективности при масштабировании.

Ручное управление потоками через \textbf{std::thread} обеспечило лучшие результаты как по ускорению, так и по эффективности. Это связано с тем, что разработчик может гибко управлять логикой распараллеливания и избегать излишних накладных расходов. Однако std::thread требует больше усилий при проектировании и отладке: ответственность за синхронизацию и корректное распределение задач ложится на программиста. Эта технология особенно эффективна при небольшом количестве потоков, соответствующем числу физических ядер.

Гибридный подход, сочетающий технологии \textbf{MPI + OpenMP} , не показал улучшения производительности в условиях одного вычислительного узла. Напротив, выполнение стало заметно медленнее из-за значительных накладных расходов на межпроцессные коммуникации и управление потоками внутри каждого процесса. Этот подход актуален в условиях многомашинных кластеров, где процессы располагаются на разных физических узлах. В однопроцессорной среде использование MPI в сочетании с OpenMP приводит к избыточной нагрузке и потере производительности.

Корректность всех реализованных версий алгоритма линейной фильтрации изображений проверялась с помощью автоматических тестов. Для каждого варианта реализации – последовательного и параллельных (с использованием OpenMP, TBB, std::thread и гибридной OpenMP совместно с MPI) проводилось сравнение результатов фильтрации на одинаковых входных данных. Тесты покрывали как корректную работу на больших изображениях, так и граничные случаи (например, изображения малого размера или блоки на краях).
Все эксперименты завершились успешно – расхождений в вычислениях не зафиксировано, что подтверждает корректность реализации алгоритма во всех рассмотренных параллельных вариантах.


\newpage

\section*{Заключение}
В рамках данной лабораторной работы была исследована задача линейной фильтрации изображений с применением ядра Гуасса размером 3×3. Реализация была выполнена как в последовательной форме, так и в виде нескольких параллельных вариантов с применением современных технологий: OpenMP, Intel TBB, стандартных потоков C++ (std::thread), а также гибридной модели MPI совместно с OpenMP. Для повышения эффективности кэширования и балансировки нагрузки использовался метод блочного разбиения изображения.

Экспериментальные измерения производительности показали, что параллельные реализации значительно превосходят последовательную по времени выполнения при обработке изображений большого размера. Наиболее высокие показатели ускорения и эффективности продемонстрировала реализация с использованием std::thread. Это объясняется как низким уровнем абстракции и, соответственно, минимальными накладными расходами, так и высокой гибкостью управления потоками. Также std::thread-реализация показала хорошую масштабируемость при увеличении количества потоков.

OpenMP продемонстрировал достойные результаты благодаря удобству реализации и автоматическому управлению потоками, что делает его особенно подходящим для быстрого прототипирования. TBB также показал хорошие показатели, особенно при высокой загрузке CPU, благодаря внутренней системе планирования задач. Гибридный подход MPI совместно с OpenMP, несмотря на свою сложность, оказался полезен в условиях потенциальной работы на многомашинных системах, но при запуске на одном узле уступил остальным технологиям по эффективности.

Корректность всех реализаций была подтверждена с помощью автоматических тестов, которые показали совпадение результатов.

Таким образом, поставленные цели лабораторной работы были достигнуты: реализованы и проанализированы различные подходы к линейной фильтрации изображений с ядром Гаусса 3×3, исследована производительность различных параллельных подходов, а также подтверждена корректность их функционирования.

\newpage

\section*{Список литературы}
\begin{enumerate}
  \item Воеводин В. В. Параллельные вычисления / Воеводин Вл. В. – СПб.: БХВ-Петербург, 2002. – 608 с.
  \item Немнюгин С. А. Параллельное программирование для многопроцессорных вычислительных систем / Стесик О. Л. – СПб.: БХВ-Петербург, 2002. – 397 с.
  \item Вильямс А. Системное программирование в Windows 2000 для профессионалов. – СПб.: БХВ-Петербург, 2001. – 624 с.
  \item Andrews, G. R. (2000). Foundations of Multithreaded, Parallel, and Distributed Programming. – Reading, MA: Addison-Wesley.
  \item Chandra, R., Dagum, L., Kohr, D., Maydan, D., McDonald, J., and Melon, R. (2000). Parallel Programming in OpenMP. San-Francisco, CA: Morgan Kaufmann Publishers.
  \item Butenhof, D. R. (1007) Programming with POSIX Threads. Boston, MA: Addison-Wesley Professional.
  \item Quinn, M. J. (2004). Parallel Programming in C with MPI and OpenMP. – New York, NY: McGraw-Hill.
\end{enumerate}

\newpage

\section*{Приложения}

\subsection*{Приложение A. Реализация последовательной версии}
\begin{lstlisting}
std::vector<double> kernel = {0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625};

for (int i = 1; i < height_ - 1; ++i) {
  for (int j = 1; j < width_ - 1; ++j) {
    double sum = 0;
    for (int l = -1; l <= 1; ++l) {
      for (int k = -1; k <= 1; ++k) {
        sum += image_[((i - l) * width_) + j - k] * kernel[((l + 1) * 3) + k + 1];
      }
    }
    filtered_image_[(i * width_) + j] = sum;
  }
}
\end{lstlisting}

\newpage

\subsection*{Приложение B. Реализация OpenMP-версии}
\begin{lstlisting}
std::vector<double> kernel = {0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625};

int block_size = 32;

int num_blocks_x = (width_ + block_size - 1) / block_size;
int num_blocks_y = (height_ + block_size - 1) / block_size;

#pragma omp parallel for schedule(dynamic, 1)
for (int block_y = 0; block_y < num_blocks_y; ++block_y) {
  for (int block_x = 0; block_x < num_blocks_x; ++block_x) {
    int start_i = block_y * block_size;
    int start_j = block_x * block_size;
    int end_i = std::min(start_i + block_size, height_ - 1);
    int end_j = std::min(start_j + block_size, width_ - 1);

    for (int i = std::max(1, start_i); i < end_i; ++i) {
      for (int j = std::max(1, start_j); j < end_j; ++j) {
        double sum = 0;
        for (int l = -1; l <= 1; ++l) {
          for (int k = -1; k <= 1; ++k) {
            sum += image_[((i - l) * width_) + (j - k)] * kernel[((l + 1) * 3) + (k + 1)];
          }
        }
        filtered_image_[(i * width_) + j] = sum;
      }
    }
  }
}
\end{lstlisting}

\newpage

\subsection*{Приложение C. Реализация TBB-версии}
\begin{lstlisting}
std::vector<double> kernel = {0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625};

void ProcessBlock(const std::vector<double> &image, std::vector<double> &filtered_image, int width, int height,
                  int start_i, int start_j, int block_size) {
  int end_i = std::min(start_i + block_size, height - 1);
  int end_j = std::min(start_j + block_size, width - 1);

  for (int i = std::max(1, start_i); i < end_i; ++i) {
    for (int j = std::max(1, start_j); j < end_j; ++j) {
      double sum = 0;
      for (int l = -1; l <= 1; ++l) {
        for (int k = -1; k <= 1; ++k) {
          sum += image[((i - l) * width) + (j - k)] * kernel[((l + 1) * 3) + (k + 1)];
        }
      }
      filtered_image[(i * width) + j] = sum;
    }
  }
}

int block_size = 64;

oneapi::tbb::task_arena arena(ppc::util::GetPPCNumThreads());
arena.execute([&] {
  tbb::task_group tg;
  for (int i = 0; i < height_; i += block_size) {
    for (int j = 0; j < width_; j += block_size) {
      tg.run([&, i, j] { ProcessBlock(image_, filtered_image_, width_, height_, i, j, block_size); });
    }
  }
  tg.wait();
});
\end{lstlisting}

\newpage

\subsection*{Приложение D. Реализация STL-версии}
\begin{lstlisting}
void ProcessBlock(const std::vector<double> &image, std::vector<double> &filtered_image, int width, int height,
                  int start_i, int start_j, int block_size) {
  int end_i = std::min(start_i + block_size, height - 1);
  int end_j = std::min(start_j + block_size, width - 1);
  for (int i = std::max(1, start_i); i < end_i; ++i) {
    for (int j = std::max(1, start_j); j < end_j; ++j) {
      double sum = 0;
      for (int l = -1; l <= 1; ++l) {
        for (int k = -1; k <= 1; ++k) {
          sum += image[((i - l) * width) + (j - k)] * kernel[((l + 1) * 3) + (k + 1)];
        }
      }
      filtered_image[(i * width) + j] = sum;
    }
  }
}
const int block_size = 64;
const int num_threads = ppc::util::GetPPCNumThreads();
std::vector<std::pair<int, int>> blocks;
for (int i = 0; i < height_; i += block_size) {
  for (int j = 0; j < width_; j += block_size) {
      blocks.emplace_back(i, j);
  }
}
std::atomic<int> block_id{0};
std::vector<std::thread> threads(num_threads);
for (int t = 0; t < num_threads; ++t) {
  threads[t] = std::thread([&]() {
    int id = 0;
    while ((id = block_id.fetch_add(1)) < static_cast<int>(blocks.size())) {
      const auto &[i, j] = blocks[id];
      ProcessBlock(image_, filtered_image_, width_, height_, i, j, block_size);
    }
  });
}
for (auto &t : threads) { t.join();}
\end{lstlisting}

\newpage

\subsection*{Приложение E. Реализация MPI + OpenMP-версии}
\begin{lstlisting}
broadcast(world_, width_, 0);
broadcast(world_, height_, 0);

int rank = world_.rank();
int num_procs = world_.size();

if (num_procs == 0) {
  return false;
}

std::vector<int> block_sizes(num_procs);
std::vector<int> displs(num_procs);

int remaining_blocks = height_;
int blocks_per_proc = height_ / num_procs;

block_sizes[0] = blocks_per_proc * width_;
displs[0] = 0;

for (int proc = 1; proc < num_procs; ++proc) {
  remaining_blocks -= blocks_per_proc;
  blocks_per_proc = remaining_blocks / (num_procs - proc);
  block_sizes[proc] = blocks_per_proc * width_;
  displs[proc] = displs[proc - 1] + block_sizes[proc - 1];
}

int local_rows = block_sizes[rank] / width_;

int halo_top = 0;
if (rank > 0) {
  halo_top = 1;
}

int halo_bottom = 0;
if (rank < num_procs - 1) {
  halo_bottom = 1;
}

int extended_rows = local_rows + halo_top + halo_bottom;
std::vector<double> local_image(extended_rows * width_);
scatterv(world_, image_.data(), block_sizes, displs, local_image.data() + (halo_top * width_), local_rows * width_,
           0);

if (halo_top != 0) {
  world_.send(rank - 1, 0, local_image.data() + (halo_top * width_), width_);
  world_.recv(rank - 1, 1, local_image.data(), width_);
}
if (halo_bottom != 0) {
  world_.send(rank + 1, 1, local_image.data() + ((halo_top + local_rows - 1) * width_), width_);
  world_.recv(rank + 1, 0, local_image.data() + ((halo_top + local_rows) * width_), width_);
}

std::vector<double> kernel = {0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625};

std::vector<double> local_filtered_image(local_rows * width_, 0);

#pragma omp parallel for schedule(static)
for (int i = 1; i < extended_rows - 1; ++i) {
  for (int j = 1; j < width_ - 1; ++j) {
    double sum = 0.0;
    for (int di = -1; di <= 1; ++di) {
      for (int dj = -1; dj <= 1; ++dj) {
        int ni = i + di;
        int nj = j + dj;
        sum += local_image[(ni * width_) + nj] * kernel[((di + 1) * 3) + (dj + 1)];
      }
    }
    if (i >= halo_top && i < halo_top + local_rows) {
      local_filtered_image[((i - halo_top) * width_) + j] = sum;
    }
  }
}

gatherv(world_, local_filtered_image.data(), local_rows * width_, filtered_image_.data(), block_sizes, displs, 0);
\end{lstlisting}

\end{document}
