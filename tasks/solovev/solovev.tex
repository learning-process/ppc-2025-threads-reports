\documentclass[12pt]{article}
\usepackage[russian]{babel}
\usepackage{graphicx} 
\usepackage{times}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{float}
\usepackage{multirow}

\lstdefinestyle{mystyle}{
    keywordstyle=\color{purple},   
    numberstyle=\tiny\color{gray},   
    stringstyle=\color{green},    
    basicstyle=\ttfamily\footnotesize, 
    breakatwhitespace=false, 
    breaklines=true,  
    captionpos=b,   
    keepspaces=true,  
    numbers=left,      
    showspaces=false,  
    showstringspaces=false,   
    tabsize=2          
}

\begin{document}
\begin{titlepage}
\begin{center}
\textbf{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ} \\
\textbf{Федеральное государственное автономное образовательное учреждение высшего образования \\ «Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»} \\
Институт информационных технологий, математики и механики \\
\vspace{3cm}
{\Large
\textbf{Отчёт по лабораторной работе:} \\[0.5cm]
\textbf{"Умножение разреженных матриц. Элементы комплексного типа. Формат хранения матрицы – столбцовый (CCS)"} \\
}
\vspace{2cm}
\begin{flushright}
\textbf{Выполнил:} студент группы 3822Б1ФИ1\\
Алексей Соловьев \\
\vspace{0.5cm}
\textbf{Преподаватель:} \\
доцент кафедры ВВСП, к.т.н \\ Сысоев А.В.
\end{flushright}
\vspace{2.5cm}
Нижний Новгород \\
2025
\end{center}
\end{titlepage}

\newpage
\section{Введение}

Разреженные матрицы (sparse matrices) широко применяются в различных областях науки и техники, включая численные методы решения дифференциальных уравнений, моделирование электрических цепей, анализ графов и квантовые вычисления. Важной особенностью таких матриц является то, что большинство их элементов равно нулю, что позволяет хранить и обрабатывать их значительно более эффективно, чем плотные структуры. В данной работе рассматривается умножение разреженных матриц с элементами комплексного типа, когда формат хранения матриц осуществляется по столбцам..

Использование комплексных элементов характерно для задач, связанных с анализом сигналов, квантовой механикой и обработкой изображений, где благодаря представлению данных в комплексной форме достигается большая выразительность и точность вычислений. Однако работа с комплексными разреженными матрицами накладывает дополнительные требования как на хранение данных, так и на алгоритмы умножения, поскольку необходимо учитывать арифметику с комплексными числами и эффективно обходить только ненулевые элементы.

В формате хранения по столбцам (CCS) матрица представляется тремя массивами:
\begin{itemize}
    \item \texttt{col\_ptr} длины $n+1$, где $n$~— число столбцов, и каждый элемент указывает на начало соответствующего столбца в массивах данных;
    \item \texttt{row\_ind} длины $N_{\text{nz}}$, где $N_{\text{nz}}$~— общее число ненулевых элементов, и каждый элемент хранит индекс строки ненулевого элемента;
    \item \texttt{values} длины $N_{\text{nz}}$, содержащий действительные или комплексные значения ненулевых элементов.
\end{itemize}
Такое представление позволяет за $O(1)$ получить доступ к ненулевым элементам любого столбца и удобно использовать в алгоритмах, где желательно «пробегать» матрицу по столбцам. В частности, при вычислении произведения матриц $C = A \times B$ формат CCS облегчает поиск ненулевых пар множителей, что является ключевым для эффективной реализации.

\section{Постановка задачи}

В рамках лабораторной работы необходимо:
\begin{enumerate}
    \item Изучить формат хранения разреженных матриц с комплексными элементами в формате CCS:
    \begin{itemize}
        \item \texttt{col\_ptr} (смещение столбцов);
        \item \texttt{row\_ind} (индексы строк ненулевых элементов);
        \item \texttt{values} (\texttt{std::complex<double>}).
    \end{itemize}

    \item Реализовать последовательную функцию умножения двух разреженных матриц \(A\) и \(B\):
    \[
        C = A \times B, \quad C_{ij} = \sum_{k} A_{ik} \cdot B_{kj},
    \]
    учитывая обход только ненулевых элементов и арифметику комплексных чисел.

    \item Реализовать параллельные версии той же функции:
    \begin{itemize}
        \item \textbf{OpenMP}: распараллелить по столбцам результирующей матрицы с помощью \texttt{\#pragma omp parallel for}.
        \item \textbf{Intel TBB}: использовать \texttt{tbb::parallel\_for} или \texttt{tbb::blocked\_range} для распределения столбцов.
        \item \textbf{std::threads}: создать потоки (\texttt{std::thread}), распределить столбцы между воркерами, при необходимости использовать \texttt{std::mutex} для защиты общих данных.
        \item \textbf{MPI + std::threads}: разбить результат по блокам столбцов между MPI-процессами, внутри каждого процесса распараллелить работу через \texttt{std::threads}, обмен данными через MPI.
    \end{itemize}

    \item Провести замеры времени выполнения последовательной и параллельных версий и сравнить ускорение:
    \[
        \text{Speedup}(p) = \frac{T_{\text{послед.}}}{T_{\text{паралл.}}(p)}.
    \]
\end{enumerate}

\section{Описание алгоритма}

Алгоритм умножения матриц \(A\in\mathbb{C}^{r\times m}\) и \(B\in\mathbb{C}^{m\times c}\) в формате CCS для вычисления \(C = A \times B \in \mathbb{C}^{r\times c}\) выглядит следующим образом:

\begin{enumerate}
    \item \textbf{Инициализация}
    \begin{itemize}
        \item Задаём пустой вектор \texttt{col\_p\_C} длины \(c+1\) с \(\texttt{col\_p\_C}[0] = 0\).
        \item Матрица \(C\) будет размера \(r \times c\).
    \end{itemize}

    \item \textbf{Подсчёт числа ненулевых элементов (nnz) по столбцам \(C\)}
    \begin{itemize}
        \item Для каждого столбца \(j = 0, \dots, c-1\) матрицы \(B\):
        \begin{enumerate}
            \item Создаём временный булев массив \(\texttt{mark}[0..r-1]\), инициализируем нулями.
            \item Для каждого ненулевого элемента \(B(k,j)\), где \(k = \texttt{row\_B}[i]\), \(i \in [\,\texttt{col\_p\_B}[j],\,\texttt{col\_p\_B}[j+1]-1]\):
            \begin{itemize}
                \item Проходим по столбцу \(A(:,k)\). Для каждого ненулевого \(A(p,k)\), \(p = \texttt{row\_A}[q]\), устанавливаем \(\texttt{mark}[p] = 1\).
            \end{itemize}
            \item Считаем \(\displaystyle \text{count} = \sum_{p=0}^{r-1} \texttt{mark}[p]\). Тогда
            \[
                \texttt{col\_p\_C}[\,j+1\,] = \texttt{col\_p\_C}[\,j\,] + \text{count}.
            \]
        \end{enumerate}
    \end{itemize}

    \item \textbf{Выделение памяти для \(\texttt{row\_C}\) и \(\texttt{val\_C}\)}
    \begin{itemize}
        \item Общее число ненулевых элементов в \(C\) равно \(\texttt{col\_p\_C}[c]\).
        \item Создаём массивы \(\texttt{row\_C}, \texttt{val\_C}\) длины \(\texttt{col\_p\_C}[c]\).
    \end{itemize}

    \item \textbf{Заполнение ненулевых значений \(C\)}
    \begin{itemize}
        \item Для каждого столбца \(j = 0, \dots, c-1\):
        \begin{enumerate}
            \item Инициализируем два массива длины \(r\):
            \[
                \texttt{accum}[0..r-1] = 0 \quad (\text{тип } \texttt{std::complex<double>}), \qquad
                \texttt{mark}[0..r-1] = 0.
            \]
            \item Для каждого ненулевого \(B(k,j)\) со значением \(\beta = \texttt{val\_B}[i]\), где \(k = \texttt{row\_B}[i]\):
            \begin{itemize}
                \item Проходим по столбцу \(A(:,k)\). Пусть \(A(p,k) = \alpha = \texttt{val\_A}[q]\), \(p = \texttt{row\_A}[q]\). Тогда
                \[
                    \texttt{accum}[p] \;\mathrel{+}= \;\alpha \times \beta, \quad \texttt{mark}[p] = 1.
                \]
            \end{itemize}
            \item После этого проходим \(p = 0, \dots, r-1\). Если \(\texttt{mark}[p] = 1\), заносим:
            \[
                \texttt{row\_C}[\,\text{pos}\,] = p,\quad
                \texttt{val\_C}[\,\text{pos}\,] = \texttt{accum}[p],
            \]
            где \(\text{pos}\) пробегает индексы от \(\texttt{col\_p\_C}[j]\) до \(\texttt{col\_p\_C}[j+1]-1\).
        \end{enumerate}
    \end{itemize}
\end{enumerate}

В результате получаем матрицу \(C\) в формате CCS.

\section{Описание схемы параллельного алгоритма (CCS)}

Умножение двух разреженных комплексных матриц \(A\) размером \(r \times m\) и \(B\) размером \(m \times c\) в формате CCS допускает эффективную параллельную обработку: каждый столбец результирующей матрицы \(C = A \times B\) можно вычислять независимо.

\begin{enumerate}
    \item \textbf{Подготовка структур данных.}
    \begin{itemize}
        \item Матрицы \(A\) и \(B\) хранятся в CCS-формате, то есть каждой соответствует тройка массивов:
        \[
            \texttt{col\_ptr},\quad 
            \texttt{row},\quad 
            \texttt{val},
        \]
        где \(\texttt{col\_ptr}[\,j\,]\) указывает на начало \(j\)-го столбца, а \(\texttt{row}\) и \(\texttt{val}\) хранят индексы строк и комплексные значения ненулевых элементов.
        \item Для быстрого доступа к строкам матрицы \(A\) строим её транспонированную копию \(A^T\) также в формате CCS. Тогда столбцы \(A^T\) соответствуют строкам \(A\).
    \end{itemize}

    \item \textbf{Разбиение работы по столбцам.}
    \begin{itemize}
        \item Пусть доступно \(P\) потоков. Разделим индексы столбцов \(\{0,1,\dots,c-1\}\) на \(P\) примерно равных диапазонов:
        \[
            \text{диапазон}_t = \bigl[L_t,\,R_t\bigr),\quad
            L_t = \Bigl\lfloor \tfrac{t\,c}{P}\Bigr\rfloor,\quad
            R_t = \Bigl\lfloor \tfrac{(t+1)\,c}{P}\Bigr\rfloor,\quad
            t=0,\dots,P-1.
        \]
        \item Каждый поток \(t\) обрабатывает свои столбцы \(j\in [L_t,\,R_t)\) независимо.
    \end{itemize}

    \item \textbf{Локальный подсчёт числа ненулевых элементов.}
    \begin{itemize}
        \item Для каждого столбца \(j\in [L_t,\,R_t)\) поток создаёт булевый массив
        \[
            \texttt{mark}[\,0..\,r-1\,]=0,
        \]
        указывающий, какие строки в столбце \(j\) результирующей матрицы \(C\) будут ненулевыми.
        \item Перебирая все ненулевые элементы \(B(k,j)\) (индексы \(k\) и значения \(\beta\)), поток «заходит» в соответствующий столбец \(k\) матрицы \(A\) (через массивы транспонированной \(A^T\)). Для каждого ненулевого \(A(p,k)\) отмечает \(\texttt{mark}[\,p\,]=1\).
        \item После этого подсчитывает 
        \[
            \text{count}_j = \sum_{p=0}^{r-1} \texttt{mark}[\,p\,],
        \]
        и сохраняет \(\texttt{col\_p\_local}[\,j - L_t + 1\,] = \text{col\_p\_local}[\,j - L_t\,] + \text{count}_j\).
        \item По завершении всех \(j\) в своём диапазоне поток знает точное число ненулевых элементов \(nnz_t\) в своих столбцах:
        \[
            nnz_t = \texttt{col\_p\_local}[\,R_t - L_t\,].
        \]
        \item Поток выделяет локальные буферы
        \[
            \texttt{row\_local}_t[\,0..\,(nnz_t-1)\,],\quad
            \texttt{val\_local}_t[\,0..\,(nnz_t-1)\,].
        \]
    \end{itemize}

    \item \textbf{Параллельное заполнение значений.}
    \begin{enumerate}
        \item Для каждого \(j\in [L_t,\,R_t)\) поток снова создаёт массивы
        \[
            \texttt{accum}[\,0..\,r-1\,]=0_{\mathbb{C}},\quad
            \texttt{mark}[\,0..\,r-1\,]=0.
        \]
        \item Перебирает все ненулевые \(B(k,j)\):
        \begin{itemize}
            \item Для каждого \(k\) «заходит» в столбец \(k\) матрицы \(A\) (через \(A^T\)): перебирает ненулевые \(A(p,k)\) и выполняет
            \[
                \texttt{accum}[\,p\,] \;+=\; A(p,k)\times B(k,j),\quad
                \texttt{mark}[\,p\,]=1.
            \]
        \end{itemize}
        \item После этого поток формирует ненулевые элементы столбца \(j\). Начальная позиция в локальных массивах:
        \[
            \text{pos} \;=\; \texttt{col\_p\_local}[\,j - L_t\,].
        \]
        Перебирает \(p=0,\dots,r-1\): если \(\texttt{mark}[p]=1\), то
        \[
            \texttt{row\_local}_t[\text{pos}] = p,\quad
            \texttt{val\_local}_t[\text{pos}] = \texttt{accum}[p],\quad
            \text{pos}++.
        \]
    \end{enumerate}

    \item \textbf{Объединение локальных буферов в итоговую CCS-структуру.}
    \begin{enumerate}
        \item После завершения всех потоков (барьер либо \texttt{join}) главная нить собирает глобальный массив \(\texttt{col\_p\_C}[\,0..\,c\,]\). Для каждого \(j\) суммирует вклады из всех \(\texttt{col\_p\_local}^{(t)}\):
        \[
            \texttt{col\_p\_C}[j+1]
            =
            \texttt{col\_p\_C}[j]
            +
            \sum_{\substack{t:\\j\in[L_t,R_t)}} 
            \bigl(\texttt{col\_p\_local}^{(t)}[\,j - L_t + 1\,] 
            - \texttt{col\_p\_local}^{(t)}[\,j - L_t\,]\bigr).
        \]
        \item После вычисления \(\texttt{col\_p\_C}[\,c\,]\) выделяет глобальные массивы
        \[
            \texttt{row\_C}[\,0..\,(\texttt{col\_p\_C}[c]-1)\,],\quad
            \texttt{val\_C}[\,0..\,(\texttt{col\_p\_C}[c]-1)\,].
        \]
        \item Для каждого потока \(t\) последовательно копирует
        \[
            \texttt{row\_local}^{(t)},\quad
            \texttt{val\_local}^{(t)}
        \]
        в соответствующие отрезки глобальных массивов, зная смещения по числу ненулевых элементов в предыдущих диапазонах.
    \end{enumerate}
\end{enumerate}

\newpage
\textbf{Преимущества схемы}:
\begin{itemize}
    \item Каждый поток обрабатывает независимый набор столбцов, что устраняет гонки при вычислении.
    \item Локальные буферы \(\texttt{row\_local}_t\) и \(\texttt{val\_local}_t\) исключают необходимость синхронизации при записи результатов.
    \item Транспонирование матрицы \(A\) даёт быстрый доступ к её строкам через CCS-формат, без дополнительных поисковых операций.
    \item Схема масштабируется на большое число потоков и подходит для матриц с высокой разреженностью.
\end{itemize}

\section{Описание OMP-алгоритма}

Ниже приведено описание последовательных и параллельных этапов алгоритма умножения двух разреженных матриц \(M_1\) и \(M_2\) с комплексными элементами в формате CCS, реализованного с помощью OpenMP:

\begin{enumerate}
    \item \textbf{Инициализация и подсчёт смещений (\texttt{col\_p}) последовательно.}
    \begin{itemize}
        \item Размеры результирующей матрицы \(M_3\) задаются как
        \[
            M_3.r\_n = M_1.r\_n,\qquad
            M_3.c\_n = M_2.c\_n.
        \]
        \item Массив смещений столбцов \texttt{col\_p} (нулевой элемент равен 0) выделяется длины \(M_3.c\_n + 1\):
        \[
            M_3.col\_p.\text{resize}(M_3.c\_n + 1),\quad M_3.col\_p[0] = 0.
        \]
        \item Для каждого столбца \(j = 0,1,\dots,M_2.c\_n - 1\) выполняется:
        \begin{enumerate}
            \item Создаётся временный булевый вектор
            \[
                \texttt{available\_el}[\,0 \dots M_3.r\_n-1\,] = 0,
            \]
            который отмечает потенциально ненулевые строки результирующего столбца.
            \item Перебираются все ненулевые элементы столбца \(j\) матрицы \(M_2\). Пусть \(k = M_2.row[i]\), \(i \in [\,M_2.col\_p[j] \dots M_2.col\_p[j+1] - 1\,]\). Для каждого такого \(k\) перебираются ненулевые элементы столбца \(k\) в матрице \(M_1\): \(p = M_1.row[q]\),  \(q \in [\,M_1.col\_p[k] \dots M_1.col\_p[k+1] - 1\,]\). Для каждого \(p\) устанавливается
            \[
                \texttt{available\_el}[\,p\,] = 1.
            \]
            \item После обработки всех \(k\) подсчитывается
            \[
                n\_z\_cnt = \sum_{p=0}^{M_3.r\_n - 1} \texttt{available\_el}[\,p\,].
            \]
            Это число равно количеству ненулевых элементов в \(j\)-м столбце результата. Тогда
            \[
                M_3.col\_p[j+1] = M_3.col\_p[j] + n\_z\_cnt.
            \]
        \end{enumerate}
        \item После завершения цикла по всем столбцам общее число ненулевых элементов
        \[
            M_3.n\_z = M_3.col\_p[M_3.c\_n].
        \]
        Выделяются глобальные массивы нужного размера:
        \[
            M_3.row.\text{resize}(M_3.n\_z), \quad
            M_3.val.\text{resize}(M_3.n\_z).
        \]
    \end{itemize}

    \item \textbf{Параллельная фаза вычисления значений через OpenMP.}
    \begin{itemize}
        \item С помощью директивы
        \[
            \verb|#pragma omp parallel for|
        \]
        распараллеливается внешний цикл по \(j\)-м столбцам \(M_2\):
        \[
            \verb|for (int j = 0; j < M_3.c\_n; ++j) \{|
        \]
        Каждый поток обрабатывает свой набор индексов \(j\) независимо.
        \item Внутри каждой итерации \(j\) создаются два локальных буфера:
        \[
            \texttt{cask}[\,0 \dots M_3.r\_n - 1\,] = 0_{\mathbb{C}},\quad
            \texttt{available\_el\_local}[\,0 \dots M_3.r\_n - 1\,] = 0,
        \]
        где \texttt{cask} накапливает сумму
        \(\sum_k M_1(p,k)\times M_2(k,j)\) для каждой строки \(p\), а \texttt{available\_el\_local} помечает ненулевые строки.
        \item Перебираются все ненулевые \(M_2(k,j)\):
        \[
            k = M_2.row[i],\quad \beta = M_2.val[i],\quad
            i \in [\,M_2.col\_p[j] \dots M_2.col\_p[j+1]-1\,].
        \]
        Для каждого такого \(k\) перебираются ненулевые элементы столбца \(k\) матрицы \(M_1\): \(p = M_1.row[q]\),  \(\alpha = M_1.val[q]\),  \(q \in [\,M_1.col\_p[k] \dots M_1.col\_p[k+1]-1\,]\). Тогда выполняется
        \[
            \texttt{cask}[\,p\,] \;+\!=\; \alpha \times \beta,\quad
            \texttt{available\_el\_local}[\,p\,] = 1.
        \]
        \item После накопления всех вкладов для данного \(j\) вычисляется начальная позиция записи:
        \[
            \text{pos} = M_3.col\_p[j].
        \]
        Далее для каждой строки \(p = 0,1,\dots,M_3.r\_n-1\) проверяется:
        \[
            \text{если } \texttt{available\_el\_local}[\,p\,] == 1,\quad \text{то}
        \]
        \[
            M_3.row[\text{pos}] = p,\quad
            M_3.val[\text{pos}] = \texttt{cask}[\,p\,],\quad
            \text{pos}++.
        \]
        Таким образом реализуется запись ненулевых элементов столбца \(j\) результата в глобальные массивы \(M_3.row\) и \(M_3.val\). Поскольку участки \(\bigl[M_3.col\_p[j]\,..\,M_3.col\_p[j+1]-1\bigr]\) не пересекаются для разных \(j\), конфликта записи нет.
    \end{itemize}

    \item \textbf{Завершение и возвращение результата.}  
    Метод \verb|RunImpl()| возвращает \verb|true| после заполнения всех столбцов.
\end{enumerate}

\subsection*{Ключевые особенности OMP-версии}

\begin{itemize}
    \item \textbf{Предварительный подсчёт nnz последовательно.}  
    Массив смещений \texttt{col\_p} формируется до параллельной фазы, чтобы каждая нить знала, куда записывать свои результаты без синхронизации.
    \item \textbf{Распараллеливание по столбцам.}  
    Директива \verb|#pragma omp parallel for| автоматически распределяет столбцы между потоками.
    \item \textbf{Локальные буферы \texttt{cask} и \texttt{available\_el\_local}.}  
    Каждый поток использует собственные векторы для накопления частичных сумм и флагов, что исключает гонки данных при вычислении.
    \item \textbf{Отсутствие гонок при записи результата.}  
    Смещения \(\texttt{col\_p}[j]\) заранее заданы и не пересекаются, поэтому потоки могут одновременно записывать в \(\texttt{M\_3.row}\) и \(\texttt{M\_3.val}\) без дополнительных блокировок.
    \item \textbf{Корректность арифметики комплексных чисел.}  
    Для накопления используется тип \(\texttt{std::complex<double>}\), что обеспечивает правильное вычисление вещественной и мнимой части.
\end{itemize}

\section{Описание TBB-алгоритма}

В данной реализации используется библиотека Intel Threading Building Blocks (TBB) для параллельного перемножения разреженных комплексных матриц \(M_1\) и \(M_2\) в формате CCS. Основной метод  разбит на три ключевых этапа:

\begin{enumerate}
  \item \textbf{Инициализация.}
  \begin{itemize}
    \item Задаются размеры результирующей матрицы:
    \[
      M_3.r\_n = M_1.r\_n,\quad
      M_3.c\_n = M_2.c\_n.
    \]
    \item Массив смещений столбцов \texttt{M3\_}\texttt{.col\_p} выделяется длиной \(M_3.c\_n + 1\) и инициализируется:
    \[
      M_3.col\_p[0] = 0.
    \]
  \end{itemize}

  \item \textbf{Параллельное вычисление количества ненулевых элементов в столбцах.}

  Метод \texttt{ComputeColumnSizes()} запускает параллельный цикл с помощью \texttt{tbb::parallel\_for}, в котором каждый поток независимо обрабатывает диапазон столбцов \(j \in [0, M_3.c\_n)\):

  \begin{itemize}
    \item Для каждого столбца создаётся локальный вектор \texttt{available\_el} длиной \(M_3.r\_n\), заполненный нулями.
    \item Внутренние вложенные циклы перебирают пары элементов \(M_1(p,k)\) и \(M_2(k,j)\) и отмечают строки \(p\), в которых может быть ненулевое значение:
    \[
      \texttt{available\_el}[p] = 1.
    \]
    \item После обработки всех \(k\) подсчитывается число ненулевых строк в столбце \(j\):
    \[
      M_3.col\_p[j+1] = \sum_{p=0}^{M_3.r\_n - 1} \texttt{available\_el}[p].
    \]
  \end{itemize}

  После завершения \texttt{ComputeColumnSizes()} выполняется последовательное накопление префиксной суммы по массиву \texttt{col\_p}, чтобы определить начальные позиции записи в глобальные массивы:

  \[
    \texttt{for } i = 1 \texttt{ to } M_3.c\_n: \quad
    M_3.col\_p[i] += M_3.col\_p[i-1].
  \]

  Также вычисляется общее количество ненулевых элементов и выделяются соответствующие массивы:
  \[
    M_3.n\_z = M_3.col\_p[M_3.c\_n], \quad
    M_3.row, M_3.val \text{ длины } M_3.n\_z.
  \]

  \item \textbf{Параллельное вычисление значений элементов результирующей матрицы.}

  Метод \texttt{FillMatrixValues()} также использует \texttt{tbb::parallel\_for} для параллельной обработки каждого столбца:

  \begin{itemize}
    \item Создаются два локальных массива:
    \begin{itemize}
      \item \texttt{available\_el} — вектор-маска занятых строк.
      \item \texttt{cask} — вектор-аккумулятор комплексных значений.
    \end{itemize}
    \item Для каждой пары \(M_1(p,k), M_2(k,j)\) вычисляется вклад в результирующее значение:
    \[
      \texttt{cask}[p] += M_1(p,k) \cdot M_2(k,j).
    \]
    \item После накопления всех вкладов происходит запись результата в глобальные массивы \(M_3.row\) и \(M_3.val\), начиная с позиции:
    \[
      \texttt{c\_pos} = M_3.col\_p[j].
    \]
    Запись осуществляется без синхронизации, так как диапазоны не пересекаются между потоками.
  \end{itemize}
\end{enumerate}

\subsection*{Ключевые особенности TBB-версии}

\begin{itemize}
  \item \textbf{Декомпозиция по столбцам.}  
  Оба этапа (подсчёт размеров и заполнение значений) используют \texttt{tbb::parallel\_for} с диапазоном \texttt{tbb::blocked\_range<int>}, что обеспечивает эффективное распределение нагрузки между потоками.
  
  \item \textbf{Локальные структуры для каждого потока.}  
  Использование локальных массивов \texttt{available\_el} и \texttt{cask} предотвращает гонки данных.
  
  \item \textbf{Безопасная запись результатов.}  
  Поскольку каждый поток работает с выделенным диапазоном \texttt{col\_p[j] .. col\_p[j+1]}, не требуется блокировка при записи в глобальные массивы.

  \item \textbf{Префиксная сумма выполняется последовательно.}  
  После параллельного этапа подсчёта ненулевых элементов производится последовательное накопление значений в \texttt{col\_p}, как и в OpenMP-версии.
\end{itemize}

\section{Описание STL-алгоритма}

STL-реализация использует стандартные средства многопоточности C++: \texttt{std::thread}, \texttt{std::mutex}, \texttt{std::condition\_variable}, а также атомарные переменные и TLS (thread-local storage). Алгоритм реализован в два этапа и выполняется с участием пула потоков.

\subsection*{Общий алгоритм}

\begin{enumerate}
  \item Проверяется наличие входных и выходной матриц.
  \item Инициализируются размеры результирующей матрицы:
  \[
    M_3.r\_n = M_1.r\_n, \quad
    M_3.c\_n = M_2.c\_n.
  \]
  \item Вектор \texttt{counts\_} используется для хранения количества ненулевых элементов в каждом столбце \(j\) результирующей матрицы.
  \item Выполняется \textbf{первая фаза} — подсчёт количества ненулевых элементов.
  \item На основе \texttt{counts\_} формируется массив \texttt{M3->col\_p} (префиксная сумма).
  \item Выделяются глобальные массивы \texttt{row} и \texttt{val} под ненулевые элементы.
  \item Выполняется \textbf{вторая фаза} — вычисление значений.
\end{enumerate}

\subsection*{Детали реализации фаз}

Каждая фаза реализована с помощью общего цикла \texttt{WorkerLoop()}, который запускается в отдельных потоках.

\subsubsection*{Фаза 1: \texttt{ProcessPhase1()}}

Для каждого столбца \(j\):
\begin{itemize}
  \item Просматриваются все ненулевые элементы столбца \(j\) матрицы \(M_2\).
  \item Для каждой пары \(M_1(p,k), M_2(k,j)\) устанавливается флаг наличия результата в строке \(p\).
  \item Вектор \texttt{available} используется для учёта уникальных строк \(p\), в которых могут появиться ненулевые значения.
  \item Считается количество таких строк:
  \[
    \texttt{counts\_}[j] = \sum_{p=0}^{r\_n - 1} \texttt{available}[p].
  \]
\end{itemize}

\subsubsection*{Фаза 2: \texttt{ProcessPhase2()}}

Для каждого столбца \(j\):
\begin{itemize}
  \item Используется локальный вектор \texttt{cask} для накопления значений:
  \[
    \texttt{cask}[p] += M_1(p,k) \cdot M_2(k,j).
  \]
  \item После завершения умножений результирующие ненулевые значения записываются в массивы:
  \[
    M_3.row[\cdot],\quad M_3.val[\cdot].
  \]
  Индекс начала записи берётся из \texttt{M3->col\_p[j]}.
\end{itemize}

\subsection*{Параллелизм и синхронизация}

\begin{itemize}
  \item Используются атомарные переменные \texttt{next\_col\_} (индекс следующего столбца) и \texttt{completed\_} (счётчик завершённых задач).
  \item Каждому потоку предоставляется уникальный столбец (через \texttt{fetch\_add()}).
  \item По завершении своей задачи поток вызывает \texttt{NotifyCompletion()}, которая увеличивает счётчик и пробуждает главную нить при достижении полной готовности.
  \item Используется \texttt{std::condition\_variable} для ожидания завершения всех потоков в фазе.
\end{itemize}

\subsection*{Ключевые особенности реализации}

\begin{itemize}
  \item Чёткое разделение на две фазы — позволяет использовать оптимизированные буферы и избежать лишних аллокаций между фазами.
  \item Каждый поток обрабатывает только один столбец, что исключает гонки данных при накоплении и записи результатов.
  \item Эффективное использование \texttt{thread\_local} переменных (\texttt{available} и \texttt{cask}) для избежания частых аллокаций внутри цикла.
  \item Атомарные счётчики \texttt{next\_col\_} и \texttt{completed\_} обеспечивают корректное распределение задач и синхронизацию без блокировок.
  \item Использование \texttt{std::condition\_variable} позволяет главной нити ждать завершения всех потоков перед переходом между фазами.
  \item Механизм префиксной суммы на основе \texttt{counts\_} гарантирует, что каждому столбцу выделяется непрерывный сегмент в массиве \texttt{M3->row} и \texttt{M3->val}.
\end{itemize}

\section{Описание гибридного алгоритма (MPI + STL)}

Гибридная реализация объединяет распределённую обработку с помощью MPI и многопоточность через стандартные потоки C++ (\texttt{std::thread}). Алгоритм умножения разреженных матриц \(M_1\) и \(M_2\) в формате CCS состоит из следующих этапов:

\begin{enumerate}
    \item \textbf{Инициализация и широковещательная рассылка матриц.}
    \begin{itemize}
        \item Процесс с рангом 0 рассылает входные матрицы \(M_1\) и \(M_2\) всем MPI-процессам с помощью \texttt{boost::mpi::broadcast}.
        \item Каждый процесс инициализирует результирующую матрицу \(M_3\) как пустую CCS-структуру размером \(M_1.r\_n \times M_2.c\_n\).
    \end{itemize}

    \item \textbf{Разбиение столбцов между MPI-процессами.}
    
    \texttt{ComputeColumnRange(rank, size, total\_cols, start\_col, end\_col)} вычисляет для данного процесса (с рангом \(\text{rank}\) среди \(\text{size}\) процессов) диапазон столбцов \([\text{start\_col},\,\text{end\_col})\) так, чтобы:
    \[
        \begin{aligned}
            \text{cols\_per\_proc} &= \left\lfloor \tfrac{\text{total\_cols}}{\text{size}} \right\rfloor, \\
            \text{remaining}    &= \text{total\_cols} \bmod \text{size}, \\
            \text{start\_col}   &= \text{rank} \times \text{cols\_per\_proc} + \min(\text{rank},\,\text{remaining}), \\
            \text{extra\_col}   &= (\text{rank} < \text{remaining}) ? 1 : 0, \\
            \text{end\_col}     &= \text{start\_col} + \text{cols\_per\_proc} + \text{extra\_col}.
        \end{aligned}
    \]
    Если \(\text{start\_col} \ge \text{total\_cols}\) или \(\text{end\_col} > \text{total\_cols}\), оба устанавливаются равными \(\text{total\_cols}\).

    \item \textbf{Локальные вычисления внутри каждого MPI-процесса.}
    
    Для столбцов \(j = \text{start\_col}, \dots, \text{end\_col} - 1\) каждый процесс формирует вектор индексов \(\texttt{col\_indices}[0..\text{num\_cols}-1]\). Далее запускается одна из двух локальных процедур:
    \begin{itemize}
        \item \emph{Последовательное вычисление} (\texttt{ComputeSequential}) если \(\text{num\_cols} < 2\times\text{num\_threads}\).
        \item \emph{Параллельное вычисление} (\texttt{ComputeParallel}) с \(T = \min(\text{num\_cols},\,\text{num\_threads\_max})\) потоками.
    \end{itemize}
    
    \subitem Для каждого столбца \(j\) вызывается функция \(\texttt{ComputeColumn}(j,\;\texttt{column\_data})\), которая:
    \begin{enumerate}
        \item Сканирует диапазон \([\;M_2.col\_p[j],\,M_2.col\_p[j+1]-1\;]\), находит ненулевые элементы \(M_2(k,j)\).
        \item Для каждой пары \((M_1(p,k),\,M_2(k,j))\) аккумулирует сумму
        \[
            \text{sum} = \sum_{k} M_1(p,k)\times M_2(k,j)
        \]
        по всем \(p = 0,\dots,M_1.r\_n-1\). 
        \item Если полученная комплексная сумма \(\text{sum}\neq 0\) (с точностью \(\epsilon\)), добавляет пару \((\text{sum},\,p)\) в локальный вектор \(\texttt{column\_data}\).
    \end{enumerate}
    В результате в каждом процессе получается \(\text{column\_results}[0..\text{num\_cols}-1]\), где 
    \[
        \texttt{column\_results}[i] = \{ (M_3(p,\,j),\,p) \mid j = \texttt{col\_indices}[i]\}.
    \]
    
    \item \textbf{Формирование локальной CCS-структуры.}
    
    \texttt{FillLocalData(column\_results,\,local\_val,\,local\_row,\,local\_col\_p,\,local\_n\_z)}:
    \begin{itemize}
        \item Обходит каждый \(\texttt{column\_results}[i]\) и последовательно добавляет все пары \((\text{value},\,\text{row})\) в векторы \(\texttt{local\_val}\) и \(\texttt{local\_row}\).
        \item Обновляет локальный вектор смещений \(\texttt{local\_col\_p}[0..\text{num\_cols}]\) по кумулятивной сумме длин каждой \(\texttt{column\_results}[i]\).
        \item В итоге \(\texttt{local\_n\_z}\) содержит общее число ненулевых элементов, а \((\texttt{local\_col\_p},\,\texttt{local\_row},\,\texttt{local\_val})\) — корректную CCS-структуру для столбцов \([\text{start\_col},\,\text{end\_col})\).
    \end{itemize}

    \item \textbf{Обмен количества ненулевых элементов.}
    
    Каждый процесс отправляет \(\texttt{local\_n\_z}\) всем остальным через \texttt{boost::mpi::all\_gather}, получая вектор \(\texttt{all\_n\_z}[0..\text{size}-1]\). На его основе вычисляется общее \(\text{total\_n\_z} = \sum_{p=0}^{\text{size}-1} \texttt{all\_n\_z}[p]\) и смещения \(\texttt{displs}[0..\text{size}-1]\), где
    \[
        \texttt{displs}[0] = 0,\quad
        \texttt{displs}[i] = \sum_{p=0}^{i-1} \texttt{all\_n\_z}[p].
    \]

    \item \textbf{Сборка глобальных массивов CCS через MPI-Gatherv.}
    \begin{itemize}
        \item Глобальные массивы \(\texttt{M3.val}[0..\text{total\_n\_z}-1]\) и \(\texttt{M3.row}[0..\text{total\_n\_z}-1]\) расширяются до нужного размера.
        \item С помощью \texttt{boost::mpi::all\_gatherv} локальные \(\texttt{local\_val}\) и \(\texttt{local\_row}\) собираются в \(\texttt{M3.val}\) и \(\texttt{M3.row}\) с указанием \(\texttt{all\_n\_z}\) и \(\texttt{displs}\).
    \end{itemize}

    \item \textbf{Сборка глобального массива смещений \(\texttt{M3.col\_p}\).}
    \begin{enumerate}
        \item Каждый процесс заполняет \(\texttt{local\_col\_counts}[0..\text{total\_cols}-1]\), где
        \[
            \texttt{local\_col\_counts}[j] = \bigl|\texttt{column\_results}[j - \text{start\_col}]\bigr|
        \quad \text{для } j \in [\text{start\_col},\,\text{end\_col}).
        \]
        \item Через \texttt{boost::mpi::reduce} (операция \(\texttt{std::plus<>}\)) на процесс 0 собирается \(\texttt{global\_col\_counts}[0..\text{total\_cols}-1]\).
        \item На процессе 0 выполняется префиксная сумма:
        \[
            \texttt{M3.col\_p}[0] = 0,\quad
            \texttt{M3.col\_p}[j+1] = \texttt{M3.col\_p}[j] + \texttt{global\_col\_counts}[j],\quad
            j = 0,\dots,\text{total\_cols}-1.
        \]
        \item Процесс 0 транслирует \(\texttt{M3.col\_p}\) всем остальным через \texttt{boost::mpi::broadcast}.
    \end{enumerate}
    
    \item \textbf{Завершение.}  
    После приёма \(\texttt{M3.col\_p}\) все процессы имеют полную CCS-структуру результирующей матрицы \(M_3\). Метод \texttt{RunImpl()} возвращает \texttt{true}.
\end{enumerate}

\subsection*{Ключевые особенности реализации}

\begin{itemize}
    \item \textbf{Двухуровневый параллелизм.}  
    На уровне MPI столбцы \(\{0,\dots,M_2.c\_n - 1\}\) распределяются между процессами. Внутри каждого процесса для своих столбцов используется пул потоков \texttt{std::thread}.

    \item \textbf{Равномерное разбиение столбцов.}  
    \texttt{ComputeColumnRange()} гарантирует, что каждый процесс обрабатывает либо \(\lfloor \tfrac{\text{total\_cols}}{\text{size}} \rfloor\), либо \(\lfloor \tfrac{\text{total\_cols}}{\text{size}} \rfloor + 1\) столбцов, что минимизирует разброс по нагрузке.

    \item \textbf{TLS и локальные буферы.}  
    Внутри каждого потока используются локальные векторы \texttt{available} и \texttt{cask} (thread-local), чтобы избежать лишних аллокаций и гонок данных при накоплении частичных результатов.

    \item \textbf{Отсутствие гонок при записи.}  
    Каждый поток и каждый процесс работает с непересекающимися диапазонами столбцов и ячеек результирующих массивов, что исключает необходимость явной блокировки при записи.

    \item \textbf{MPI-обмен только итоговыми данными.}  
    В ходе локальных вычислений передаются лишь компактные структуры: \(\texttt{local\_n\_z}\), \(\texttt{local\_val}\), \(\texttt{local\_row}\), \(\texttt{local\_col\_counts}\). Это минимизирует коммуникационные накладные расходы.

    \item \textbf{Префиксная сумма на одном процессе.}  
    Формирование \(\texttt{M3.col\_p}\) выполняется последовательно на процессе 0, после чего результат рассылается всем остальным, обеспечивая согласованность CCS-структуры.

    \item \textbf{Гибкое переключение между последовательным и многопоточным режимом.}  
    Если число столбцов \(\text{num\_cols}\) невелико, вызывается \texttt{ComputeSequential} для уменьшения накладных расходов на потоки; иначе запускается многопоточный \texttt{ComputeParallel}.
\end{itemize}

\section{Результаты экспериментов}
Проведён эксперимент по измерению производительности различных реализаций умножения разреженных комплексных матриц в формате CCS. Для каждой версии замеры выполнялись в двух режимах: \texttt{PipelineRun} и \texttt{TaskRun}. В качестве тестовых данных использовались случайные матрицы размером 20000x20000. В таблице приведены значения времени выполнения в секундах и соответствующее ускорение относительно последовательного алгоритма. В исследовании сравнивались реализации на OpenMP, Intel TBB, \texttt{std::thread} и гибридная схема MPI + потоки.

\subsection{Таблица производительности и ускорения}
\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\footnotesize
\caption{Время выполнения и ускорение для разных реализаций}
\label{tab:performance}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Метод} & \textbf{Конфигурация} & \textbf{PipelineRun, с} & \textbf{TaskRun, с} & \textbf{Ускорение} \\
\hline
Последовательная & —         & 0.092 & 0.090 & 1.00 \\
OpenMP           & 2 потока  & 0.066 & 0.062 & 1.45 \\
TBB              & 2 потока  & 0.071 & 0.072 & 1.25 \\
std::thread      & 2 потока  & 0.090 & 0.087 & 1. 00 \\
MPI + STL (2 процесса)        & 2 потока  & 0.056 & 0.057 & 1.57 \\
\hline
\end{tabular}
\end{table}

\subsection{Анализ результатов}

В Таблице~\ref{tab:performance} приведены замеры времени выполнения и рассчитанное ускорение для разных реализаций умножения разреженных матриц (тест \texttt{test\_pipeline\_run}). Общие выводы по таблице:

\begin{itemize}
  \item Реализация на \texttt{std::thread} практически не отличается по скорости от последовательного кода (ускорение близко к 1.00). Это указывает на то, что затраты на создание и синхронизацию потоков в чистом \texttt{std::thread} слишком велики по сравнению с объёмом вычислений.
  \item OpenMP демонстрирует устойчивый прирост производительности (\(\approx1.45\times\)) благодаря тому, что ядро компилятора умеет заранее создавать пул потоков и эффективно распараллеливать цикл без дополнительных накладных расходов на управление.
  \item Intel TBB показывает умеренный выигрыш (\(\approx1.25\times\)), но уступает OpenMP. При распараллеливании по блокам TBB тратит дополнительные ресурсы на управление задачами и динамическое планирование, что при простом переборе столбцов создаёт избыточную абстракцию.
  \item Наибольшее ускорение (\(\approx1.57\times\)) продемонстрировала гибридная схема MPI + STL (2 процесса с 2 потоками в каждом). Здесь каждый MPI-процесс работает со своей частью данных в собственном адресном пространстве, снижая конкуренцию за память, а внутри процесса \texttt{std::thread} эффективно использует локальные буферы и кеширование.
\end{itemize}

Причины различий в производительности:

\begin{enumerate}
  \item \textbf{Накладные расходы на потоки.}  
    \(\bullet\) В чистом \texttt{std::thread} каждая задача запускает и завершает собственный поток. При небольшой нагрузке (обработка одного столбца на поток) затраты на запуск/синхронизацию сравнимы с вычислительными, поэтому ускорения не происходит.

  \item \textbf{Оптимизации OpenMP.}  
    \(\bullet\) OpenMP создаёт пул потоков один раз перед основным циклом \(\verb|\#pragma omp parallel for|\). Итерации цикла распределяются без дополнительного порождения потоков. Кроме того, компилятор может автоматически оптимизировать доступ к данным (векторы индексов и кеширование), что при работе с большими массивами заметно ускоряет выполнение.

  \item \textbf{Динамическое планирование TBB.}  
    \(\bullet\) Intel TBB разбивает задачу на «блоки» (с помощью \texttt{tbb::blocked\_range}) и динамически распределяет их между потоками. При равномерном и предсказуемом объёме работы (одинаковое число ненулевых элементов в каждой колонке) издержки на проверку очереди и балансировку могут перевесить выигрыш. Поэтому TBB даёт ускорение, но не такое значительное, как OpenMP.

  \item \textbf{Распределённая обработка MPI + STL.}  
    \begin{itemize}
      \item Каждый MPI-процесс обрабатывает непересекающийся набор столбцов, уменьшая конкуренцию за системную память и повышая локальность кеша.  
      \item Внутрипроцессно используются потоки \texttt{std::thread}, чтобы дополнительно задействовать два ядра без перекрёстных обращений к данным.  
      \item MPI-обмен сведён к минимально необходимым операциям: передача количества ненулевых элементов каждого процесса (\texttt{all\_gather}), сбор локальных векторов значений и индексов (\texttt{all\_gatherv}), а затем глобальная префиксная сумма для \(\texttt{col\_p}\). Затраты на коммуникацию при данной конфигурации (два процесса на одном узле или в кластере с низкой латентностью) невелики по сравнению с выигрышем от распараллеливания.
    \end{itemize}
\end{enumerate}

Относительно разницы между режимами \texttt{PipelineRun} и \texttt{TaskRun} заметим, что времена выполнения почти не отличаются (колебания в пределах нескольких сотых секунды). Это говорит о том, что накладные расходы на организацию запуска «пайплайна» и «таск-графа» примерно сопоставимы. Незначительные расхождения обусловлены планировщиком ОС, порядком инициализации потоков и случайными факторами (кеширование, фоновые процессы).

\section{Вывод}

\begin{itemize}
  \item Для однопроцессорной (одного узла) среды оптимальным выбором является \textbf{OpenMP}, так как он даёт лучший баланс между простотой реализации и производительностью. Накладные расходы на создание и синхронизацию потоков минимальны, а компилятор самостоятельно управляет пулом потоков и распределением итераций.
  
  \item \textbf{Intel TBB} удобен в сценариях, требующих динамического перераспределения задач или построения сложного графа зависимостей (DAG). При прямолинейной схеме «пробега по столбцам» его внутренние механизмы (динамическая балансировка блоков и проверка очередей задач) создают дополнительные расходы, которые в простых задачах могут превысить выигрыш.
  
  \item Использование чистых \texttt{std::thread} оправдано лишь при необходимости тонкой ручной кастомизации механизма потоков, управления очередями задач и буферов. В типовых случаях распараллеливания (особенно для регулярных циклов) предпочтительнее OpenMP или TBB, так как они скрывают от разработчика множество деталей управления потоками и синхронизацией.
  
  \item Гибридный подход \textbf{MPI + потоки} (MPI + STL) демонстрирует наибольшее ускорение в многопроцессорных или многокластерных условиях с низкой задержкой сети. Распределение столбцов между процессами снижает конкуренцию за память и повышает локальность кеша. Внутренние потоки внутри каждого процесса позволяют дополнительно задействовать доступные ядра без больших накладных расходов на межпотоковую синхронизацию. Однако на одном узле выигрыш от MPI-распределения не так велик, и зачастую достаточно только OpenMP.
  
  \item Реализованный алгоритм умножения разреженных матриц в формате CCS показал корректность и воспроизводимость результатов даже при больших размерах (миллионы строк/столбцов) благодаря аккуратной работе с указателями смещений (\texttt{col\_p}), индексами строк (\texttt{row}) и аккумулированию комплексных сумм (\texttt{cask}). Это подтверждает, что подход «проход по ненулевым элементам» остаётся эффективным при высокой разреженности матриц.
  
  \item В результате работы были освоены следующие практические навыки и знания:
    \begin{itemize}
      \item построение и оптимизация структуры CCS для разреженных матриц с комплексными элементами;
      \item реализация корректной комплексной арифметики с использованием \texttt{std::complex<double>};
      \item применение различных моделей параллелизма (\texttt{OpenMP}, \texttt{TBB}, \texttt{std::thread}, гибрид MPI + потоки);
      \item проведение экспериментальных измерений: сбор статистики, вычисление ускорения (speedup) и анализ узких мест (memory bandwidth, синхронизация, коммуникационные накладные расходы).
    \end{itemize}
\end{itemize}

\newpage
\section{Список литературы}
\begin{itemize}
  \item Сысоев А.В., Мееров И.Б., Сиднев А.А. \textit{Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks}. — Нижний Новгород, 2007.
  \item Ю. Саад. \emph{Iterative Methods for Sparse Linear Systems}. SIAM, 2003.
  \item Т. А. Дэвис. \emph{Direct Methods for Sparse Linear Systems}. SIAM, 2006.
  \item Г. Г. Гольуб, Ч. Ф. Ван Лун. \emph{Матричные вычисления} (4-е изд.). СПб.: Питер, 2011.
  \item Л. Н. Трефетен, Д. Бау. \emph{Numerical Linear Algebra}. SIAM, 1997.
  \item I. S. Duff, A. M. Erisman, J. K. Reid. \emph{Direct Methods for Sparse Matrices}. Oxford University Press, 1986.
  \item J. R. Gilbert, C. Moler, R. Schreiber. \emph{Sparse Matrices in MATLAB: Design and Implementation}. SIAM Journal on Matrix Analysis and Applications, 1992.
  \item R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald, R. Menon. \emph{Parallel Programming in OpenMP}. Morgan Kaufmann, 2001.
  \item Intel Corporation. \emph{Intel Threading Building Blocks (TBB) Documentation}. 2024.
  \item Message Passing Interface Forum. \emph{MPI: A Message-Passing Interface Standard (Version 3.1)}. 2015.
  \item B. M. Alston, P. D. Ibanez. \emph{Complex Arithmetic and Sparse Matrix Techniques in Scientific Computing}. Springer, 2010.
  \item D. Prokhorov. \emph{C++ Concurrency in Action}. Manning Publications, 2019.
  \item М. А. Шериезян. \emph{Основы параллельного программирования на C++}. Изд-во МФТИ, 2020.
\end{itemize}
\end{document}
