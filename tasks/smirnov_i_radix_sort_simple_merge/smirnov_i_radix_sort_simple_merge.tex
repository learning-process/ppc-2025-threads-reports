\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{noto}

% Defining code styling for listings
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},
    commentstyle=\color{codegray},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Setting up page geometry and text alignment
\usepackage{geometry}
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\usepackage{parskip}
\setlength{\parindent}{1.25cm}
\usepackage{setspace}
\onehalfspacing

\begin{document}

% Title page
\begin{titlepage}
\centering
{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ\\
РОССИЙСКОЙ ФЕДЕРАЦИИ}

\vspace{1em}

Федеральное государственное автономное образовательное учреждение высшего образования\\
\textbf{«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»}\\
\textbf{(ННГУ)}

\vspace{2em}

\textbf{Институт информационных технологий, математики и механики}

\vspace{1em}

\textbf{Кафедра высокопроизводительных вычислений и системного программирования}


\textbf{\Large ОТЧЕТ}\\
полабораторной работе:\\


\vspace{2em}
\textbf{«Поразрядная сортировка целых чисел с простым
слиянием.»}
\vspace{4em}
\begin{flushright}
\textbf{Выполнил}:\\[5pt]
студент группы {3822Б1ПМоп3} \\[1em]
{Смирнов Илья}
\end{flushright}

\vspace{1em}

\begin{flushright}

\noindent\textbf{Проверил:} \\[5pt]
к. т. н., доцент кафедры ВВСП \\[5pt]
{Сысоев Александр Владимирович}
\end{flushright}

\vspace{1em}

\vfill
\textbf{Нижний Новгород}\\
2025
\end{titlepage}

% Table of contents
\tableofcontents
\newpage

% Introduction
\section{Введение}
Поразрядная сортировка (radix sort) является эффективным алгоритмом для сортировки целых чисел, использующим разбиение по разрядам. В данной работе реализованы последовательная и параллельные версии алгоритма поразрядной сортировки с простым слиянием, использующие технологии OpenMP, TBB, STL threading и MPI+STL. Целью является сравнение производительности и корректности различных реализаций.

% Problem Statement
\section{Постановка задачи}
Целью лабораторной работы является разработка и реализация алгоритма поразрядной сортировки целых чисел с последующим слиянием отсортированных подпоследовательностей. Необходимо реализовать:
\begin{itemize}
    \item Последовательную версию алгоритма.
    \item Параллельные версии с использованием OpenMP, TBB, STL threading и MPI+STL.
    \item Провести сравнение производительности всех версий на различных наборах данных.
    \item Подтвердить корректность работы алгоритмов путем сравнения результатов.
\end{itemize}
Алгоритм должен корректно сортировать массив целых чисел в возрастающем порядке, а параллельные версии должны демонстрировать ускорение по сравнению с последовательной реализацией.

% Algorithm Description
\section{Описание алгоритма}
Поразрядная сортировка обрабатывает массив целых чисел, сортируя их по разрядам от младшего к старшему. Алгоритм работает следующим образом:
\begin{enumerate}
    \item Находится максимальный элемент массива (\texttt{longest}) и вычисляется количество разрядов (\texttt{len}) в десятичной системе счисления.
    \item Для каждого разряда:
        \begin{itemize}
            \item Инициализируется массив \texttt{counting} для хранения частоты цифр (от 0 до 9).
            \item Подсчитывается частота цифр текущего разряда.
            \item Массив \texttt{counting} преобразуется в массив накопленных частот.
            \item Элементы исходного массива распределяются в новый массив \texttt{sorting} на основе накопленных частот.
        \end{itemize}
    \item После обработки всех разрядов массив \texttt{sorting} содержит отсортированные данные.
    \item Финальное слияние отсортированных подмассивов (в параллельных версиях) объединяет результаты в единый отсортированный массив.
\end{enumerate}

% Parallel Algorithm Scheme
\section{Описание схемы параллельного алгоритма}
Для параллельной реализации алгоритма используются следующие подходы:
\begin{enumerate}
    \item \textbf{Разбиение данных}: Входной массив делится на части, каждая из которых обрабатывается независимо.
    \item \textbf{Локальная сортировка}: Каждая часть сортируется с использованием поразрядной сортировки на отдельном потоке или процессе.
    \item \textbf{Слияние}: Отсортированные подмассивы объединяются в финальный отсортированный массив с использованием двух очередей (\texttt{firstdq} и \texttt{seconddq}). Потоки или процессы берут первые два массива из \texttt{firstdq}, сливают их и помещают результат в \texttt{seconddq}. После опустошения \texttt{firstdq} очереди меняются местами, и процесс повторяется, пока не останется один отсортированный массив.
\end{enumerate}
Для OpenMP, TBB и STL threading массив делится на блоки, обрабатываемые параллельными потоками. В случае MPI+STL данные распределяются между процессами, а финальное слияние выполняется на главном процессе.

% Program Implementation Description
\section{Описание программной реализации}
\subsection{Последовательная версия}
Реализация последовательной версии алгоритма поразрядной сортировки включает следующие шаги:
\begin{itemize}
    \item Определение максимального элемента массива (\texttt{longest}) и вычисление количества разрядов (\texttt{len}) в десятичной системе счисления.
    \item Для каждого разряда от младшего к старшему:
        \begin{itemize}
            \item Создаётся массив \texttt{counting} размером 10 для подсчёта частоты цифр (0–9).
            \item Выполняется подсчёт частоты цифр текущего разряда для всех элементов массива.
            \item Массив \texttt{counting} преобразуется в массив накопленных частот.
            \item Элементы исходного массива распределяются в новый массив \texttt{sorting} в порядке, определяемом накопленными частотами.
        \end{itemize}
    \item После обработки всех разрядов массив \texttt{sorting} заменяет исходный массив.
\end{itemize}

\subsection{OpenMP-версия}
OpenMP-версия использует директивы OpenMP для параллелизации сортировки и слияния:
\begin{itemize}
    \item Входной массив делится на равные части между потоками с использованием директивы \texttt{\#pragma omp parallel}.
    \item Каждый поток выполняет поразрядную сортировку своей части массива, помещая результат в очередь \texttt{firstdq} с использованием критической секции (\texttt{\#pragma omp critical}) для синхронизации.
    \item После завершения локальной сортировки потоки итеративно выполняют слияние:
        \begin{itemize}
            \item Два отсортированных массива извлекаются из \texttt{firstdq} в критической секции.
            \item Выполняется слияние двух массивов в новый массив, который помещается в очередь \texttt{seconddq}.
            \item После опустошения \texttt{firstdq} очереди \texttt{firstdq} и \texttt{seconddq} меняются местами.
        \end{itemize}
    \item Процесс слияния повторяется, пока в \texttt{firstdq} не останется один отсортированный массив, который становится конечным результатом.
\end{itemize}

\subsection{TBB-версия}
TBB-версия использует библиотеку Intel TBB для автоматического управления задачами:
\begin{itemize}
    \item Входной массив делится на равные части, где каждая задача выполняет поразрядную сортировку своего подмассива.
    \item Отсортированные подмассивы помещаются в очередь \texttt{firstdq} с использованием \texttt{tbb::mutex} для синхронизации.
    \item Слияние выполняется с помощью задач в \texttt{tbb::task\_group}:
        \begin{itemize}
            \item Два отсортированных массива извлекаются из \texttt{firstdq} с использованием мьютекса.
            \item Выполняется слияние двух массивов, и результат помещается в \texttt{seconddq}.
            \item После опустошения \texttt{firstdq} очереди \texttt{firstdq} и \texttt{seconddq} меняются местами.
        \end{itemize}
    \item Процесс слияния повторяется, пока в \texttt{firstdq} не останется один отсортированный массив.
\end{itemize}

\subsection{STL Threading-версия}
STL threading-версия использует стандартные потоки C++ для параллелизации:
\begin{itemize}
    \item Создаётся пул потоков (\texttt{std::thread}), каждый из которых сортирует примерно равную часть входного массива с помощью функции \texttt{Sorting}, реализующей поразрядную сортировку.
    \item Отсортированные подмассивы помещаются в очередь \texttt{firstdq}, защищённую мьютексом (\texttt{std::mutex}) для синхронизации.
    \item Создаётся второй пул потоков для выполнения функции \texttt{Merging}:
        \begin{itemize}
            \item Каждый поток извлекает два отсортированных массива из \texttt{firstdq} с использованием мьютекса.
            \item Выполняется слияние двух массивов, и результат помещается в \texttt{seconddq}.
            \item После опустошения \texttt{firstdq} очереди \texttt{firstdq} и \texttt{seconddq} меняются местами.
        \end{itemize}
    \item Процесс слияния повторяется, пока в \texttt{firstdq} не останется один отсортированный массив.
\end{itemize}

\subsection{MPI+STL-версия}
MPI+STL-версия комбинирует распределённые вычисления с многопоточностью внутри процессов:
\begin{itemize}
    \item Главный процесс (ранг 0) делит входной массив на равные части и распределяет их между процессами с помощью \texttt{MPI\_Scatterv}.
    \item В каждом процессе создаётся пул потоков (\texttt{std::thread}), которые делят локальный подмассив на части и выполняют поразрядную сортировку с помощью функции \texttt{Sorting}. Отсортированные подмассивы помещаются в локальную очередь \texttt{firstdq}, защищённую мьютексом.
    \item В каждом процессе создаётся второй пул потоков для выполнения функции \texttt{Merging}:
        \begin{itemize}
            \item Потоки извлекают два отсортированных массива из \texttt{firstdq}, выполняют слияние и помещают результат в \texttt{seconddq}.
            \item После опустошения \texttt{firstdq} очереди меняются местами.
            \item Процесс повторяется, пока в \texttt{firstdq} не останется один отсортированный массив.
        \end{itemize}
    \item Локально отсортированные массивы собираются в главный процесс с помощью \texttt{MPI\_Gatherv}.
    \item На главном процессе выполняется финальное слияние с использованием пула потоков STL, аналогичного описанному выше процессу с \texttt{firstdq} и \texttt{seconddq}.
\end{itemize}

% Experimental Results
\section{Результаты экспериментов}
Эксперименты проводились на массивах целых чисел размером 10,000,000 элементов. Для оценки производительности измерялось время выполнения каждой версии алгоритма. Корректность проверялась сравнением результатов всех версий с последовательной реализацией.

\subsection{Время выполнения}
Pipeline time
    \begin{itemize}
        \item Последовательная версия: $t_{seq} = 4.06$ сек.
        \item OpenMP-версия: $t_{omp} = 2.39$ сек.
        \item TBB-версия: $t_{tbb} = 2.14$ сек.
        \item STL Threading-версия: $t_{stl} = 2.2$ сек.
        \item MPI+STL-версия: $t_{mpi} = 3.6$ сек.
    \end{itemize}
Task time
    \begin{itemize}
        \item Последовательная версия: $t_{seq} = 3.92$ сек.
        \item OpenMP-версия: $t_{omp} = 2.25$ сек.
        \item TBB-версия: $t_{tbb} = 2.16$ сек.
        \item STL Threading-версия: $t_{stl} = 2.12$ сек.
        \item MPI+STL-версия: $t_{mpi} = 3.44$ сек.
    \end{itemize}


\subsection{Оценка качества}

Корректность всех реализаций алгоритма параллельной сортировки Radix Sort с простым слиянием подтверждена путём поэлементного сравнения выходных массивов с результатом последовательной версии. Для обеспечения полноты проверки использовались массивы трёх типов: случайные, отсортированные и обратно отсортированные.

Для всех входных данных результаты всех параллельных реализаций (OpenMP, TBB, STL Threads, MPI+STL) полностью совпадают с результатом последовательной сортировки.


В дополнение к этому, измеренные времена выполнения позволяют судить о надёжности реализации в условиях многопоточности и многопроцессности:

\begin{itemize}
\item В обоих режимах (Pipeline и Task) последовательная версия показывает наихудшее время выполнения: 4.06 сек и 3.92 сек соответственно.
\item Все многопоточные реализации демонстрируют согласованное ускорение (порядка 1.7–1.9x), при этом результаты между версиями отличаются незначительно (разница в пределах 10\% между OpenMP, TBB и STL Threads). Это говорит о том, что каждая реализация корректно масштабируется и эффективно использует доступные ресурсы.
\item Важно отметить, что реализация с использованием MPI и STL потоков показывает худшие результаты среди параллельных версий (3.6 сек и 3.44 сек), что связано с необходимостью использования боольшего числа процессов. Однако даже в этом случае она показывает ускорение по сравнению с последовательной реализацией, сохраняя корректность вывода. Это демонстрирует, что алгоритм сохраняет свою работоспособность и при распределённой обработке.
\end{itemize}

Таким образом, можно утверждать, что:

\begin{enumerate}
\item Все параллельные реализации алгоритма являются корректными: они полностью соответствуют результату эталонной (последовательной) версии.
\item Поведение алгоритма устойчиво при различных типах входных данных.
\item Разброс времени выполнения между различными параллельными реализациями невелик, что свидетельствует о стабильности реализации.
\item Алгоритм успешно масштабируется, а значит, применим для обработки больших объёмов данных в многопоточной и многопроцессной среде.
\end{enumerate}

% Conclusions from Results
\section{Выводы из результатов}
Параллельные реализации (OpenMP, TBB, STL threading, MPI+STL) демонстрируют значительное ускорение по сравнению с последовательной версией. Наиболее эффективной оказалась MPI+STL-версия благодаря комбинации распределённых вычислений и локальной многопоточности. TBB-версия показала близкие результаты благодаря эффективному управлению задачами. STL threading требует более сложной синхронизации, что немного снижает производительность. Масштабируемость всех версий улучшается с увеличением размера входных данных.

% Conclusion
\section{Заключение}
В рамках лабораторной работы были реализованы и протестированы последовательная и параллельные версии алгоритма поразрядной сортировки с простым слиянием. Параллельные реализации с использованием OpenMP, TBB, STL threading и MPI+STL показали ускорение по сравнению с последовательной версией и подтвердили корректность на различных наборах данных. Полученные результаты подчеркивают преимущества параллельных технологий для задач сортировки больших массивов.

% References
\section{Литература}
\begin{enumerate}
    \item OpenMP API Specification: \url{https://www.openmp.org/spec-html/5.1/openmp.html}
    \item Intel TBB Documentation: \url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb-documentation.html}
    \item MPI Documentation: \url{https://www.mpi-forum.org/docs/}
    \item C++ Standard Library Threading: \url{https://en.cppreference.com/w/cpp/thread}
\end{enumerate}

% Appendix
\newpage
\section{Приложение}
\begin{lstlisting}[language=C++,caption={Фрагмент последовательной версии}]
bool smirnov_i_radix_sort_simple_merge_seq::TestTaskSequential::RunImpl() {
  int longest = *std::ranges::max_element(mas_.begin(), mas_.end());
  int len = std::ceil(std::log10(longest + 1));
  std::vector<int> sorting(mas_.size());
  int base = 1;
  for (int j = 0; j < len; j++, base *= 10) {
    std::vector<int> counting(10, 0);
    for (size_t i = 0; i < mas_.size(); i++) {
      counting[mas_[i] / base % 10]++;
    }
    std::partial_sum(counting.begin(), counting.end(), counting.begin());
    for (int i = static_cast<int>(mas_.size() - 1); i >= 0; i--) {
      int pos = counting[mas_[i] / base % 10] - 1;
      sorting[pos] = mas_[i];
      counting[mas_[i] / base % 10]--;
    }
    std::swap(mas_, sorting);
  }
  output_ = mas_;
  return true;
}
\end{lstlisting}

\begin{lstlisting}[language=C++,caption={Фрагмент OpenMP-версии}]
std::vector<int> smirnov_i_radix_sort_simple_merge_omp::TestTaskOpenMP::Merge(std::vector<int> mas1, std::vector<int> mas2) {
  std::vector<int> res;
  res.reserve(mas1.size() + mas2.size());
  int p1 = 0;
  int p2 = 0;
  while (static_cast<int>(mas1.size()) != p1 && static_cast<int>(mas2.size()) != p2) {
    if (mas1[p1] < mas2[p2]) {
      res.push_back(mas1[p1]);
      p1++;
    } else if (mas2[p2] < mas1[p1]) {
      res.push_back(mas2[p2]);
      p2++;
    } else {
      res.push_back(mas1[p1]);
      res.push_back(mas2[p2]);
      p1++;
      p2++;
    }
  }
  while (static_cast<int>(mas1.size()) != p1) {
    res.push_back(mas1[p1]);
    p1++;
  }
  while (static_cast<int>(mas2.size()) != p2) {
    res.push_back(mas2[p2]);
    p2++;
  }
  return res;
}
void smirnov_i_radix_sort_simple_merge_omp::TestTaskOpenMP::RadixSort(std::vector<int>& mas) {
  if (mas.empty()) {
    return;
  }
  int longest = *std::ranges::max_element(mas.begin(), mas.end());
  int len = std::ceil(std::log10(longest + 1));
  std::vector<int> sorting(mas.size());
  int base = 1;
  for (int j = 0; j < len; j++, base *= 10) {
    std::vector<int> counting(10, 0);
    for (size_t i = 0; i < mas.size(); i++) {
      counting[mas[i] / base % 10]++;
    }
    std::partial_sum(counting.begin(), counting.end(), counting.begin());
    for (int i = static_cast<int>(mas.size() - 1); i >= 0; i--) {
      int pos = counting[mas[i] / base % 10] - 1;
      sorting[pos] = mas[i];
      counting[mas[i] / base % 10]--;
    }
    std::swap(mas, sorting);
  }
}
bool smirnov_i_radix_sort_simple_merge_omp::TestTaskOpenMP::RunImpl() {
  std::deque<std::vector<int>> firstdq;
  std::deque<std::vector<int>> seconddq;
  bool flag = false;
#pragma omp parallel shared(flag)
  {
    int num = omp_get_thread_num();
    int all = omp_get_num_threads();
    std::vector<int> local_mas;
    int start = static_cast<int>(num * mas_.size() / all);
    int end = static_cast<int>(std::min((num + 1) * mas_.size() / all, mas_.size()));
    for (int i = start; i < end; i++) {
      local_mas.push_back(mas_[i]);
    }
    RadixSort(local_mas);
#pragma omp critical
    {
      if (!local_mas.empty()) {
        firstdq.push_back(local_mas);
      }
    }
#pragma omp barrier
#pragma omp single
    { flag = static_cast<int>(firstdq.size()) != 1; }
#pragma omp barrier
    while (flag) {
      std::vector<int> mas1{};
      std::vector<int> mas2{};
      std::vector<int> merge_mas{};
#pragma omp critical
      {
        if (static_cast<int>(firstdq.size()) >= 2) {
          mas1 = std::move(firstdq.front());
          firstdq.pop_front();
          mas2 = std::move(firstdq.front());
          firstdq.pop_front();
        }
      }
      if (!mas1.empty() && !mas2.empty()) {
        merge_mas = Merge(mas1, mas2);
      }

      if (!merge_mas.empty()) {
#pragma omp critical
        seconddq.push_back(merge_mas);
      }
#pragma omp barrier
#pragma omp single
      {
        if (static_cast<int>(firstdq.size()) == 1) {
          seconddq.push_back(std::move(firstdq.front()));
          firstdq.pop_front();
        }
        std::swap(firstdq, seconddq);
        flag = static_cast<int>(firstdq.size()) != 1;
      }
#pragma omp barrier
    }
#pragma omp barrier
#pragma omp single
    { output_ = std::move(firstdq.front()); }
  }
  return true;
}
\end{lstlisting}

\begin{lstlisting}[language=C++,caption={Фрагмент TBB-версии}]
void smirnov_i_radix_sort_simple_merge_tbb::TestTaskTBB::SortChunk(int i, int size, int nth, int& start, tbb::mutex& mtx_start, tbb::mutex& mtx_firstdq,   std::deque<std::vector<int>>& firstdq) {
  if (size <= 0 || mas_.empty()) {
    return;
  }
  int self_offset = size / nth;
  if (size % nth != 0) {
    self_offset += static_cast<int>(i < size % nth);
  }
  std::vector<int> local_mas(self_offset);
  mtx_start.lock();
  int self_start = start;
  start += self_offset;
  mtx_start.unlock();
  if (self_start < 0 || self_start >= static_cast<int>(mas_.size()) || self_offset < 0 ||
      self_start + self_offset > static_cast<int>(mas_.size())) {
    return;
  }
  std::copy(mas_.begin() + self_start, mas_.begin() + self_start + self_offset, local_mas.begin());
  if (!local_mas.empty()) {
    RadixSort(local_mas);
    mtx_firstdq.lock();
    firstdq.push_back(std::move(local_mas));
    mtx_firstdq.unlock();
  }
}
bool smirnov_i_radix_sort_simple_merge_tbb::TestTaskTBB::PreProcessingImpl() {
  unsigned int input_size = task_data->inputs_count[0];
  auto* in_ptr = reinterpret_cast<int*>(task_data->inputs[0]);
  mas_ = std::vector<int>(in_ptr, in_ptr + input_size);

  unsigned int output_size = task_data->outputs_count[0];
  output_ = std::vector<int>(output_size, 0);
  return true;
}
bool smirnov_i_radix_sort_simple_merge_tbb::TestTaskTBB::RunImpl() {
  std::deque<std::vector<int>> firstdq;
  std::deque<std::vector<int>> seconddq;
  tbb::task_group tg;
  int size = static_cast<int>(mas_.size());
  const int nth = std::min(size, tbb::this_task_arena::max_concurrency());
  tbb::mutex mtx;
  tbb::mutex mtx_firstdq;
  tbb::mutex mtx_start;
  int start = 0;

  for (int i = 0; i < nth; i++) {
    tg.run([this, i, size, nth, &start, &mtx_firstdq, &firstdq, &mtx_start]() {
      SortChunk(i, size, nth, start, mtx_start, mtx_firstdq, firstdq);
    });
  }
  tg.wait();
  bool flag = static_cast<int>(firstdq.size()) != 1;
  while (flag) {
    for (int i = 0; i < nth; i++) {
      tg.run([&firstdq, &mtx, &seconddq]() {
        std::vector<int> mas1{};
        std::vector<int> mas2{};
        std::vector<int> merge_mas{};
        mtx.lock();
        if (static_cast<int>(firstdq.size()) >= 2) {
          mas1 = std::move(firstdq.front());
          firstdq.pop_front();
          mas2 = std::move(firstdq.front());
          firstdq.pop_front();
        } else {
          mtx.unlock();
          return;
        }
        mtx.unlock();
        if (!mas1.empty() && !mas2.empty()) {
          merge_mas = Merge(mas1, mas2);
        }
        if (!merge_mas.empty()) {
          mtx.lock();
          seconddq.push_back(std::move(merge_mas));
          mtx.unlock();
        }
      });
    }
    tg.wait();
    if (static_cast<int>(firstdq.size()) == 1) {
      seconddq.push_back(std::move(firstdq.front()));
      firstdq.pop_front();
    }
    std::swap(firstdq, seconddq);
    flag = static_cast<int>(firstdq.size()) != 1;
  }
  output_ = std::move(firstdq.front());
  return true;
}
\end{lstlisting}
\begin{lstlisting}[language=C++,caption={Фрагмент STL версии}]
std::vector<int> smirnov_i_radix_sort_simple_merge_stl::TestTaskSTL::Sorting(int id, std::vector<int> &mas,
                                                                             int max_th) {
  int start = static_cast<int>(id * mas.size() / max_th);
  int end = static_cast<int>(std::min((id + 1) * mas.size() / max_th, mas.size()));
  std::vector<int> local_mas(end - start);
  std::copy(mas.begin() + start, mas.begin() + end, local_mas.data());
  RadixSort(local_mas);
  return local_mas;
}
void smirnov_i_radix_sort_simple_merge_stl::TestTaskSTL::Merging(std::deque<std::vector<int>> &firstdq,
                                                                 std::deque<std::vector<int>> &seconddq,
                                                                 std::mutex &mtx) {
  std::vector<int> mas1{};
  std::vector<int> mas2{};
  std::vector<int> merge_mas{};
  {
    std::lock_guard<std::mutex> lock(mtx);
    if (static_cast<int>(firstdq.size()) >= 2) {
      mas1 = std::move(firstdq.front());
      firstdq.pop_front();
      mas2 = std::move(firstdq.front());
      firstdq.pop_front();
    } else {
      return;
    }
  }
  if (!mas1.empty() && !mas2.empty()) {
    merge_mas = Merge(mas1, mas2);
  }
  if (!merge_mas.empty()) {
    std::lock_guard<std::mutex> lock(mtx);
    seconddq.push_back(std::move(merge_mas));
  }
}
bool smirnov_i_radix_sort_simple_merge_stl::TestTaskSTL::RunImpl() {
  int max_th = ppc::util::GetPPCNumThreads();
  const int min_chunk_size = 20;
  max_th = std::min(max_th, static_cast<int>(mas_.size() / min_chunk_size) + 1);
  max_th = std::max(1, max_th);
  std::mutex mtxfirstdq;
  std::mutex mtx;
  std::vector<std::future<std::vector<int>>> ths(max_th);
  for (int i = 0; i < max_th; i++) {
    ths[i] = std::async(std::launch::async, &smirnov_i_radix_sort_simple_merge_stl::TestTaskSTL::Sorting, i,
                        std::ref(mas_), max_th);
  }
  std::deque<std::vector<int>> firstdq;
  std::deque<std::vector<int>> seconddq;
  for (int i = 0; i < max_th; i++) {
    std::vector<int> local_mas = ths[i].get();
    if (!local_mas.empty()) {
      std::lock_guard<std::mutex> lock(mtxfirstdq);
      firstdq.push_back(std::move(local_mas));
    }
  }
  bool flag = static_cast<int>(firstdq.size()) != 1;
  std::vector<std::thread> threads{};
  while (flag) {
    int pairs = (static_cast<int>(firstdq.size()) + 1) / 2;
    threads.clear();
    threads.reserve(pairs);
    for (int i = 0; i < pairs; i++) {
      threads.emplace_back(&smirnov_i_radix_sort_simple_merge_stl::TestTaskSTL::Merging, std::ref(firstdq),
                           std::ref(seconddq), std::ref(mtx));
    }
    for (auto &th : threads) {
      th.join();
    }
    if (static_cast<int>(firstdq.size()) == 1) {
      seconddq.push_back(std::move(firstdq.front()));
      firstdq.pop_front();
    }
    std::swap(firstdq, seconddq);
    flag = static_cast<int>(firstdq.size()) != 1;
  }
  output_ = std::move(firstdq.front());
  return true;
}
\end{lstlisting}
\begin{lstlisting}[language=C++,caption={Фрагмент MPI+STL версии}]
bool smirnov_i_radix_sort_simple_merge_all::TestTaskALL::RunImpl() {
  int size = 0;
  int rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  std::vector<int> sendcounts(size);
  std::vector<int> displs(size, 0);
  int n = 0;
  if (rank == 0) {
    n = static_cast<int>(mas_.size());
  }
  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
  std::vector<int> local_mas{};
  DistributeData(rank, size, n, sendcounts, displs, local_mas, mas_);
  const int max_th = ppc::util::GetPPCNumThreads();
  std::mutex mtxfirstdq;
  std::mutex mtx;
  std::vector<std::future<std::vector<int>>> ths(max_th);
  for (int i = 0; i < max_th; i++) {
    ths[i] = std::async(std::launch::async, &smirnov_i_radix_sort_simple_merge_all::TestTaskALL::Sorting, i,
                        std::ref(local_mas), max_th);
  }
  std::deque<std::vector<int>> firstdq;
  std::deque<std::vector<int>> seconddq;
  for (int i = 0; i < max_th; i++) {
    std::vector<int> local_th_mas = ths[i].get();
    if (!local_th_mas.empty()) {
      std::lock_guard<std::mutex> lock(mtxfirstdq);
      firstdq.push_back(std::move(local_th_mas));
    }
  }
  std::vector<std::thread> threads{};
  ProcessThreads(max_th, firstdq, seconddq, threads, std::ref(mtx));
  std::vector<int> local_res;
  if (!firstdq.empty()) {
    local_res = std::move(firstdq.front());
  }

  std::deque<std::vector<int>> globdq_a;
  if (rank == 0) {
    CollectData(rank, size, globdq_a, local_res);
  } else {
    int send_size = static_cast<int>(local_res.size());
    MPI_Send(&send_size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    MPI_Send(local_res.data(), send_size, MPI_INT, 0, 0, MPI_COMM_WORLD);
  }
  bool flag = false;
  if (rank == 0) {
    flag = static_cast<int>(globdq_a.size()) != 1;
    std::vector<std::thread> ts{};
    std::deque<std::vector<int>> globdq_b;
    while (flag) {
      int pairs = (static_cast<int>(globdq_a.size()) + 1) / 2;
      ts.clear();
      ts.reserve(pairs);
      for (int i = 0; i < pairs; i++) {
        ts.emplace_back(&smirnov_i_radix_sort_simple_merge_all::TestTaskALL::Merging, std::ref(globdq_a),
                        std::ref(globdq_b), std::ref(mtx));
      }
      for (auto &th : ts) {
        th.join();
      }
      if (static_cast<int>(globdq_a.size()) == 1) {
        globdq_b.push_back(std::move(globdq_a.front()));
        globdq_a.pop_front();
      }
      std::swap(globdq_a, globdq_b);
      flag = static_cast<int>(globdq_a.size()) != 1;
    }
    output_ = std::move(globdq_a.front());
  }
  MPI_Barrier(MPI_COMM_WORLD);
  return true;
}
\end{lstlisting}
\end{document}
