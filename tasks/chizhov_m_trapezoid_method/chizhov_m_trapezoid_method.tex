\documentclass[a4paper,14pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{float}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    tabsize=4
}


\begin{document}
\begin{titlepage}
    \centering
    \large
    Министерство науки и высшего образования Российской Федерации\\[0.5cm]
    Федеральное государственное автономное образовательное учреждение высшего образования\\[0.5cm]
    \textbf{«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»}\\
    (ННГУ)\\[1cm]
    Институт информационных технологий, математики и механики\\[0.5cm]

    \vfill
    {\LARGE \textbf{Отчет по лабораторным работам }}\\[0.5cm]
    {\Large курса «Параллельное программирование» }\\[0.5cm]
    {\Large по задаче}\\[0.5cm]
    {\LARGE \textbf{Вычисление многомерных интегралов с использованием многошаговой схемы (метод трапеций)}}\\[0.5cm]
    {\Large \textbf{Вариант 10}}\\[2.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Выполнил:} \\
        студент группы 3822Б1ФИ3 \\
        Чижов Максим
    }\\[0.5cm]

    \hfill\parbox{0.5\textwidth}{
        \textbf{Преподаватель:} \\
        Сысоев А.В.

    }\\[2cm]

    \vfill
    { Нижний Новгород } \\
    { 2025 }
\end{titlepage}

\newpage
\section*{Содержание}
\addcontentsline{toc}{section}{Содержание}

\begin{enumerate}
    \item Введение
    \item Постановка задачи
    \item Описание последовательной версии алгоритма
    \item Описание параллельных версий алгоритма
    \begin{enumerate}
        \item Технология OpenMP
        \item Технология TBB
        \item Технология STL
        \item Комбинированный подход MPI + TBB
    \end{enumerate}
    \item Результаты
    \item Вывод
    \item Список литературы
\end{enumerate}


\newpage
\section{Введение}

Численное интегрирование играет важную роль в прикладных задачах, связанных с обработкой данных, моделированием физических процессов и другими вычислительными задачами. В частности, вычисление многомерных интегралов возникает при решении задач математической физики, машинного обучения, статистики и других областей.

Аналитическое решение многомерных интегралов зачастую невозможно или крайне затруднено, поэтому требуется использовать численные методы. Одним из таких методов является метод трапеций, который позволяет аппроксимировать значение интеграла путём разбиения области интегрирования на небольшие элементы и вычисления суммы значений функции в этих точках.

В данной работе рассматривается реализация многошаговой схемы метода трапеций для вычисления многомерных интегралов. Кроме того, проводится исследование производительности различных параллельных версий алгоритма, реализованных с использованием технологий OpenMP, TBB, стандартной библиотеки потоков C++ (std::thread) и комбинированного подхода с использованием MPI и TBB.

Целью работы является анализ эффективности различных способов параллелизации метода трапеций и сравнение их производительности при решении задач численного интегрирования.

\newpage
\section{Постановка задачи}

Необходимо реализовать алгоритм численного вычисления определённого многомерного интеграла на основе многошаговой схемы метода трапеций. Задача формулируется следующим образом:

\begin{itemize}
    \item Требуется вычислить значение интеграла от заданной функции\\ $f(x_1, x_2, ..., x_n)$ на области $[a_1, b_1] \times [a_2, b_2] \times \dots \times [a_n, b_n]$.
    \item Для приближённого вычисления используется метод трапеций, обобщённый на многомерный случай.
    \item Алгоритм должен быть реализован в виде:
    \begin{itemize}
        \item последовательной версии;
        \item параллельных версий с использованием OpenMP, Intel TBB,\\ std::thread и гибридного подхода MPI + TBB.
    \end{itemize}
    \item Результаты работы алгоритмов необходимо сравнить по времени выполнения и точности вычислений при одинаковых входных данных.
    \item Реализация должна позволять легко изменять количество переменных (размерность интеграла), границы интегрирования и количество разбиений по каждой оси.
\end{itemize}

Таким образом, задача заключается не только в реализации базового метода численного интегрирования, но и в исследовании эффективности различных средств параллельного программирования применительно к этой задаче.

\newpage
\section{Описание последовательной версии алгоритма}

Последовательная версия алгоритма вычисления многомерного интеграла реализована с использованием многошаговой схемы метода трапеций. Алгоритм разбивает область интегрирования на равные интервалы по каждой из переменных, затем вычисляет значения функции в узлах сетки и суммирует их с учетом весов, соответствующих формуле метода трапеций.

Ниже приведён фрагмент кода, реализующий вычисление интеграла:

\begin{lstlisting}[language=C++]
double TrapezoidMethod(Function& f, size_t div, size_t dim,
                       std::vector<double>& lower_limits,
                       std::vector<double>& upper_limits) {
  int int_div = static_cast<int>(div);
  int int_dim = static_cast<int>(dim);
  std::vector<double> h(int_dim);
  std::vector<int> steps(int_dim);

  for (int i = 0; i < int_dim; i++) {
    steps[i] = int_div;
    h[i] = (upper_limits[i] - lower_limits[i]) / steps[i];
  }

  int total_nodes = 1;
  for (const auto& step : steps) {
    total_nodes *= (step + 1);
  }

  double result = 0.0;
  std::vector<double> point(int_dim);

  for (int i = 0; i < total_nodes; i++) {
    int temp = i;

    for (int j = 0; j < int_dim; j++) {
      int node_index = temp % (steps[j] + 1);
      point[j] = lower_limits[j] + node_index * h[j];
      temp /= (steps[j] + 1);
    }

    double weight = 1.0;
    for (int j = 0; j < int_dim; j++) {
      if (point[j] == lower_limits[j] || point[j] == upper_limits[j]) {
        weight *= 1.0;
      } else {
        weight *= 2.0;
      }
    }

    result += weight * f(point);
  }

  for (int i = 0; i < int_dim; i++) {
    result *= h[i] / 2.0;
  }

  return result;
}
\end{lstlisting}

Основные этапы алгоритма:
\begin{itemize}
    \item вычисление шагов разбиения $h_i$ по каждой размерности;
    \item подсчёт общего числа узлов сетки;
    \item проход по всем узлам, вычисление координат точки и соответствующего веса;
    \item суммирование значений функции с учётом весов;
    \item умножение итоговой суммы на произведение половин шагов для получения приближённого значения интеграла.
\end{itemize}

Данный подход является классическим численным методом и служит основой для последующей параллельной оптимизации.

\newpage
\section{Описание параллельных версий алгоритма}
\subsection{Технология OpenMP}

Параллельная версия алгоритма, реализованная с использованием технологии OpenMP, сохраняет общую логику последовательного метода, но распределяет вычисления по узлам сетки между потоками для ускорения работы.

Основная идея — распараллелить цикл обхода всех точек разбиения интегральной области. Для этого используется директива \- \texttt{\#pragma omp parallel} с приватным локальным накоплением результата в каждом потоке, а затем результаты аккумулируются с помощью атомарной операции.

Ниже представлен ключевой фрагмент реализации:

\begin{lstlisting}[language=C++]
double TrapezoidMethod(Function& f, size_t div, size_t dim,
                       std::vector<double>& lower_limits,
                       std::vector<double>& upper_limits) {
  int int_div = static_cast<int>(div);
  int int_dim = static_cast<int>(dim);
  std::vector<double> h(int_dim);
  std::vector<int> steps(int_dim);

  for (int i = 0; i < int_dim; i++) {
    steps[i] = int_div;
    h[i] = (upper_limits[i] - lower_limits[i]) / steps[i];
  }

  int total_nodes = 1;
  for (const auto& step : steps) {
    total_nodes *= (step + 1);
  }

  double result = 0.0;

#pragma omp parallel
  {
    double local_result = 0.0;
    std::vector<double> point(int_dim);

#pragma omp for
    for (int i = 0; i < total_nodes; i++) {
      int temp = i;
      double weight = 1.0;

      for (int j = 0; j < int_dim; j++) {
        int node_index = temp % (steps[j] + 1);
        point[j] = lower_limits[j] + node_index * h[j];
        temp /= (steps[j] + 1);
      }

      for (int j = 0; j < int_dim; j++) {
        if (point[j] == lower_limits[j] || point[j] == upper_limits[j]) {
          weight *= 1.0;
        } else {
          weight *= 2.0;
        }
      }

      local_result += weight * f(point);
    }

#pragma omp atomic
    result += local_result;
  }

  for (int i = 0; i < int_dim; i++) {
    result *= h[i] / 2.0;
  }

  return std::round(result * 100.0) / 100.0;
}
\end{lstlisting}

Использование OpenMP позволяет значительно ускорить вычисления за счёт распараллеливания основного цикла обхода точек интегрирования, сохраняя при этом точность и корректность результата.

\subsection{Технология TBB}

Параллельная реализация с использованием библиотеки Intel TBB основана на применении алгоритма \texttt{parallel\_reduce}, который эффективно распараллеливает вычисления, используя динамическое распределение работы между потоками.

В данном подходе вычисление суммы значений функции по узлам сетки разбивается на блоки, которые обрабатываются параллельно в рамках \texttt{task\_arena} с числом потоков, соответствующим количеству аппаратных ядер.

Основной цикл реализован с помощью \texttt{parallel\_reduce}, что позволяет аккумулировать частичные результаты локально, а затем объединять их для получения итогового значения интеграла.

Ниже представлен ключевой фрагмент кода:

\begin{lstlisting}[language=C++]
double TrapezoidMethod(Function& f, size_t div, size_t dim,
                       std::vector<double>& lower_limits,
                       std::vector<double>& upper_limits) {
  int int_dim = static_cast<int>(dim);
  std::vector<double> h(int_dim);
  std::vector<int> steps(int_dim);

  for (int i = 0; i < int_dim; i++) {
    steps[i] = static_cast<int>(div);
    h[i] = (upper_limits[i] - lower_limits[i]) / steps[i];
  }

  long long total_nodes = 1;
  for (const auto& step : steps) {
    total_nodes *= (step + 1);
  }

  double result = 0.0;
  const int num_threads = ppc::util::GetPPCNumThreads();
  oneapi::tbb::task_arena arena(num_threads);

  arena.execute([&] {
    result = oneapi::tbb::parallel_reduce(
        tbb::blocked_range<long>(0, total_nodes, 16), 0.0,
        [&](const tbb::blocked_range<long>& r, double local_res) {
          for (long i = r.begin(); i != r.end(); ++i) {
            int temp = static_cast<int>(i);
            double weight = 1.0;
            std::vector<double> point(int_dim);

            for (int j = 0; j < int_dim; j++) {
              int node_index = temp % (steps[j] + 1);
              point[j] = lower_limits[j] + node_index * h[j];
              temp /= (steps[j] + 1);
            }

            for (int j = 0; j < int_dim; j++) {
              if (point[j] == lower_limits[j] || point[j] == upper_limits[j]) {
                weight *= 1.0;
              } else {
                weight *= 2.0;
              }
            }

            local_res += weight * f(point);
          }
          return local_res;
        },
        [](double a, double b) { return a + b; });
  });

  for (int i = 0; i < int_dim; i++) {
    result *= h[i] / 2.0;
  }

  return std::round(result * 100.0) / 100.0;
}
\end{lstlisting}

Использование TBB позволяет добиться эффективного распараллеливания с динамическим балансировкой нагрузки, что положительно сказывается на производительности при больших объёмах вычислений.

\subsection{Технология STL}
В данной реализации для распараллеливания используется стандартная библиотека потоков. Общее количество вычислительных узлов разбивается на равные части, каждая из которых обрабатывается отдельным потоком. Каждый поток накапливает локальный результат интегрирования, который затем объединяется с результатами других потоков.

Основные шаги алгоритма:
\begin{itemize}
  \item Вычисляется общее число узлов сетки интегрирования.
  \item Количество потоков определяется динамически, исходя из возможностей аппаратного обеспечения.
  \item Каждому потоку выделяется своя часть узлов, по которым вычисляется значение функции с соответствующими весами.
  \item После завершения работы потоков их локальные результаты суммируются.
  \item Итоговое значение масштабируется шагами сетки для получения результата интегрирования.
\end{itemize}

Такой подход позволяет эффективно использовать ресурсы процессора и добиться ускорения вычислений за счёт параллелизма.

Ниже приведён ключевой фрагмент кода, демонстрирующий распараллеливание с использованием \texttt{std::thread}:

\begin{lstlisting}[language=C++]
double TrapezoidMethod(Function& f, size_t div, size_t dim,
                       std::vector<double>& lower_limits,
                       std::vector<double>& upper_limits) {
  int int_dim = static_cast<int>(dim);
  std::vector<double> h(int_dim);
  std::vector<int> steps(int_dim);

  for (int i = 0; i < int_dim; i++) {
    steps[i] = static_cast<int>(div);
    h[i] = (upper_limits[i] - lower_limits[i]) / steps[i];
  }

  long long total_nodes = 1;
  for (const auto& step : steps) {
    total_nodes *= (step + 1);
  }

  auto num_threads = ppc::util::GetPPCNumThreads();
  std::vector<double> local_results(num_threads, 0.0);
  std::vector<std::thread> threads(num_threads);

  for (int t = 0; t < num_threads; ++t) {
    threads[t] = std::thread([&, t]() {
      long long start = t * (total_nodes / num_threads);
      long long end = (t + 1) * (total_nodes / num_threads);
      if (t == num_threads - 1) {
        end = total_nodes;
      }

      std::vector<double> point(int_dim);

      for (long long i = start; i < end; ++i) {
        int temp = static_cast<int>(i);

        for (int j = 0; j < int_dim; j++) {
          int node_index = temp % (steps[j] + 1);
          point[j] = lower_limits[j] + node_index * h[j];
          temp /= (steps[j] + 1);
        }

        double weight = ComputeWeight(point, lower_limits, upper_limits);

        local_results[t] += weight * f(point);
      }
    });
  }

  for (auto& thread : threads) {
    thread.join();
  }

  double result = 0.0;
  for (const auto& local_result : local_results) {
    result += local_result;
  }

  for (int i = 0; i < int_dim; i++) {
    result *= h[i] / 2.0;
  }

  return std::round(result * 100.0) / 100.0;
}
\end{lstlisting}

Применение потоков STL обеспечивает независимое выполнение вычислений в отдельных потоках, что позволяет масштабировать программу на многоядерных системах без использования сторонних библиотек.

\subsection{Комбинированный подход MPI + TBB}

Данная реализация комбинирует межпроцессное распределение задач с помощью MPI и многопоточную обработку внутри каждого процесса с использованием Intel TBB.

Общий объём вычислений, соответствующий числу узлов сетки, разделяется между MPI-процессами с учётом возможного остатка для равномерного распределения нагрузки. Каждый процесс обрабатывает свой непрерывный диапазон индексов узлов.

Для ускорения вычислений в каждом процессе применяется библиотека TBB: вычисления по диапазону узлов распараллеливаются с помощью \texttt{parallel\_reduce} внутри \texttt{task\_arena}, где количество потоков соответствует числу аппаратных ядер.

В конце локальные результаты процессов агрегируются с помощью MPI-операции редукции, обеспечивая получение общего результата интегрирования.

Ниже представлен ключевой фрагмент кода:

\begin{lstlisting}[language=C++]
double TrapezoidMethod(Function& f, size_t div, size_t dim,
                       std::vector<double>& lower_limits,
                       std::vector<double>& upper_limits,
                       const boost::mpi::communicator& world) {
  int int_dim = static_cast<int>(dim);
  std::vector<double> h(int_dim);
  std::vector<int> steps(int_dim);

  for (int i = 0; i < int_dim; i++) {
    steps[i] = static_cast<int>(div);
    h[i] = (upper_limits[i] - lower_limits[i]) / steps[i];
  }

  long long total_nodes = 1;
  for (int i = 0; i < int_dim; ++i) {
    total_nodes *= (steps[i] + 1);
  }

  int rank = world.rank();
  int size = world.size();

  long long base_count = total_nodes / size;
  long long remainder = total_nodes % size;

  long start = (rank < remainder) ? (rank * (base_count + 1))
                                  : (remainder * (base_count + 1)) + ((rank - remainder) * base_count);

  long end = start + base_count + (rank < remainder ? 1 : 0);

  double local_result = 0.0;

  const int num_threads = ppc::util::GetPPCNumThreads();
  oneapi::tbb::task_arena arena(num_threads);

  arena.execute([&] {
    local_result = oneapi::tbb::parallel_reduce(
        tbb::blocked_range<long>(start, end, 16), 0.0,
        [&](const tbb::blocked_range<long>& r, double local_res) {
          for (long i = r.begin(); i != r.end(); ++i) {
            int temp = static_cast<int>(i);
            double weight = 1.0;
            std::vector<double> point(int_dim);

            for (int j = 0; j < int_dim; j++) {
              int node_index = temp % (steps[j] + 1);
              point[j] = lower_limits[j] + node_index * h[j];
              temp /= (steps[j] + 1);
            }

            for (int j = 0; j < int_dim; j++) {
              if (point[j] == lower_limits[j] || point[j] == upper_limits[j]) {
                weight *= 1.0;
              } else {
                weight *= 2.0;
              }
            }

            local_res += weight * f(point);
          }
          return local_res;
        },
        [](double a, double b) { return a + b; });
  });

  double global_result = 0.0;
  boost::mpi::reduce(world, local_result, global_result, std::plus<>(), 0);

  if (rank == 0) {
    for (int i = 0; i < int_dim; ++i) {
      global_result *= h[i] / 2.0;
    }
    return std::round(global_result * 100.0) / 100.0;
  }

  return 0.0;
}
\end{lstlisting}

\newpage
\section{Результаты}

Для оценки производительности различных реализаций метода трапеций были проведены тесты на вычисление интеграла. Все тесты выполнялись на машине с числом потоков, ограниченным двумя, чтобы обеспечить равные условия для сравнения.

В таблице~\ref{tab:performance} приведены средние времена выполнения и ускорение относительно последовательной реализации.

\begin{table}[H]
  \centering
  \caption{Время выполнения и ускорение различных реализаций метода трапеций}
  \label{tab:performance}
  \begin{tabular}{|l|r|r|}
    \hline
    \textbf{Технология} & \textbf{Время (мс)} & \textbf{Ускорение} \\
    \hline
    Последовательная & 1729 & 1.00 \\
    OpenMP         & 818  & 2.11 \\
    Intel TBB       & 973  & 1.78 \\
    STL         & 1204 & 1.44 \\
    MPI + TBB (2 процесса)  & 1527 & 1.13 \\
    \hline
  \end{tabular}
\end{table}

Ускорение вычислено как отношение времени последовательной реализации к времени параллельной.

Как видно из результатов, реализация на OpenMP продемонстрировала наибольшее ускорение — более чем в 2 раза. Использование TBB и STL также привело к заметному сокращению времени по сравнению с последовательной версией. MPI в сочетании с TBB показал умеренное ускорение, что связано с накладными расходами на коммуникацию между процессами.

В целом, полученные результаты подтверждают эффективность распараллеливания задачи численного интегрирования и демонстрируют преимущества различных технологий в зависимости от архитектуры и модели параллелизма.

\newpage
\section{Выводы}

В ходе работы была реализована и сравнительно протестирована несколько параллельных версий метода трапеций для численного интегрирования многомерных функций с использованием различных технологий: последовательной реализации, OpenMP, Intel TBB, стандартных потоков C++ STL, а также гибридного варианта MPI + TBB.

Результаты тестирования показали, что все параллельные реализации обеспечивают ускорение по сравнению с последовательной, при этом максимальное ускорение было достигнуто с использованием OpenMP, что обусловлено низкими накладными расходами и эффективным распределением задач между потоками.

Реализация с использованием Intel TBB также показала высокую эффективность, а версия на стандартных потоках STL оказалась несколько менее производительной, что связано с особенностями управления потоками и синхронизацией в данной технологии.

Гибридная реализация MPI + TBB показала умеренное ускорение, ограниченное коммуникационными издержками, что характерно для распределённых вычислений.

Таким образом, выбор конкретной технологии параллелизма должен базироваться на архитектуре вычислительной системы и требованиях к масштабируемости. Для задач, выполняемых на одном многоядерном узле, предпочтительнее OpenMP или TBB, тогда как для распределённых систем целесообразно применять гибридные подходы с MPI.

В дальнейшем возможна оптимизация и доработка алгоритмов с целью повышения эффективности, например, за счёт более тонкого баланса нагрузки и минимизации затрат на синхронизацию и коммуникацию.

\newpage
\section{Список литературы}
\begin{enumerate}
    \item Сысоев А. В., Мееров И. Б., Сиднев А. А. Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks. — Нижний Новгород: Нижегородский государственный университет, 2007. — 122 с.
    \item Львовский С. М. Набор и вёрстка в системе LaTeX: учеб. пособие. — 3-е изд., испр. и доп. — М.: Мир, 2003. — 472 с.
\end{enumerate}
\end{document}