\documentclass[14pt,a4paper]{extarticle}
\renewcommand{\normalsize}{\fontsize{14pt}{16.8pt}\selectfont}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{framed}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}

\geometry{
  a4paper,
  left=30mm,
  right=15mm,
  top=20mm,
  bottom=20mm
}

\titleformat{\section}[block]
  {\normalfont\fontsize{14}{16}\bfseries\centering}
  {\thesection.}{0.5em}{}
\titleformat{\subsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsubsection.}{0.5em}{}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    frame=single,
    tabsize=4,
    showstringspaces=false,
    breaklines=true
}

\sloppy

\onehalfspacing

\setlength{\parindent}{1.25cm}

\begin{document}

\begin{titlepage}
\begin{center}

\onehalfspacing

\begin{center}
    \textbf{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ} \\
    Федеральное государственное автономное образовательное учреждение высшего образования \\
    «Национальный исследовательский Нижегородский университет им. Н.И. Лобачевского» (ННГУ) \\
    Институт информационных технологий, математики и механики
\end{center}

\vspace{4cm}

\begin{center}
    \textbf{ОТЧЕТ ПО ПРАКТИЧЕСКИМ РАБОТАМ} \vspace{0.5cm}\\
    по курсу «ПАРАЛЛЕЛЬНОЕ ПРОГРАММИРОВАНИЕ» \vspace{0.5cm}\\
    \textit{Задача: Выделение рёбер на изображении с использованием оператора Собеля}
\end{center}

\vspace{4cm}

\begin{flushright}
    \textbf{ВЫПОЛНИЛ:} \\ 
    Студент 3 курса 
    группы 3822Б1ПР1 \\ 
    Кораблев В.М. \\

    \vspace{1cm}

    \textbf{ПРОВЕРИЛ:} \\ 
    доцент кафедры ВВСП, к.т.н. \\ 
    Сысоев А.В.
\end{flushright}

\vspace{1cm}

\begin{center}
    Нижний Новгород\\
    2025\newpage
\end{center}

\end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Введение}

Задача выделения рёбер на изображении является одной из фундаментальных в области компьютерного зрения и цифровой обработки изображений. Выделенные рёбра представляют собой области с резким изменением яркости, что позволяет определить границы объектов, формы и структуры на изображении. Такие операции широко применяются в системах распознавания, медицинской визуализации, автономной навигации и других прикладных задачах.

В рамках данной работы рассматривается использование оператора Собеля — одного из классических методов для приближённого вычисления градиента яркости изображения. Этот оператор позволяет выделять горизонтальные и вертикальные границы путём свёртки изображения с ядрами Собеля.

Целью практической работы является анализ подходов к реализации выделения рёбер с использованием оператора Собеля, а также подготовка к последующей оптимизации алгоритма с применением методов параллельного программирования. На этапе постановки задачи акцент делается на понимании математической сути операции и особенностях обработки изображений в дискретной форме.

\section*{Постановка задачи}

Целью настоящей работы является реализация и экспериментальное исследование алгоритма выделения рёбер на изображении с использованием оператора Собеля. Задача включает не только последовательную реализацию алгоритма, но и разработку его параллельных версий с использованием различных технологий параллельного программирования.

В рамках работы необходимо:

\begin{itemize}
    \item Реализовать базовый (последовательный) алгоритм выделения рёбер на изображении с использованием оператора Собеля.
    \item Разработать параллельные версии алгоритма с использованием следующих технологий:
    \begin{enumerate}
        \item OpenMP;
        \item Intel Threading Building Blocks (TBB);
        \item Стандартная библиотека потоков C++ (STL threads);
        \item MPI с вложенным использованием OpenMP (MPI + OpenMP).
    \end{enumerate}
    \item Провести тестирование и сравнение различных реализаций по времени выполнения и корректности результатов.
\end{itemize}

Результатом работы должно стать сравнение производительности реализованных версий и выводы об эффективности применения тех или иных средств параллельного программирования в задаче обработки изображений.

\section{Описание алгоритма}

Алгоритм выделения рёбер с использованием оператора Собеля основан на приближённом вычислении градиента яркости изображения. Для каждого пикселя рассчитываются значения производных по горизонтальному и вертикальному направлениям, после чего на их основе вычисляется итоговая величина градиента, характеризующая наличие и силу границы.

Изображение обрабатывается в оттенках серого, либо каждый канал (RGB) обрабатывается отдельно. В данной реализации применяется второй подход — градиент вычисляется независимо для каждого из трёх цветовых каналов.

Основные этапы алгоритма:

\begin{enumerate}
    \item Изображение представляется в виде одномерного массива пикселей с известными размерами (ширина и высота).
    \item Для каждой внутренней точки изображения (исключая граничные пиксели) вычисляются две свёртки с использованием ядер Собеля:
    \begin{itemize}
        \item Горизонтальное ядро (по оси \emph{X}):
        \[
        G_x =
        \begin{bmatrix}
        -1 & 0 & 1 \\
        -2 & 0 & 2 \\
        -1 & 0 & 1 \\
        \end{bmatrix}
        \]
        \item Вертикальное ядро (по оси \emph{Y}):
        \[
        G_y =
        \begin{bmatrix}
        -1 & -2 & -1 \\
         0 &  0 &  0 \\
         1 &  2 &  1 \\
        \end{bmatrix}
        \]
    \end{itemize}
    \item Для каждого пикселя в трёх каналах (R, G, B) отдельно суммируются произведения значений соседних пикселей и соответствующих коэффициентов ядра.
    \item Итоговая интенсивность границы в каждой компоненте вычисляется по формуле:
    \[
    M = \min\left(\sqrt{G_x^2 + G_y^2}, 255\right)
    \]
    где \emph{M} — итоговое значение интенсивности для конкретного цветового канала. Применяется ограничение по максимальному значению (255) для корректного отображения результата.
    \item Полученные значения записываются в выходной массив, соответствующий обработанному изображению.
\end{enumerate}

Данная реализация игнорирует граничные пиксели изображения (по краям), так как для них невозможно корректно применить ядро $3 \times 3$ без выхода за границы массива. 
Такая обработка позволяет сохранить вычислительную стабильность и избежать артефактов.

Таким образом, реализованный алгоритм последовательно обходит каждый пиксель (кроме границ) и производит локальные вычисления градиента, 
на основе которых строится итоговое изображение с выделенными рёбрами.

\section{Описание схемы параллельного алгоритма}

Обработка изображения оператором Собеля представляет собой независимые вычисления для каждого пикселя (за исключением граничных). Это делает задачу хорошо распараллеливаемой, 
поскольку вычисление градиента в каждой точке не зависит от других пикселей.

Основная идея параллелизации заключается в том, чтобы разделить изображение по строкам (или по блокам строк), и обрабатывать разные части изображения одновременно на нескольких потоках. 
Такой подход позволяет значительно сократить общее время выполнения алгоритма, особенно при обработке изображений большого размера.

В рамках данной работы использовалась модель **параллелизма по данным**, где каждый поток обрабатывает свою подзадачу — вычисление градиента Собеля в конкретной строке изображения. 
Граничные строки (верхняя и нижняя) не обрабатываются, так как для них невозможно применить ядро $3 \times 3$ без выхода за границы массива.

\section{Описание OpenMP-версии алгоритма}

Для реализации параллельного варианта алгоритма была использована технология OpenMP — стандарт программирования с общей памятью, 
позволяющий упростить распараллеливание циклов с помощью директив компилятора.

В данной версии была распараллелена внешняя петля по строкам изображения с использованием директивы `\texttt{\#pragma omp parallel for}`. 
Это позволяет автоматически распределить итерации цикла по доступным потокам выполнения:

\begin{itemize}
    \item Каждый поток независимо обрабатывает одну или несколько строк изображения, вычисляя градиент в каждой внутренней точке.
    \item Так как обработка каждой строки не зависит от других, не требуется синхронизация между потоками, что позволяет избежать накладных расходов на блокировки.
    \item Внутри параллельного цикла используются локальные переменные `sum\_x` и `sum\_y`, что предотвращает возникновение состояний гонки.
    \item Распараллеливание выполняется по строкам, начиная со второй и заканчивая предпоследней, поскольку для применения свёртки с ядром $3 \times 3$ необходимы данные соседних строк.
\end{itemize}

Использование OpenMP позволяет добиться прироста производительности при минимальных изменениях кода и без сложной настройки потоков вручную. 
Такой подход хорошо масштабируется на многопроцессорных системах с общей памятью.

\section{Описание TBB-версии алгоритма}

В данной реализации для распараллеливания обработки изображения используется библиотека Intel oneAPI Threading Building Blocks (TBB) — 
мощный инструмент для организации параллелизма с автоматическим управлением задачами и балансировкой нагрузки.
Параллелизация организована с использованием конструкции `\texttt{tbb::parallel\_for}` совместно с диапазоном `\texttt{tbb::blocked\_range}`, что позволяет автоматически разбивать диапазон строк изображения на поддиапазоны и распределять их между потоками.

Основные особенности реализации:

\begin{itemize}
    \item Распараллеливание осуществляется по строкам изображения, исключая граничные (первую и последнюю).
    \item Каждая подзадача (блок строк) обрабатывается независимо, что исключает необходимость синхронизации между потоками.
    \item Вычисления градиента проводятся в трёх каналах (RGB) по тем же формулам, что и в последовательной версии.
    \item Используется `\texttt{task\_arena}` для явного задания числа потоков в рамках инфраструктуры TBB, что обеспечивает согласованную работу с внешними средами (например, PPC-фреймворком).
\end{itemize}

Вызов `\texttt{arena.execute(...)}` гарантирует, что параллельный участок кода будет выполняться в специально выделенном пуле потоков, с числом потоков, 
соответствующим настройке окружения. Это важно для корректной интеграции TBB с другими параллельными компонентами.

Благодаря применению TBB достигается высокая степень масштабируемости и эффективность распределения нагрузки, особенно на многоядерных системах, 
где TBB динамически адаптирует выполнение к доступным ресурсам.

\section{Описание STL-версии алгоритма}

В данной реализации параллельного алгоритма используется стандартная библиотека потоков C++ (STL threads), 
которая предоставляет низкоуровневый механизм многопоточности без дополнительных абстракций планирования или автоматического управления задачами.

Параллелизация организована вручную, путём создания набора потоков, каждый из которых обрабатывает определённую часть строк изображения. 
Такой подход требует ручного деления задачи и явного управления диапазонами, передаваемыми каждому потоку.

Ключевые особенности реализации:

\begin{itemize}
    \item Общее количество потоков определяется на основе конфигурации окружения с помощью утилиты `\texttt{ppc::util::GetPPCNumThreads()}`.
    \item Изображение по строкам (исключая граничные строки) равномерно разбивается между потоками. При наличии остатка (`mod`) первые потоки получают на одну строку больше.
    \item Каждый поток запускается с помощью `\texttt{std::thread}` и получает замыкание, обрабатывающее выделенный диапазон строк.
    \item Внутри каждого потока выполняется полный расчёт градиента Собеля по всем трём каналам RGB.
    \item После завершения работы все потоки объединяются с помощью метода `\texttt{join()}`, чтобы обеспечить завершение вычислений до возврата из функции.
\end{itemize}

Данный способ обеспечивает явный контроль над потоками и позволяет наглядно понять механику многопоточности в C++. Однако по сравнению с OpenMP или TBB требует большего внимания к деталям: 
правильное распределение нагрузки, обработка диапазонов, отсутствие гонок и корректное завершение потоков.

\section{Описание MPI + OpenMP-версии алгоритма}

Данная версия алгоритма сочетает в себе два уровня параллелизма: распределённый с использованием библиотеки Boost.MPI и потоковый (многопроцессорный) с использованием OpenMP. 
Такой гибридный подход позволяет эффективно задействовать ресурсы как кластерной системы, так и многоядерных узлов.

Основная идея состоит в следующем: изображение распределяется между процессами по строкам с перекрытием граничных областей, а каждый процесс, 
получив свой участок, параллельно обрабатывает его с помощью OpenMP.

Ключевые этапы реализации:

\begin{itemize}
    \item На нулевом процессе изображение считывается и разбивается на фрагменты по строкам, с добавлением "строк-призраков" (overlap), необходимых для корректной свёртки по краям.
    \item С помощью функции `\texttt{boost::mpi::scatterv}` каждый процесс получает свой блок изображения с учётом необходимого перекрытия.
    \item После получения данных каждый процесс запускает параллельную обработку своего участка с помощью OpenMP. 
    Параллелизм реализован через директиву `\texttt{\#pragma omp parallel for}` по строкам блока.
    \item Градиент изображения вычисляется локально по тем же формулам, что и в предыдущих реализациях. 
    Обработка осуществляется для всех внутренних пикселей, исключая граничные строки в пределах локального блока.
    \item После завершения обработки результаты пересылаются обратно на нулевой процесс с помощью функции `\texttt{boost::mpi::gatherv}`.
    \item На главном процессе результирующее изображение собирается из полученных блоков и сохраняется в выходной буфер.
\end{itemize}

Особенности и преимущества:

\begin{itemize}
    \item Использование MPI позволяет распараллеливать обработку по узлам, что делает возможным масштабирование на кластерах.
    \item OpenMP обеспечивает дополнительный уровень ускорения за счёт многопоточности внутри каждого процесса.
    \item Распределение строк осуществляется с балансировкой нагрузки: оставшиеся строки добавляются к первым процессам.
    \item Для корректной обработки граничных участков каждому процессу передаются дополнительные строки сверху и снизу (если он не крайний).
\end{itemize}

Таким образом, гибридный подход MPI + OpenMP позволяет эффективно использовать как межузловой, так и внутрисистемный параллелизм, 
обеспечивая высокую производительность при обработке изображений большого размера.

\section{Результаты экспериментов}

Для оценки производительности реализованных версий алгоритма были проведены измерения времени выполнения на изображении размером $15\,000 \times 1\,000$ пикселей (в RGB-формате). 
Для каждой конфигурации производилось по 10 запусков, фиксировалось среднее время выполнения.

Тестирование проводилось на персональном компьютере со следующими характеристиками:

\begin{itemize}
    \item \textbf{Процессор}: AMD Ryzen 7 3700X (8 ядер, 16 потоков, 3.6~ГГц);
    \item \textbf{ОЗУ}: 32~ГБ DDR4;
    \item \textbf{ОС}: Windows 11 Pro x64;
    \item \textbf{Среда сборки}: WSL2 (Ubuntu 24.04) с поддержкой OpenMP, MPI и TBB.
\end{itemize}

Оценка производительности проводилась в двух режимах:
\begin{itemize}
    \item \texttt{PipelineRun} — замер всей цепочки (валидация, препроцессинг, вычисления, постобработка);
    \item \texttt{TaskRun} — замер исключительно ядра вычислений.
\end{itemize}

\subsection{Таблица производительности и ускорения}

\renewcommand{\arraystretch}{1.3}
\begin{table}[H]
\centering
\footnotesize
\begin{tabularx}{\textwidth}{|l|l|c|c|c|}
\hline
\textbf{Версия} & \textbf{Конфигурация} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Speedup} \\
\hline
Последовательная & — & 2.3049 & 2.1857 & 1.00 \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 1.0119 & 0.9277 & 2.36 \\
  & 3 потока & 0.7623 & 0.6470 & 3.38 \\
  & 5 потоков & 0.6501 & 0.4729 & \textbf{4.62} \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 1.7800 & 1.6431 & 1.33 \\
  & 3 потока & 1.2571 & 1.1094 & 1.97 \\
  & 5 потоков & 0.8297 & 0.7242 & 3.02 \\
\hline
\multirow{3}{*}{STL (std::thread)} 
  & 2 потока & 1.6333 & 1.5553 & 1.40 \\
  & 3 потока & 1.1733 & 1.1174 & 1.96 \\
  & 5 потоков & 0.8649 & 0.7442 & 2.94 \\
\hline
\multirow{5}{*}{MPI + OpenMP} 
  & 2 + 2 & 1.1142 & 1.0117 & 2.16 \\
  & 3 + 2 & 0.9272 & 0.8419 & 2.60 \\
  & 3 + 3 & 0.7815 & 0.6699 & 3.26 \\
  & 5 + 2 & 0.8023 & 0.6781 & 3.22 \\
  & 5 + 3 & 0.7049 & 0.5722 & \textbf{3.82} \\
\hline
\end{tabularx}
\caption{Сравнение производительности различных реализаций алгоритма}
\end{table}

\subsection{Анализ результатов}

\begin{itemize}
    \item Все параллельные реализации значительно опережают по времени выполнения последовательную.
    \item OpenMP демонстрирует наилучшее ускорение при увеличении числа потоков, особенно при 5 потоках.
    \item TBB показывает хорошую масштабируемость, но проигрывает OpenMP на малом числе потоков.
    \item STL уступает из-за необходимости ручного управления потоками и отсутствия автоматической балансировки.
    \item MPI + OpenMP показала высокую эффективность: при 5 процессах и 3 потоках достигнуто минимальное время выполнения.
    \item При большом числе MPI-процессов накладные расходы на коммуникацию могут замедлять работу.
\end{itemize}

Оптимальной реализацией с точки зрения скорости стала \textbf{MPI + OpenMP (5 + 3)}, 
однако наиболее простой и эффективной на одной машине остаётся \textbf{OpenMP (5 потоков)}.

\section{Заключение}

В ходе выполнения работы был реализован и протестирован алгоритм выделения рёбер на изображении с использованием оператора Собеля. 
Разработка велась в нескольких вариантах: последовательная реализация, а также параллельные версии с применением технологий OpenMP, Intel TBB, 
стандартной библиотеки потоков STL и гибридной модели MPI + OpenMP.

Проведённое тестирование показало, что использование параллельных вычислений позволяет значительно сократить время выполнения задачи по сравнению с последовательной реализацией. 
Наибольшее ускорение достигнуто при использовании OpenMP с 5 потоками и при гибридной конфигурации MPI + OpenMP (5 процессов × 3 потока), 
что подтверждает эффективность подхода к масштабированию как в пределах одного узла, так и при использовании распределённых вычислений.

Также была реализована система измерения производительности, позволившая объективно сравнить разные версии алгоритма. 
Таблица продемонстрировала влияние числа потоков и процессов на скорость выполнения и эффективность параллелизации.

Работа позволила углубить понимание особенностей различных моделей параллельного программирования и получить практические навыки их применения к задачам обработки изображений.

\section{Список литературы}

\begin{enumerate}
    \item Статья: \textit{Что такое OpenMP?}
    \item Мещеряков И.Н. \textit{Набор и вёрстка в системе \LaTeX}
    \item Сысоев А.В., Мееров И.Б., Сиднев А.А. \textit{Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks}. — Нижний Новгород, 2007.
    \item Статья: \textit{Технологии параллельного программирования. Message Passing Interface (MPI)}
\end{enumerate}

\section{Приложение}

В данном разделе приведены ключевые фрагменты реализации алгоритма выделения рёбер с использованием оператора Собеля для разных технологий параллельного программирования. 
Представлены только основные вычислительные фрагменты (функции \texttt{RunImpl}), отражающие особенности каждой версии.

\newpage

\subsection{Последовательная версия}
\begin{lstlisting}[language=C++]
for (std::size_t y = 1; y < height - 1; ++y) {
  for (std::size_t x = 1; x < width - 1; ++x) {
    std::array<int32_t, 3> sum_x{0}, sum_y{0};
    for (int ky = -1; ky <= 1; ++ky) {
      for (int kx = -1; kx <= 1; ++kx) {
        int idx = ((y + ky) * width + (x + kx)) * 3;
        for (int j = 0; j < 3; j++) {
          sum_x[j] += kSobelX[ky + 1][kx + 1] * image[idx + j];
          sum_y[j] += kSobelY[ky + 1][kx + 1] * image[idx + j];
        }
      }
    }
    for (int i = 0; i < 3; ++i) {
      out_.data[((y * width + x) * 3) + i] =
        static_cast<uint8_t>(
          std::min(static_cast<int32_t>(
            std::sqrt(sum_x[i]*sum_x[i] 
            + sum_y[i]*sum_y[i])), 255));
    }
  }
}
\end{lstlisting}

\newpage

\subsection{OpenMP-версия}
\begin{lstlisting}[language=C++]
#pragma omp parallel for
for (int y = 1; y < static_cast<int>(height) - 1; ++y) {
  for (int x = 1; x < static_cast<int>(width) - 1; ++x) {
    std::array<int32_t, 3> sum_x{0}, sum_y{0};
    // the same convolution cycle as in the sequential version
  }
}
\end{lstlisting}

\subsection{TBB-версия}
\begin{lstlisting}[language=C++]
tbb::parallel_for(
  tbb::blocked_range<std::size_t>(1, height - 1),
  [&](const tbb::blocked_range<std::size_t>& r) {
    for (std::size_t y = r.begin(); y < r.end(); ++y) {
      for (std::size_t x = 1; x < width - 1; ++x) {
        std::array<int32_t, 3> sum_x{0}, sum_y{0};
        // similar processing of gradients
      }
    }
  }
);
\end{lstlisting}

\subsection{STL (std::thread) версия}
\begin{lstlisting}[language=C++]
for (std::size_t threadnum = 0; threadnum < threads.size(); threadnum++) {
  threads[threadnum] = std::thread(
    [&](std::size_t lidx, std::size_t ridx) {
      for (std::size_t y = lidx; y < ridx; ++y) {
        for (std::size_t x = 1; x < width - 1; ++x) {
          std::array<int32_t, 3> sum_x{0}, sum_y{0};
          // standard convolution
        }
      }
    }, pos, pos + dedicated);
}
\end{lstlisting}

\newpage

\subsection{MPI + OpenMP версия}
\begin{lstlisting}[language=C++]
if (world_.rank() == 0) {
  // Preparing in_.data and splitting it into blocks
}

boost::mpi::broadcast(world_, width, 0);
boost::mpi::broadcast(world_, total_height, 0);

std::vector<int> sendcounts(numproc), senddispls(numproc);
std::vector<int> recvcounts(numproc), recvdispls(numproc);

boost::mpi::scatterv(
  world_, in_.data, sendcounts, senddispls,
  partial_image.data(), sendcounts[rank], 0);

#pragma omp parallel for
for (int y = 1; y < height - 1; ++y) {
  for (int x = 1; x < width - 1; ++x) {
    std::array<int32_t, 3> sum_x{0}, sum_y{0};
    // Processing partial_image
  }
}

boost::mpi::gatherv(
  world_, partial_out.data() + (rank == 0 ? 0 : row_stride),
  recvcounts[rank], out_.data.data(),
  recvcounts, recvdispls, 0);
\end{lstlisting}

\end{document}