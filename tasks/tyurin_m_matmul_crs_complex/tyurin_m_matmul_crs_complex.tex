\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}

\usepackage{amsmath,amssymb,siunitx}
\sisetup{
    output-decimal-marker={,},
    group-digits = false
}

\usepackage{geometry}
\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}

\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{tabularx}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    frame=single,
    tabsize=4,
    showstringspaces=false,
    breaklines=true
}

\usepackage{titlesec}
\titleformat{\section}{\normalfont\Large\bfseries\centering}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection.}{1em}{}

\usepackage{indentfirst}

\setlength{\parindent}{1.25cm}
\setlength{\parskip}{0pt}

\begin{document}

\begin{titlepage}
\begin{center}

\onehalfspacing

\textbf{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ} \\[0.3cm]
Федеральное государственное автономное образовательное учреждение высшего образования \\[0.3cm]
\textbf{«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»} (ННГУ) \\[0.3cm]
Институт информационных технологий, математики и механики \\[2cm]

{\Large \textbf{ОТЧЕТ}} \\[0.5cm]
по курсу \\[0.3cm]
{\large «Параллельное программирование»} \\[1cm]

\textbf{Тема задачи:} \\[0.3cm]
{\large Умножение разреженных матриц. Элементы комплексного типа.Формат хранения матрицы — строковый (CRS)} \\

\vfill
\begin{flushright}
\textbf{Выполнил:} \\
студент 3 курса \\
группа 3822Б1ПР1 \\
Тюрин М. Д. \\[1cm]

\textbf{Проверил:} \\
доцент кафедры ВВСП, к.т.н. \\ 
Сысоев А.В.
\end{flushright}

\vfill

Нижний Новгород \\
2025

\end{center}
\end{titlepage}

\newpage

\onehalfspacing

\section{Введение}

В современном вычислительном мире задачи обработки больших объёмов данных требуют эффективных алгоритмов и использования возможностей параллельных вычислений. Особую роль в различных областях науки и техники занимают операции с разреженными матрицами, которые содержат преимущественно нулевые элементы. 
Эффективное умножение разреженных матриц позволяет  значительно ускорить вычисления при сохранении экономии памяти.

В данной работе рассматривается задача умножения двух разреженных матриц, элементы которых являются комплексными числами. Матрицы хранятся в формате \textit{Compressed Row Storage} (CRS) — компактном формате, позволяющем хранить только ненулевые элементы и необходимые для доступа к ним индексы.  
Формат CRS широко применяется в научных вычислениях для работы с разреженными матрицами.

Целью работы является реализация алгоритма умножения разреженных комплексных матриц в формате CRS и его параллельная оптимизация с использованием различных технологий: OpenMP, Intel Threading Building Blocks (TBB), стандартных потоков библиотеки STL и гибридного подхода MPI+STL. 
Использование нескольких подходов позволяет сравнить производительность и оценить эффективность каждой из технологий при решении поставленной задачи.

В отчёте описаны особенности реализации алгоритма, применённые методы распараллеливания, а также представлены результаты тестирования производительности на различных наборах данных.

\newpage

\section{Постановка задачи}

В данной работе рассматривается задача эффективного умножения двух разреженных  
комплексных матриц, представленных в формате CRS (Compressed Row Storage).  
Основное внимание уделяется не только корректной последовательной реализации,  
но и созданию высокопроизводительных параллельных версий алгоритма.

\vspace{0.3cm}

Для успешного решения поставленной задачи необходимо:

\begin{itemize}
    \item Разработать и реализовать последовательный алгоритм умножения разреженных матриц, учитывающий особенности комплексных элементов и формат хранения CRS.
    
    \vspace{0.2cm}
    \item Реализовать параллельные варианты алгоритма с применением следующих технологий:
    \begin{itemize}
        \item OpenMP — для распараллеливания вычислений на уровне потоков с общей памятью;
        \item Intel Threading Building Blocks (TBB) — библиотека для автоматического управления параллельными задачами;
        \item std::thread — стандартные средства многопоточности C++ для ручного управления потоками;
        \item Комбинированный подход MPI + std::thread — для реализации распределённых и многопоточных вычислений.
    \end{itemize}
    
    \vspace{0.2cm}
    \item Провести всестороннее тестирование и сравнение всех вариантов алгоритма по критериям корректности и производительности.
\end{itemize}

\vspace{0.3cm}

В результате работы планируется получить подробное сравнение эффективности различных параллельных методов и рекомендации по выбору оптимальной реализации для задач умножения разреженных матриц с комплексными элементами.

\newpage

\section{Описание алгоритма}

В работе реализован алгоритм умножения разреженных матриц, элементы которых представлены комплексными числами. Для хранения матриц используется формат \textit{Compressed Row Storage} (CRS), позволяющий эффективно хранить только ненулевые элементы, что существенно экономит память и снижает вычислительную нагрузку.

Алгоритм состоит из нескольких основных этапов:

\begin{enumerate}
    \item \textbf{Представление матриц.} Исходные матрицы сначала представлены в обычном плотном формате — двумерном массиве комплексных чисел. Для преобразования в формат CRS выполняется обход всех элементов, при этом ненулевые значения сохраняются в отдельный массив, а для каждой строки фиксируются индексы начала и конца элементов (вектора \texttt{rowptr} и \texttt{colind}).
    
    \item \textbf{Транспонирование второй матрицы.} Для удобства умножения и эффективного доступа к столбцам в формате CRS вторая матрица транспонируется. Это позволяет свести операцию умножения к проходу по строкам обеих матриц, что упрощает сравнение индексов ненулевых элементов.
    
    \item \textbf{Умножение матриц.} Для каждого элемента результирующей матрицы вычисляется скалярное произведение соответствующих строк первой матрицы и транспонированной второй матрицы. При этом осуществляется проход по индексам ненулевых элементов обеих строк с синхронизацией позиций с помощью сравнений индексов столбцов. 
    Если индексы совпадают, умножаются соответствующие элементы и результат аккумулируется в сумму. В случае различия индексов производится сдвиг указателей, что позволяет эффективно пропускать нулевые элементы.
    
    \item \textbf{Сбор результата.} Если сумма произведений ненулевая, она добавляется в результатирующую структуру CRS вместе с соответствующим индексом столбца. Для каждой строки фиксируется обновлённое значение в векторе \texttt{rowptr}, отражающее количество ненулевых элементов.
\end{enumerate}

Реализация обеспечивает корректное умножение разреженных матриц с комплексными элементами, при этом все операции выполняются с учётом особенностей формата CRS. Использование транспонирования позволяет значительно упростить и оптимизировать проходы по структурам данных.

Данный алгоритм служит базовой последовательной реализацией, на основе которой в дальнейшем выполняется параллелизация с использованием различных технологий и библиотек.

\newpage

\section{Описание схемы параллельного алгоритма}

Умножение разреженных комплексных матриц в формате CRS предполагает большое количество независимых операций, что делает задачу хорошо распараллеливаемой.

Основная идея параллельного алгоритма заключается в том, что каждая строка первой матрицы может быть обработана независимо от других. Это позволяет распределить строки между потоками и выполнять их обработку одновременно.

Для ускорения доступа к данным транспонированная версия второй матрицы предварительно строится также в CRS-формате.

Параллельная реализация сводится к тому, что внешняя итерация по строкам первой матрицы делится между потоками. Каждый поток получает на обработку определённый диапазон строк и формирует ненулевые элементы результата, не конфликтуя с другими потоками.

При реализации алгоритма важно учитывать, что запись результата в CRS-структуру также должна быть организована без гонок данных. Для этого каждая нить временно сохраняет свои данные в локальный буфер, после чего они объединяются в итоговую матрицу.

Такой подход позволяет эффективно распараллелить вычисления при сохранении корректности результата, особенно при больших размерах матриц и высокой степени их разреженности.

\newpage

\section{Описание OpenMP-версии алгоритма}

OpenMP-версия алгоритма реализует параллельное умножение разреженных матриц комплексного типа, представленных в формате CRS (Compressed Row Storage).

Основные особенности и этапы алгоритма:

\begin{enumerate}
  \item \textbf{Представление данных:}
  \begin{itemize}
    \item Матрицы хранятся в формате CRS, который включает:
      \begin{itemize}
        \item массив ненулевых значений \texttt{data};
        \item массив индексов столбцов ненулевых элементов \texttt{colind};
        \item массив указателей на начало каждой строки \texttt{rowptr}.
      \end{itemize}
    \item Правая матрица транспонируется, чтобы упростить доступ к её столбцам, 
    преобразовав её структуру CRS.
  \end{itemize}

  \item \textbf{Параллельное умножение:}
  \begin{itemize}
    \item Основной цикл по строкам левой матрицы распараллеливается с помощью директивы 
    \texttt{\#pragma omp parallel for}.
    \item Для каждой пары строк — строки из левой матрицы и строки из транспонированной правой матрицы — вычисляется скалярное произведение с помощью прохода двумя указателями по ненулевым элементам.
    \item Суммируются произведения совпадающих по индексу столбца элементов.
    \item Если результат скалярного произведения не равен нулю, он сохраняется во временный буфер для текущей строки.
  \end{itemize}

  \item \textbf{Формирование результирующей матрицы:}
  \begin{itemize}
    \item По завершении параллельных вычислений происходит последовательное объединение буферов каждой строки в итоговую структуру CRS результирующей матрицы.
    \item Заполняются массивы \texttt{data}, \texttt{colind} и \texttt{rowptr}.
  \end{itemize}
\end{enumerate}

Использование OpenMP позволяет эффективно распараллелить умножение, распределяя строки левой матрицы между потоками, что значительно улучшает производительность на многопроцессорных системах.

\newpage

\section{Описание TBB-версии алгоритма}

TBB-версия алгоритма реализует умножение разреженных матриц комплексного типа в формате CRS (Compressed Row Storage) с использованием библиотеки Intel TBB для эффективного параллельного выполнения.

Основные особенности и этапы алгоритма:

\begin{enumerate}
  \item \textbf{Представление данных:}
  \begin{itemize}
    \item Матрицы представлены структурами \texttt{Matrix} для плотного формата и \texttt{MatrixCRS} для формата CRS.
    \item Формат CRS содержит массив ненулевых значений \texttt{data}, массив индексов столбцов \texttt{colind} и массив указателей на начало каждой строки \texttt{rowptr}.
  \end{itemize}

  \item \textbf{Преобразование формата:}
  \begin{itemize}
    \item Функция \texttt{RegularToCRS} преобразует плотную матрицу в CRS.
    \item Функция \texttt{CRSToRegular} восстанавливает плотный формат из CRS.
    \item Правая матрица транспонируется функцией \texttt{TransposeMatrixCRS} для упрощения доступа к столбцам.
  \end{itemize}

  \item \textbf{Параллельное умножение с использованием TBB:}
  \begin{itemize}
    \item Выполнение параллельного цикла \texttt{oneapi::tbb::parallel\_for} по строкам левой матрицы.
    \item Для каждой строки и каждого столбца транспонированной правой матрицы вычисляется скалярное произведение ненулевых элементов с использованием двух указателей.
    \item Ненулевые результаты временно сохраняются в буфере для каждой строки.
  \end{itemize}

  \item \textbf{Формирование результирующей матрицы:}
  \begin{itemize}
    \item После завершения параллельных вычислений данные из буферов объединяются в итоговую структуру CRS.
    \item Заполняются массивы \texttt{data}, \texttt{colind} и \texttt{rowptr} результирующей матрицы.
  \end{itemize}
\end{enumerate}

Использование Intel TBB обеспечивает эффективное распараллеливание на уровне потоков, облегчая балансировку нагрузки и масштабируемость вычислений с минимальными накладными расходами.

\newpage

\section{Описание STL-версии алгоритма}

STL-версия алгоритма реализует умножение разреженных матриц комплексного типа в формате CRS (Compressed Row Storage) с использованием стандартных контейнеров и потоков C++.

Основные особенности и этапы алгоритма:

\begin{enumerate}
  \item \textbf{Представление данных:}
  \begin{itemize}
    \item Матрицы представлены структурами \texttt{Matrix} для плотного формата и \texttt{MatrixCRS} для формата CRS.
    \item Формат CRS включает массив ненулевых значений \texttt{data}, массив индексов столбцов ненулевых элементов \texttt{colind} и массив указателей на начало каждой строки \texttt{rowptr}.
  \end{itemize}

  \item \textbf{Преобразование форматов:}
  \begin{itemize}
    \item Функция \texttt{RegularToCRS} преобразует плотную матрицу в CRS.
    \item Функция \texttt{CRSToRegular} восстанавливает плотный формат из CRS.
    \item Правая матрица транспонируется функцией \texttt{TransposeMatrixCRS} для упрощения доступа к её столбцам.
  \end{itemize}

  \item \textbf{Параллельное умножение:}
  \begin{itemize}
    \item Строки левой матрицы распределяются по потокам с помощью \texttt{std::thread}.
    \item Для каждой строки и каждого столбца транспонированной правой матрицы вычисляется скалярное произведение двумя указателями по ненулевым элементам.
    \item Результаты ненулевых произведений сохраняются во временный буфер.
  \end{itemize}

  \item \textbf{Формирование результирующей матрицы:}
  \begin{itemize}
    \item По окончании вычислений буферы объединяются в итоговую структуру CRS.
    \item Заполняются массивы \texttt{data}, \texttt{colind} и \texttt{rowptr}.
  \end{itemize}
\end{enumerate}

Использование стандартных потоков и контейнеров STL обеспечивает удобство и переносимость. При этом достигается эффективное распараллеливание и работа с разреженными матрицами.

\newpage

\section{Описание MPI + STL-версии алгоритма}

MPI + STL-версия алгоритма реализует умножение разреженных матриц комплексного типа в формате CRS (Compressed Row Storage) с использованием распределённых вычислений на базе MPI и стандартных контейнеров и потоков C++.

Основные особенности и этапы алгоритма:

\begin{enumerate}
  \item \textbf{Представление данных:}
  \begin{itemize}
    \item Матрицы представлены структурами \texttt{Matrix} для плотного формата и \texttt{MatrixCRS} для формата CRS.
    \item Формат CRS включает массив ненулевых значений \texttt{data}, массив индексов столбцов ненулевых элементов \texttt{colind} и массив указателей на начало каждой строки \texttt{rowptr}.
  \end{itemize}

  \item \textbf{Распределение данных:}
  \begin{itemize}
    \item Правая матрица транспонируется функцией \texttt{TransposeMatrixCRS} для удобного доступа к столбцам.
    \item Строки левой матрицы распределяются между процессами MPI поровну.
    \item Корневой процесс отправляет подматрицы остальным процессам с помощью MPI.
  \end{itemize}

  \item \textbf{Параллельное умножение:}
  \begin{itemize}
    \item Каждый процесс умножает свою часть левой матрицы на всю транспонированную правую матрицу.
    \item Внутри каждого процесса строки распределяются по потокам \texttt{std::thread} для распараллеливания вычислений.
    \item Для каждой пары строк и столбцов вычисляется скалярное произведение ненулевых элементов.
  \end{itemize}

  \item \textbf{Сбор результатов:}
  \begin{itemize}
    \item Каждый процесс формирует свою часть результирующей матрицы в формате CRS.
    \item Результаты собираются корневым процессом с помощью MPI и объединяются в итоговую матрицу.
  \end{itemize}
\end{enumerate}

Использование MPI обеспечивает масштабируемость и распределённость вычислений, а стандартные контейнеры и потоки C++ позволяют эффективно реализовать распараллеливание на уровне каждого процесса.

\newpage

\section{Результаты экспериментов}

\hspace*{1.25em}Были проведены замеры времени выполнения реализаций умножения разреженных комплексных матриц в формате CRS с использованием различных технологий.  

\hspace*{1.25em}Каждая версия тестировалась в двух режимах:  \texttt{PipelineRun} и \texttt{TaskRun}. 

\hspace*{1.25em}Для оценки использовались случайные матрицы размером $730 \times 730$ с плотностью 22\%.  

\hspace*{1.25em}В таблице приведены значения времени выполнения (в секундах), а также ускорение по сравнению с последовательной реализацией.  

\hspace*{1.25em}Эксперименты охватывают реализации на OpenMP, Intel TBB, \texttt{std::thread}, а также гибридную версию на MPI с потоками.  

\subsection{Таблица производительности и ускорения}
\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Метод} & \textbf{Конфигурация} & \textbf{PipelineRun, с} & \textbf{TaskRun, с} & \textbf{Ускорение} \\
\hline
\textbf{Последовательная} & — & 7.8850 & 7.9583 & \textbf{1.00} \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 4.0249 & 4.0757 & 1.95 \\
  & 3 потока & 2.7360 & 2.7015 & 2.95 \\
  & 5 потоков & 1.7497 & 1.7281 & \textbf{4.61} \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 4.0853 & 4.0975 & 1.94 \\
  & 3 потока & 2.7800 & 2.7701 & 2.87 \\
  & 5 потоков & 1.7780 & 1.7663 & \textbf{4.51} \\
\hline
\multirow{3}{*}{std::thread} 
  & 2 потока & 4.3395 & 4.3417 & 1.83 \\
  & 3 потока & 3.0044 & 2.9881 & 2.66 \\
  & 5 потоков & 1.8954 & 1.9403 & \textbf{4.10} \\
\hline
\multirow{3}{*}{MPI + STL (1 процесс)} 
  & 2 потока & 5.5943 & 5.4959 & 1.45 \\
  & 3 потока & 5.8640 & 5.7132 & 1.39 \\
  & 5 потоков & 5.6496 & 5.6000 & \textbf{1.42} \\
\hline
\multirow{3}{*}{MPI + STL (2 процесса)} 
  & 2 потока & 2.5555 & 2.4755 & 3.21 \\
  & 3 потока & 2.6468 & 2.5401 & 3.13 \\
  & 5 потоков & 2.6469 & 2.6193 & \textbf{3.04} \\
\hline
\end{tabular}
\caption{Сравнение производительности различных реализаций алгоритма умножения разреженных матриц}
\label{tab:new_parallel_perf}
\end{table}

\subsection{Анализ и сравнение реализаций}

На основании экспериментальных данных можно сформулировать следующие выводы:

\vspace{1em}
\textbf{1. Последовательная реализация}
Последовательный вариант используется в качестве базового ориентира. Время выполнения составляет около 7.9 секунд. Все параллельные реализации демонстрируют значительное снижение времени выполнения по сравнению с последовательной.

\vspace{1em}
\textbf{2. OpenMP}
Реализация с использованием OpenMP показывает наивысшее ускорение среди всех протестированных подходов. При увеличении числа потоков наблюдается стабильное уменьшение времени выполнения, а при использовании 5 потоков достигается максимальное ускорение — \textbf{4.61}. Этот подход отличается высокой эффективностью и хорошей масштабируемостью при выполнении на одном вычислительном узле.

\vspace{1em}
\textbf{3. Intel TBB}
Технология TBB демонстрирует схожие с OpenMP результаты, однако уступает ей на всех уровнях параллелизма. При использовании 5 потоков ускорение составляет \textbf{4.51}, что также указывает на высокую степень масштабируемости.

\vspace{1em}
\textbf{4. std::thread}
При реализации параллельности с использованием стандартной библиотеки потоков C++ (\texttt{std::thread}) достигнуто ускорение до \textbf{4.10}. Тем не менее, эффективность несколько ниже по сравнению с OpenMP и TBB. Дополнительные трудозатраты на управление потоками и синхронизацию делают этот подход менее предпочтительным.

\vspace{1em}
\textbf{5. MPI + STL}
\begin{itemize}
\item В конфигурации с одним процессом использование многопоточности приводит к незначительному увеличению производительности (ускорение до \textbf{1.45}). Вероятной причиной являются накладные расходы на создание и координацию потоков в рамках одного процесса.
\item При использовании двух MPI-процессов наблюдается улучшение производительности (ускорение до \textbf{3.21}), однако полученные значения уступают результатам OpenMP и TBB. Увеличение числа процессов сопровождается дополнительными затратами на межпроцессное взаимодействие, что ограничивает эффективность.
\end{itemize}

\vspace{1em}
\textbf{Вывод:}
Наиболее эффективной с точки зрения производительности и удобства реализации на одном узле является реализация с использованием \textbf{OpenMP (5 потоков)}. Использование MPI целесообразно при построении распределённых вычислений, однако в рассматриваемом случае оно не обеспечило превосходства по скорости по сравнению с многопоточными решениями.

\newpage

\section{Заключение}

В ходе выполнения лабораторной работы были разработаны и реализованы несколько вариантов алгоритма умножения разреженных комплексных матриц в формате CRS с применением различных технологий параллельного и распределённого программирования: OpenMP, Intel TBB, стандартных потоков C++ (\texttt{std::thread}), а также гибридного подхода MPI + STL.

В процессе работы были подробно изучены особенности формата CRS, реализованы функции преобразования между плотным и разреженным форматами, а также транспонирования матриц для удобства доступа к столбцам. Для каждой технологии была создана версия параллельного умножения, включающая эффективное распределение вычислительной нагрузки и сохранение результатов в формате CRS.

Экспериментальное тестирование реализованных алгоритмов с использованием случайных матриц размером $730 \times 730$ и плотностью 22\% позволило оценить производительность каждой реализации. Полученные результаты продемонстрировали значительное снижение времени выполнения по сравнению с последовательной версией, подтверждая правильность и эффективность применённых методов параллелизации.

Наилучшие показатели показали реализации с использованием OpenMP и Intel TBB, обеспечившие ускорение более чем в 4 раза при работе с 5 потоками. Версия с использованием стандартных потоков C++ показала несколько меньшую производительность, но сохранила переносимость и гибкость. Гибридная версия на MPI с распределением по процессам и многопоточностью внутри каждого процесса продемонстрировала масштабируемость, однако из-за коммуникационных накладных расходов не превзошла локальные многопоточные реализации.

В результате проделанной работы были получены не только высокоэффективные программные решения, но и проведён глубокий анализ преимуществ и ограничений различных технологий параллельного и распределённого программирования для задачи умножения разреженных матриц.

Дальнейшие направления развития включают оптимизацию межпроцессного взаимодействия в MPI-версии, расширение поддержки гетерогенных вычислительных платформ и интеграцию алгоритмов в прикладные научные приложения.

\newpage

\section{Список литературы}

\begin{enumerate}
    \item Сысоев А.В., Мееров И.Б., Сиднев А.А. \textit{Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks}. — Нижний Новгород, 2007. — 112 с.
    \item Грама А., Гупта А., Карипис Г., Кумар В. \textit{Введение в параллельные вычисления}. — 2-е изд., перераб. и доп. — М.: Мир, 2003. — 392 с.
    \item Саад Ю. \textit{Итеративные методы решения разреженных линейных систем}. — М.: Наука, 2003. — 415 с.
    \item Message Passing Interface Forum. \textit{MPI: A Message-Passing Interface Standard, Version 3.1}. — 2015.
    \item OpenMP Architecture Review Board. \textit{OpenMP Application Programming Interface, Version 4.5}. — 2015.
\end{enumerate}

\newpage

\section{Приложение}

В данном приложении представлены основные реализации алгоритма умножения разреженных матриц в формате CRS (Compressed Row Storage). Рассмотрены как последовательный вариант, так и параллельные реализации с использованием различных технологий: OpenMP, Intel TBB, стандартных потоков STL (std::thread), а также гибридный вариант с использованием MPI и STL-потоков.

\subsection{Последовательная реализация CRS-умножения (Sequential)}

\begin{lstlisting}[language=C++,caption={Реализация CRS-умножения матриц в последовательной версии}]
bool TestTaskSequential::RunImpl() {
  const auto rows = lhs_.GetRows();
  const auto cols = rhs_.GetRows();

  for (uint32_t i = 0; i < rows; ++i) {
    for (uint32_t j = 0; j < cols; ++j) {
      auto ii = lhs_.rowptr[i];
      auto ij = rhs_.rowptr[j];
      std::complex<double> summul = 0.0;
      while (ii < lhs_.rowptr[i + 1] && ij < rhs_.rowptr[j + 1]) {
        if (lhs_.colind[ii] < rhs_.colind[ij]) {
          ++ii;
        } else if (lhs_.colind[ii] > rhs_.colind[ij]) {
          ++ij;
        } else {
          summul += lhs_.data[ii++] * rhs_.data[ij++];
        }
      }
      if (summul != 0.0) {
        res_.data.push_back(summul);
        res_.colind.push_back(j);
      }
    }
    res_.rowptr[i + 1] = res_.data.size();
  }

  return true;
}
\end{lstlisting}

\subsection{Параллельная реализация CRS-умножения с использованием OpenMP}

\begin{lstlisting}[language=C++,caption={CRS-умножение матриц с использованием OpenMP}]
bool TestTaskOpenMP::RunImpl() {
  const auto rows = lhs_.GetRows();
  const auto cols = rhs_.GetRows();

  std::vector<std::vector<std::tuple<std::complex<double>, uint32_t>>> buf(rows);

  #pragma omp parallel for schedule(static)
  for (int i = 0; i < static_cast<int>(rows); ++i) {
    for (uint32_t j = 0; j < cols; ++j) {
      auto ii = lhs_.rowptr[i];
      auto ij = rhs_.rowptr[j];
      std::complex<double> summul = 0.0;
      while (ii < lhs_.rowptr[i + 1] && ij < rhs_.rowptr[j + 1]) {
        if (lhs_.colind[ii] < rhs_.colind[ij]) {
          ++ii;
        } else if (lhs_.colind[ii] > rhs_.colind[ij]) {
          ++ij;
        } else {
          summul += lhs_.data[ii++] * rhs_.data[ij++];
        }
      }
      if (summul != 0.0) {
        buf[i].emplace_back(summul, j);
      }
    }
  }

  for (uint32_t i = 0; i < rows; ++i) {
    res_.rowptr[i + 1] = res_.rowptr[i];
    for (const auto& [summul, j] : buf[i]) {
      res_.data.push_back(summul);
      res_.colind.push_back(j);
      ++res_.rowptr[i + 1];
    }
  }

  return true;
}
\end{lstlisting}

\subsection{Параллельная реализация CRS-умножения с использованием Intel TBB}

\begin{lstlisting}[language=C++,caption={CRS-умножение матриц с использованием Intel TBB}]
bool TestTaskTbb::RunImpl() {
  const auto rows = lhs_.GetRows();
  const auto cols = rhs_.GetRows();

  std::vector<std::vector<std::tuple<std::complex<double>, uint32_t>>> buf(rows);

  oneapi::tbb::task_arena arena(ppc::util::GetPPCNumThreads());
  arena.execute([&] {
    oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, rows),
                              [&](const tbb::blocked_range<std::size_t> &r) {
        for (uint32_t i = r.begin(); i < r.end(); ++i) {
          for (uint32_t j = 0; j < cols; ++j) {
            auto ii = lhs_.rowptr[i];
            auto ij = rhs_.rowptr[j];
            std::complex<double> summul = 0.0;
            while (ii < lhs_.rowptr[i + 1] && ij < rhs_.rowptr[j + 1]) {
              if (lhs_.colind[ii] < rhs_.colind[ij]) {
                ++ii;
              } else if (lhs_.colind[ii] > rhs_.colind[ij]) {
                ++ij;
              } else {
                summul += lhs_.data[ii++] * rhs_.data[ij++];
              }
            }
            if (summul != 0.0) {
              buf[i].emplace_back(summul, j);
            }
          }
        }
    });
    return;
  });

  for (uint32_t i = 0; i < rows; ++i) {
    res_.rowptr[i + 1] = res_.rowptr[i];
    for (const auto& [summul, j] : buf[i]) {
      res_.data.push_back(summul);
      res_.colind.push_back(j);
      ++res_.rowptr[i + 1];
    }
  }

  return true;
}
\end{lstlisting}

\subsection{Параллельная реализация CRS-умножения с использованием STL и std::thread}

\begin{lstlisting}[language=C++,caption={CRS-умножение матриц с использованием стандартных потоков STL}]
bool tyurin_m_matmul_crs_complex_stl::TestTaskStl::RunImpl() {
  const auto rows = lhs_.GetRows();
  const auto cols = rhs_.GetRows();

  std::vector<std::vector<std::tuple<std::complex<double>, uint32_t>>> buf(rows);

  const std::size_t nthreads = ppc::util::GetPPCNumThreads();
  const std::size_t avg = rows / nthreads;
  const std::size_t nextra = rows % nthreads;

  std::vector<std::thread> threads(nthreads);
  uint32_t cur = 0;
  for (std::size_t t = 0; t < nthreads; t++) {
    uint32_t forthread = avg + ((t < nextra) ? 1 : 0);
    threads[t] = std::thread(
        [&](uint32_t thread_rows_begin, uint32_t thread_rows_end) {
          for (uint32_t i = thread_rows_begin; i < thread_rows_end; ++i) {
            for (uint32_t j = 0; j < cols; ++j) {
              auto ii = lhs_.rowptr[i];
              auto ij = rhs_.rowptr[j];
              std::complex<double> summul = 0.0;
              while (ii < lhs_.rowptr[i + 1] && ij < rhs_.rowptr[j + 1]) {
                if (lhs_.colind[ii] < rhs_.colind[ij]) {
                  ++ii;
                } else if (lhs_.colind[ii] > rhs_.colind[ij]) {
                  ++ij;
                } else {
                  summul += lhs_.data[ii++] * rhs_.data[ij++];
                }
              }
              if (summul != 0.0) {
                buf[i].emplace_back(summul, j);
              }
            }
          }
        },
        cur, cur + forthread);
    cur += forthread;
  }
  for (auto& thread : threads) {
    thread.join();
  }

  for (uint32_t i = 0; i < rows; i++) {
    res_.rowptr[i + 1] = res_.rowptr[i];
    for (const auto& [summul, j] : buf[i]) {
      res_.data.push_back(summul);
      res_.colind.push_back(j);
      ++res_.rowptr[i + 1];
    }
  }

  return true;
}
\end{lstlisting}

\subsection{Параллельная реализация CRS-умножения с использованием MPI и STL с потоками}

\begin{lstlisting}[language=C++,caption={CRS-умножение матриц с использованием MPI и стандартных потоков STL}]
bool tyurin_m_matmul_crs_complex_all::TestTaskAll::RunImpl() {
  int row_offset{};
  int idx_offset{};
  auto local_lhs = Scatter(row_offset, idx_offset);

  MatrixCRS local_res{};
  local_res.rowptr.resize(local_lhs.GetRows() + 1);
  local_res.cols_count = rhs_.GetRows();

  const auto rows = local_lhs.GetRows();
  const auto cols = rhs_.GetRows();

  std::vector<std::vector<std::tuple<std::complex<double>, uint32_t>>> buf(rows);

  const std::size_t nthreads = ppc::util::GetPPCNumThreads();
  const std::size_t avg = rows / nthreads;
  const std::size_t nextra = rows % nthreads;

  auto mulrow = [&](uint32_t i) {
    for (uint32_t j = 0; j < cols; ++j) {
      auto ii = local_lhs.rowptr[i];
      auto ij = rhs_.rowptr[j];
      std::complex<double> summul = 0.0;
      while (ii < local_lhs.rowptr[i + 1] && ij < rhs_.rowptr[j + 1]) {
        if (local_lhs.colind[ii] < rhs_.colind[ij]) {
          ++ii;
        } else if (local_lhs.colind[ii] > rhs_.colind[ij]) {
          ++ij;
        } else {
          summul += local_lhs.data[ii++] * rhs_.data[ij++];
        }
      }
      if (summul != 0.0) {
        buf[i].emplace_back(summul, j);
      }
    }
  };

  std::vector<std::thread> threads(nthreads);
  uint32_t cur = 0;
  for (std::size_t t = 0; t < nthreads; t++) {
    uint32_t forthread = avg + ((t < nextra) ? 1 : 0);
    threads[t] = std::thread([&](uint32_t begin, uint32_t end) {
      for (uint32_t i = begin; i < end; ++i) {
        mulrow(i);
      }
    }, cur, cur + forthread);
    cur += forthread;
  }
  for (auto& thread : threads) {
    thread.join();
  }

  for (uint32_t i = 0; i < rows; ++i) {
    local_res.rowptr[i + 1] = local_res.rowptr[i];
    for (const auto& [summul, j] : buf[i]) {
      local_res.data.push_back(summul);
      local_res.colind.push_back(j);
      ++local_res.rowptr[i + 1];
    }
  }

  return Gather(local_res, row_offset, idx_offset);
}
\end{lstlisting}
\end{document}
