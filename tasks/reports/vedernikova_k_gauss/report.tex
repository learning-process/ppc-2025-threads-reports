\documentclass[a4paper,12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{caption}

\geometry{left=2.5cm, right=2.5cm, top=2cm, bottom=2cm}

\title{Отчет по практическим работам \\ по курсу «Параллельное программирование» \\ Задача: «Линейная фильтрация изображений (горизонтальное разбиение). Ядро Гаусса 3x3»}
\author{Ведерникова К.А. \\ Группа 3822Б1ПР1}
\date{Нижний Новгород, 2025}

\begin{document}

\maketitle

\section*{Введение}
В данной работе рассматривается задача линейной фильтрации изображений с использованием ядра Гаусса 3×3. Особенностью реализации является горизонтальное разбиение изображения между несколькими процессами с применением технологий параллельного программирования. Такой подход позволяет ускорить обработку больших изображений за счет распределения вычислений между несколькими вычислительными узлами.

Целью работы является разработка и анализ эффективности параллельного алгоритма фильтрации изображений с использованием технологий OpenMP, Intel Threading Building Blocks (TBB), STL threads, MPI (Message Passing Interface) + STL. В ходе выполнения задачи предполагается:
\begin{itemize}
    \item Реализовать последовательную версию алгоритма для проверки корректности работы.
    \item Разработать параллельные версии с горизонтальным разбиением изображения.
    \item Провести тестирование на различных размерах изображений и оценить ускорение работы программы.
\end{itemize}

\section*{Основные этапы алгоритма}
Вычисляется ядро Гаусса размера 3×3 (иначе говоря, распределение), значения элементов зависит от $\sigma$, среднеквадратического отклонения нормального распределения. Для ядра размера 3×3 требуется $\sigma \in \left[\frac{1}{3}; \frac{1}{2}\right]$.

\begin{equation}
G(x,y) = \frac{1}{2\pi\sigma^{2}}e^{-\frac{x^{2} + y^{2}}{2\sigma^{2}}}
\end{equation}

\begin{itemize}
    \item Значения этого распределения используются для построения матрицы свёртки, которая применяется к исходному изображению.
    \item Новое значение каждого пикселя устанавливается равным средневзвешенному значению окрестности этого пикселя.
\end{itemize}

\subsection*{Почему выгодно распараллелить вычисления?}
В размытии по Гауссу каждый пиксель выходного изображения вычисляется на основе локальной окрестности исходного изображения. Поэтому разные области изображения можно обрабатывать параллельно без необходимости синхронизации (кроме граничных пикселей). Изображение можно разбить на блоки строк и обрабатывать их независимо.

\section*{Описание схемы параллельного алгоритма}
\begin{enumerate}
    \item Разделение изображения на строки или прямоугольные блоки.
    \item Распределение блоков между потоками (CPU) или GPU-ядрами.
    \item Параллельное применение гауссова ядра к каждому блоку.
    \item Сбор результатов.
\end{enumerate}

\section*{Описание OpenMP-версии алгоритма}
Для реализации параллельного варианта алгоритма была использована технология OpenMP — стандарт программирования с общей памятью, позволяющий упростить распараллеливание циклов с помощью директив компилятора.

\begin{itemize}
    \item \texttt{\#pragma omp parallel for} — создаёт группу потоков, где каждый поток обрабатывает часть итераций цикла.
    \item \texttt{schedule(dynamic, 256)} — определяет стратегию распределения работы между потоками:
    \begin{itemize}
        \item \texttt{dynamic} — блоки итераций динамически назначаются потокам по мере их освобождения.
        \item \texttt{256} — размер блока (число строк, обрабатываемых за одну "порцию").
    \end{itemize}
\end{itemize}

Преимущества технологии:
\begin{itemize}
    \item Хорошая масштабируемость — чем больше ядер, тем выше ускорение.
    \item Динамическая балансировка нагрузки — минимизирует простои.
    \item Простота реализации — не требуется явное разделение данных.
\end{itemize}

\section*{Описание TBB-версии алгоритма}
В отличие от OpenMP, Threading Building Blocks (TBB) предоставляет более гибкие механизмы для работы с параллельными задачами, включая автоматическую балансировку нагрузки и эффективное управление пулом потоков.

\begin{enumerate}
    \item Создается область выполнения (arena), которая управляет потоками.
    \item Параллелизация организована с использованием конструкции \texttt{tbb::parallel\_for} совместно с диапазоном \texttt{tbb::blocked\_range}, что позволяет автоматически разбивать диапазон строк изображения на поддиапазоны и распределять их между потоками.
    \item Балансировка встроена в библиотеку.
\end{enumerate}

Преимущества TBB-версии:
\begin{itemize}
    \item Автоматическая балансировка нагрузки.
    \item Гибкость — поддерживает сложные паттерны параллелизма.
    \item Лучшая локализация данных — TBB оптимизирует распределение блоков для кэша.
    \item Меньше накладных расходов, чем у OpenMP, при работе с вложенными циклами.
\end{itemize}

\section*{Описание STL-версии алгоритма}
В этой версии алгоритма задействована стандартная библиотека потоков C++ (STL threads), предоставляющая базовые механизмы многопоточности без высокоуровневых абстракций для автоматического распределения задач.

\begin{enumerate}
    \item Общее количество строк изображения делится между потоками максимально равномерно, количество потоков определяется с помощью утилиты \texttt{ppc::util::GetPPCNumThreads()}.
    \item Каждому потоку передается начальный индекс строки и количество строк для обработки.
    \item Основной поток ожидает завершения всех независимых рабочих потоков через \texttt{join()}.
\end{enumerate}

Преимущества:
\begin{itemize}
    \item Равномерное распределение нагрузки минимизирует простои потоков.
    \item Отсутствие динамического планирования уменьшает накладные расходы.
    \item Локальность данных сохраняется в пределах блока строк.
    \item Минимальная зависимость от библиотек.
\end{itemize}

\section*{Описание MPI + STL-версии алгоритма}
Данная версия алгоритма сочетает в себе два уровня параллелизма: распределённый с использованием библиотеки Boost.MPI и потоковый с использованием STL.

\begin{enumerate}
    \item Функция \texttt{fillAux()} распределяет строки изображения между процессами с учетом перекрывающихся строк.
    \item Изображение разбивается на горизонтальные полосы, которые корневой процесс рассылает другим процессам.
    \item Каждый процесс создает несколько потоков для обработки своей части.
    \item После обработки данные собираются обратно на корневом процессе.
\end{enumerate}

Преимущества:
\begin{itemize}
    \item Масштабируемость.
    \item Балансировка нагрузки.
    \item Гибкость.
    \item Оптимизация памяти.
\end{itemize}

\section*{Результаты экспериментов}
Для оценки производительности реализованных версий алгоритма были проведены измерения времени выполнения на изображении размером 1000×300 пикселей (1-канальное изображение). Время в таблице — среднее за 5 запусков.

\begin{table}[h]
\centering
\caption{Сравнение производительности реализаций алгоритма}
\begin{tabular}{lcccc}
\toprule
Версия & Конфигурация & PipelineRun, с & TaskRun, с & Ускорение \\
\midrule
Последовательная & — & 1,325 & 1,397 & 1,00 \\
OpenMP & 2 потока & 0,720 & 0,796 & 1,76 \\
 & 4 потока & 0,779 & 0,856 & 1,63 \\
TBB & 2 потока & 0,732 & 0,817 & 1,71 \\
 & 4 потока & 0,549 & 0,585 & 2,39 \\
STL & 2 потока & 0,885 & 1,079 & 1,29 \\
 & 4 потока & 0,706 & 0,727 & 1,92 \\
MPI & 2 процесса + 2 потока & 0,529 & 0,536 & 2,61 \\
 & 2 процесса + 4 потока & 0,489 & 0,529 & 2,64 \\
\bottomrule
\end{tabular}
\end{table}

Наибольшую производительность на 1 процессе показала реализация TBB на 4 потоках, на 2 процессах — MPI+STL (2+4).

\section*{Приложение}
\subsection*{SEQ-версия}
\begin{verbatim}
void vedernikova_k_gauss_seq::Gauss::ComputeKernel(double sigma) {
    // For 3x3 kernel sigma from [1/3; 1/2] is required
    for (int i = 0; i < 9; i++) {
        int ik = (i % 3) - 1;
        int jk = (i / 3) - 1;
        kernel_[i] = std::exp(-1.0 * (ik * ik + jk * jk) / (2 * sigma * sigma)) / 
                     (2 * std::numbers::pi * sigma * sigma);
    }
    double amount = std::accumulate(kernel_.begin(), kernel_.end(), 0.0);
    for (auto&& it : kernel_) {
        it /= amount;
    }
}
\end{verbatim}

\subsection*{OMP-версия}
\begin{verbatim}
bool vedernikova_k_gauss_omp::Gauss::RunImpl() {
    int h = (int)height_;
    int w = (int)width_;
    #pragma omp parallel for schedule(dynamic, 256)
    for (int j = 0; j < h; j++) {
        for (int i = 0; i < w; i++) {
            ComputePixel(i, j);
        }
    }
    return true;
}
\end{verbatim}

\subsection*{TBB-версия}
\begin{verbatim}
bool vedernikova_k_gauss_tbb::Gauss::RunImpl() {
    if (height_ == 1) {
        SetPixel(GetPixel(0, 0, channels_ - 1), 0, 0, channels_ - 1);
        return true;
    }
    tbb::parallel_for(tbb::blocked_range2d<int>(0, int(width_), 0, int(height_)),
        [&](const tbb::blocked_range2d<int>& r) {
            for (int j = r.cols().begin(); j < r.cols().end(); ++j) {
                for (int i = r.rows().begin(); i < r.rows().end(); ++i) {
                    ComputePixel(i, j);
                }
            }
        });
    return true;
}
\end{verbatim}

\subsection*{STL-версия}
\begin{verbatim}
bool vedernikova_k_gauss_stl::Gauss::RunImpl() {
    if (height_ == 1) {
        SetPixel(GetPixel(0, 0, channels_ - 1), 0, 0, channels_ - 1);
        return true;
    }
    const int h = (int)height_;
    const int w = (int)width_;
    const int tnum = std::min(h, ppc::util::GetPPCNumThreads());
    const int base = h / tnum;
    const int extra = h % tnum;
    std::vector<std::thread> threads(tnum);
    int cur = 0;
    for (int k = 0; k < tnum; k++) {
        const int cnt = base + ((k < extra) ? 1 : 0);
        threads[k] = std::thread(
            [&](int rbegin, int rcnt) {
                for (int j = rbegin; j < rbegin + rcnt; j++) {
                    for (int i = 0; i < w; i++) {
                        ComputePixel(i, j);
                    }
                }
            },
            cur, cnt);
        cur += cnt;
    }
    for (auto& t : threads) {
        t.join();
    }
    return true;
}
\end{verbatim}

\subsection*{MPI+STL-версия}
\begin{verbatim}
bool vedernikova_k_gauss_all::Gauss::RunImpl() {
    const int world_size = world_.size();
    const int rank = world_.rank();
    int overall_h{};
    if (rank == 0) {
        overall_h = static_cast<int>(g_height_);
    }
    boost::mpi::broadcast(world_, overall_h, 0);
    boost::mpi::broadcast(world_, width_, 0);
    boost::mpi::broadcast(world_, channels_, 0);
    if (overall_h == 1) {
        if (rank == 0) {
            local_input_ = input_;
            local_output_ = output_;
            SetPixel(GetPixel(0, 0, channels_ - 1), 0, 0, channels_ - 1);
            output_ = local_output_;
        }
        return true;
    }
    const uint32_t cells_per_row = width_ * channels_;
    const int participants = std::min(overall_h, world_size);
    if (world_.rank() >= participants) {
        world_.split(0);
        return true;
    }
    auto party = world_.split(1);
    std::vector<int> sendcounts;
    std::vector<int> senddispls;
    std::vector<int> recvcounts;
    std::vector<int> recvdispls;
    FillAux(participants, cells_per_row, overall_h, sendcounts, senddispls, recvcounts, recvdispls);
    l_height_ = sendcounts[rank] / cells_per_row;
    local_input_.resize(sendcounts[rank]);
    local_output_.resize(local_input_.size());
    boost::mpi::scatterv(party, input_.data(), sendcounts, senddispls, local_input_.data(), int(local_input_.size()), 0);
    const uint32_t tnum = std::min<uint32_t>(l_height_, ppc::util::GetPPCNumThreads());
    const uint32_t base = l_height_ / tnum;
    const uint32_t extra = l_height_ % tnum;
    std::vector<std::thread> threads(tnum);
    uint32_t cur = 0;
    for (uint32_t k = 0; k < tnum; ++k) {
        const uint32_t cnt = base + (k < extra ? 1 : 0);
        threads[k] = std::thread(
            [&](int rbegin, int rcnt) {
                for (int y = rbegin; y < rbegin + rcnt; ++y) {
                    for (int x = 0; x < int(width_); ++x) {
                        ComputePixel(x, y);
                    }
                }
            },
            cur, cnt);
        cur += cnt;
    }
    for (auto& t : threads) {
        t.join();
    }
    boost::mpi::gatherv(party, local_output_.data() + (rank == 0 ? 0 : cells_per_row), recvcounts[rank], output_.data(),
        recvcounts, recvdispls, 0);
    return true;
}
\end{verbatim}

\end{document}