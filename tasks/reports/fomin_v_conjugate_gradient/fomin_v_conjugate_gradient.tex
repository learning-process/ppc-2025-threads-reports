\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{tocloft}
\usepackage{xcolor} 
\usepackage{listings}
\usepackage{float}

\geometry{a4paper, left=10mm, right=10mm, top=20mm, bottom=20mm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    pdftitle={Отчет по проекту},
    pdfauthor={Фомин Владимир},
    pdfsubject={Решение систем линейных уравнений методом сопряженных градиентов}
}

\lstset{
  language=C++,
  basicstyle=\ttfamily\small,
  keywordstyle=\bfseries\color{blue},
  commentstyle=\itshape\color{green!50!black},
  stringstyle=\color{red!60!black},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=8pt,
  backgroundcolor=\color{gray!10},
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  tabsize=2,
  captionpos=b
}

\renewcommand{\cftsecaftersnum}{.}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}

\begin{document}
\begin{titlepage}
    \centering
    \large
    Министерство науки и высшего образования Российской Федерации\\[0.5cm]
    Федеральное государственное автономное образовательное учреждение высшего образования\\[0.5cm]
    «Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского» \\
    (ННГУ)\\[1cm]
    Институт информационных технологий, математики и механики\\[0.5cm]
    Направление подготовки: «Программная инженерия»\\[1cm]

    \vspace{1cm}
    {\LARGE \textbf{ОТЧЕТ}}\\[0.5cm]
    {\Large по задаче}\\[0.5cm]
    {\LARGE {«Решение систем линейных уравнений методом сопряженных градиентов»}}\\[0.5cm]
    \vspace{2cm}
    \hfill\parbox{0.4\textwidth}{
        \textbf{Выполнила:} \\
        студент группы 3822Б1ПР4 \\
        {Фомин Владимир}
    }\\[0.5cm]
    \vfill
    Нижний Новгород\\
    2025
\end{titlepage}


\thispagestyle{empty}
\clearpage
\pagenumbering{arabic} 
\setcounter{page}{2} 
\tableofcontents
\clearpage
\setcounter{page}{3} 
\section{Введение}

\hspace*{1.35em} Решение систем линейных уравнений является одной из фундаментальных задач в математическом моделировании и численных методах, широко применяемых в инженерных расчетах, физике, компьютерных науках и экономике. 
Современные прикладные задачи все чаще требуют эффективных алгоритмов для обработки больших и разреженных систем, где традиционные методы оказываются недостаточно производительными или ресурсоемкими. Метод сопряженных градиентов представляет собой эффективный итерационный подход, позволяющий решать такие системы с высокой скоростью сходимости,
что делает его актуальным объектом для исследования и оптимизации, особенно в контексте современных вычислительных архитектур и параллельных вычислений.
Традиционные прямые методы решения систем линейных уравнений часто сталкиваются с ограничениями при работе с большими и разреженными матрицами, что ведет к высокой вычислительной сложности и значительным затратам памяти. 
Итерационные методы, включая метод сопряженных градиентов, требуют тщательной настройки и анализа для обеспечения стабильности и эффективности. Дополнительные сложности возникают при параллельной реализации алгоритмов, где необходимо учитывать взаимодействие между вычислительными потоками, синхронизацию и балансировку нагрузки. 
Эти проблемы стимулируют исследование и разработку оптимальных стратегий реализации метода сопряженных градиентов на различных платформах, включая STL, TBB, OpenMP и MPI.

\section{Постановка задачи}

\hspace*{1.35em}Основная цель работы – разработка и сравнительный анализ различных параллельных реализаций алгоритма решения систем линейных уравнений методом сопряженных градиентов. 
Для этого необходимо реализовать последовательную версию алгоритма, а затем разработать и протестировать параллельные версии с использованием OpenMP, Intel TBB, STL threads и гибридного подхода MPI + TBB. Ключевой задачей является выявление наиболее эффективной параллельной реализации, обеспечивающей наилучшую производительность и масштабируемость при вычислении данной задачи.

\section{Описание алгоритма (последовательной версии)}

\hspace*{1.35em}Метод сопряженных градиентов (МСГ) — это итеративный алгоритм, используемый для решения систем линейных уравнений, особенно когда матрица системы является большой и разреженной. Он эффективен для симметричных положительно определённых матриц.
Принцип работы последовательной версии алгоритма можно описать следующим образом:
\begin{enumerate}
    \item \textbf{Инициализация:} Начинается алгоритм с инициализации, где задаются матрица $A$, вектор $b$, начальное приближение $x_0$, максимальное количество итераций и порог сходимости.
    \item \textbf{Вычисление начальных значений:} Затем вычисляется начальное значение остатка, которое представляет собой разность между вектором $b$ и результатом умножения матрицы $A$ на начальное приближение $x_0$:

\[
r_0 = b - Ax_0.
\]

Остаток устанавливается как начальное направление поиска.
    \item \textbf{Итерационный процесс:} На каждом шаге итерационного процесса проверяется, достигнута ли сходимость, то есть если норма остатка меньше заданного порога, итерации завершаются. В противном случае вычисляется шаг $\alpha_k$, который основывается на скалярном произведении остатка и направления:

\[
\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}.
\]

После этого обновляется текущее приближение решения:

\[
x_{k+1} = x_k + \alpha_k p_k,
\]

и остаток:

\[
r_{k+1} = r_k - \alpha_k A p_k.
\]

Далее вычисляется новое направление поиска, которое зависит от предыдущего остатка и направления, с использованием коэффициента $\beta_k$:

\[
\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k},
\]

и обновляется направление:

\[
p_{k+1} = r_{k+1} + \beta_k p_k.
\]
\item \textbf{Завершение:} Алгоритм продолжается до тех пор, пока не будет достигнуто максимальное количество итераций или не будет достигнута сходимость.
    
\end{enumerate}
Этот последовательный алгоритм служит основой для дальнейшей разработки параллельных версий, в которых вычисление промежуточных значений распределяются между несколькими потоками или процессами.

\section{Описание параллельных реализаций алгоритма}

\subsection{Описание параллельной реализации (OpenMP)}

\hspace*{1.35em}Использование OpenMP позволяет распараллелить вычисления по векторам и строкам матрицы, что значительно ускоряет работу на многопроцессорных системах.
Данный класс FominVConjugateGradientOmp реализует метод сопряженных градиентов для решения системы линейных уравнений \( Ax = b \) с использованием параллелизма OpenMP для ускорения вычислений.

\begin{verbatim}
double fomin_v_conjugate_gradient::FominVConjugateGradientOmp::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
double result = 0.0;
#pragma omp parallel for reduction(+ : result)
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result += a[i] * b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
#pragma omp parallel for
for (int i = 0; i < n; ++i) {
double sum = 0.0;
for (int j = 0; j < n; ++j) {
sum += a[(i * n) + j] * x[j];
}
result[i] = sum;
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
#pragma omp parallel for 
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result[i] = a[i] + b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
#pragma omp parallel for
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result[i] = a[i] - b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
#pragma omp parallel for 
for (int i = 0; i < static_cast<int>(v.size()); ++i) {
result[i] = v[i] * scalar;
}
return result;
}
\end{verbatim}



\subsection{Описание параллельной реализации (TBB)}

\hspace*{1.35em}Класс FominVConjugateGradientTbb реализует метод сопряженных градиентов для решения системы линейных уравнений \( Ax = b \) с использованием параллелизма TBB.
\subsection{Скалярное произведение (\texttt{DotProduct})}
Использует \texttt{tbb::parallel\_reduce} с \texttt{tbb::blocked\_range<size\_t>}, что позволяет распараллелить вычисление скалярного произведения двух векторов. Каждый поток обрабатывает свою часть данных, а затем результаты суммируются.

\subsection{Умножение матрицы на вектор (\texttt{MatrixVectorMultiply})}
Параллелится с использованием \texttt{tbb::parallel\_for}, где каждая строка матрицы обрабатывается в отдельном потоке. Это позволяет эффективно использовать ресурсы и ускорить вычисления.

\subsection{Поэлементное сложение и вычитание векторов (\texttt{VectorAdd}, \texttt{VectorSub})}
Операции выполняются параллельно по элементам векторов с помощью \texttt{tbb::parallel\_for}, что улучшает производительность.

\subsection{Умножение вектора на скаляр (\texttt{VectorScalarMultiply})}
Также реализовано с использованием \texttt{tbb::parallel\_for}, позволяя каждому потоку обрабатывать свою часть вектора.

\section{Предобработка и валидация данных}

\subsection{Предобработка (\texttt{PreProcessingImpl})}
В методе \texttt{PreProcessingImpl} происходит разбор входного массива, выделяя матрицу \( A \) и вектор \( b \). Размерность \( n \) вычисляется из длины входных данных.

\subsection{Валидация (\texttt{ValidationImpl})}
Метод \texttt{ValidationImpl} проверяет корректность размеров входных и выходных данных.

\section{Основной цикл метода сопряженных градиентов (\texttt{RunImpl})}
Инициализируются векторы:
\begin{itemize}
    \item \( x \) (начальное приближение, нулевой вектор)
    \item \( r = b - Ax \) (остаток)
    \item \( p = r \) (направление поиска)
\end{itemize}

На каждой итерации вычисляется:
\begin{itemize}
    \item Вектор \( Ap = A p \) с использованием \texttt{MatrixVectorMultiply}.
    \item Шаг \( \alpha = \frac{r^T r}{p^T A p} \).
    \item Обновление решения: \( x = x + \alpha p \).
    \item Обновление остатка: \( r_{\text{new}} = r - \alpha A p \).
    \item Проверка сходимости по норме остатка.
    \item Вычисление коэффициента \( \beta = \frac{r_{\text{new}}^T r_{\text{new}}}{r^T r} \).
    \item Обновление направления: \( p = r_{\text{new}} + \beta p \).
    \item Обновление переменных для следующей итерации.
\end{itemize}

Итерации продолжаются, пока не достигнут заданный порог точности или максимальное число итераций.

\section{Постобработка результатов (\texttt{PostProcessingImpl})}
Результирующий вектор решения \( x \) копируется в выходной буфер. Эта операция выполняется последовательно, так как она не требует параллелизма.
\begin{verbatim}
double fomin_v_conjugate_gradient::FominVConjugateGradientTbb::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
return tbb::parallel_reduce(
tbb::blocked_range<size_t>(0, a.size()), 0.0,
[&](const tbb::blocked_range<size_t>& r, double init) {
for (size_t i = r.begin(); i < r.end(); ++i) {
init += a[i] * b[i];
}
return init;
},
std::plus<>());
}
std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
tbb::parallel_for(tbb::blocked_range<int>(0, n), [&](const tbb::blocked_range<int>& r) {
for (int i = r.begin(); i < r.end(); ++i) {
double temp = 0.0;
for (int j = 0; j < n; ++j) {
temp += a[(i * n) + j] * x[j];
}
result[i] = temp;
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, a.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = a[i] + b[i];
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, a.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = a[i] - b[i];
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, v.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = v[i] * scalar;
}
});
return result;
}
\end{verbatim}



\subsection{Описание параллельной реализации (STL threads)}

\hspace*{1.35em}Класс \texttt{FominVConjugateGradientStl} реализует метод сопряженных градиентов для решения системы линейных уравнений \( Ax = b \) с использованием стандартной библиотеки C++ (STL) и многопоточности.

\begin{verbatim}
double fomin_v_conjugate_gradient::FominVConjugateGradientStl::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);
std::vector<double> partial_results(num_threads, 0.0);

const size_t chunk_size = a.size() / num_threads;

for (int i = 0; i < num_threads; ++i) {
const size_t start = i * chunk_size;
const size_t end = (i == num_threads - 1) ? a.size() : start + chunk_size;
threads[i] = std::thread(&, start, end, i {
for (size_t j = start; j < end; ++j) {
partial_results[i] += a[j] * b[j];
}
});
}

for (auto& thread : threads) {
thread.join();
}

double result = 0.0;
for (const auto& val : partial_results) {
result += val;
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const int chunk_size = n / num_threads;

for (int t = 0; t < num_threads; ++t) {
const int start = t * chunk_size;
const int end = (t == num_threads - 1) ? n : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (int i = start; i < end; ++i) {
for (int j = 0; j < n; ++j) {
result[i] += a[(i * n) + j] * x[j];
}
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = a.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? a.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = a[i] + b[i];
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = a.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? a.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = a[i] - b[i];
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = v.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? v.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = v[i] * scalar;
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}
\end{verbatim}

Аналогично, цикл умножения матриц также распараллеливается с использованием \texttt{std::thread}. 
Для этого вычисляется количество потоков, которое будет использовано (определяется функцией `ppc::util::GetPPCNumThreads()` или берется максимальное число доступных потоков),
а затем создается вектор потоков, каждый из которых обрабатывает свою часть данных. После завершения работы каждого потока вызывается метод \texttt{join()}, чтобы дождаться окончания их выполнения и обеспечить корректность дальнейших вычислений.  
Суммирование результатов также распараллеливается с использованием потоков, где каждый поток вычисляет локальную сумму, а затем эти локальные суммы суммируются в общую сумму.


\subsection{Описание параллельной реализации (MPI + TBB)}

\hspace*{1.35em}Данная реализация метода сопряжённых градиентов использует гибридный подход: распределённые вычисления между процессами организованы с помощью MPI (Message Passing Interface), а внутри каждого процесса вычисления распараллеливаются с использованием Intel TBB (Threading Building Blocks).
Такой подход позволяет эффективно использовать ресурсы как многомашинных кластеров, так и многоядерных процессоров на каждом узле.
В начале работы данные (матрица коэффициентов \( A \) и вектор правой части \( b \)) считываются и хранятся на процессе с рангом 0, после чего с помощью MPI выполняется их распределение между процессами. Матрица \( A \) разбивается по строкам, и каждый процесс получает свою часть — подматрицу \texttt{local\_a\_} и соответствующий подвектор \texttt{local\_b\_}.
Распределение учитывает возможное неравномерное деление строк на процессы для балансировки нагрузки.
Основные операции над векторами и матрицами внутри каждого процесса реализованы с использованием TBB для распараллеливания циклов. Например, вычисление локального скалярного произведения двух векторов происходит через \texttt{tbb::parallel\_reduce}:

\begin{verbatim}
double local_sum = tbb::parallel_reduce(
    tbb::blocked_range<size_t>(0, a.size()), 0.0,
    [&](const tbb::blocked_range<size_t>& r, double sum) {
      for (size_t i = r.begin(); i < r.end(); ++i) {
        sum += a[i] * b[i];
      }
      return sum;
    },
    std::plus<>());
\end{verbatim}

Затем локальные суммы сводятся в глобальную с помощью MPI-функции \texttt{all\_reduce}:

\begin{verbatim}
double global_sum = 0.0;
all_reduce(world, local_sum, global_sum, std::plus<>());
\end{verbatim}

Аналогично, умножение матрицы на вектор реализовано как локальный параллельный цикл по строкам с помощью \texttt{tbb::parallel\_for}, после чего результаты собираются с помощью MPI-функции \texttt{gatherv} и рассылаются всем процессам через \texttt{broadcast}.

Векторные операции сложения, вычитания и умножения на скаляр реализованы последовательно, так как они не являются узким местом, а их параллелизация внутри процесса может быть менее эффективной из-за накладных расходов.

Основной цикл метода сопряжённых градиентов выполняется итеративно, где на каждом шаге вычисляются скалярные произведения и произведения матрицы на вектор с использованием описанных выше параллельных функций. Все глобальные скалярные величины синхронизируются через MPI, что обеспечивает корректность алгоритма.

Таким образом, комбинирование MPI для распределения данных и объединения результатов с TBB для параллельных вычислений внутри процесса позволяет добиться высокой производительности и масштабируемости решения задачи.
\section{Результаты экспериментов}

\hspace*{1.35em}Для оценки производительности реализации метода сопряжённых градиентов были проведены замеры времени выполнения в конфигурациях с различным числом потоков и процессов. В таблице ниже представлены усреднённые значения времени выполнения (в секундах) для двух режимов тестирования — \texttt{PipelineRun} и \texttt{TaskRun}.

\subsection{Таблица производительности}

\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Версия} & \textbf{Конфигурация} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} \\
\hline
\textbf{Последовательная} & 1 поток & \centering 1.200 & 1.205 \\
\hline
\multirow{3}{*}{OpenMP} 
  & 2 потока & 1.150 & 1.160 \\
  & 4 потока & 1.020 & 1.050 \\
  & 8 потоков & 0.900 & 0.950 \\
\hline
\multirow{3}{*}{TBB} 
  & 2 потока & 1.180 & 1.175 \\
  & 4 потока & 1.000 & 1.020 \\
  & 8 потоков & 0.850 & 0.870 \\
\hline
\multirow{3}{*}{STL (std::thread)} 
  & 2 потока & 1.250 & 1.240 \\
  & 4 потока & 1.100 & 1.090 \\
  & 8 потоков & 0.950 & 0.940 \\
\hline
\multirow{5}{*}{MPI + TBB} 
  & 2 процесса и 2 потока & 1.800 & 1.850 \\
  & 2 процесса и 4 потока & 1.500 & 1.600 \\
  & 4 процесса и 2 потока & 1.200 & 1.300 \\
  & 4 процесса и 4 потока & 0.950 & 0.900 \\
  & 4 процесса и 8 потоков & 0.700 & 0.750 \\
\hline
\end{tabular}
\caption{Сравнение производительности различных реализаций параллельного вычисления}
\label{tab:parallel_perf}
\end{table}
\subsection{Анализ производительности}
\hspace*{1.35em}Представленные результаты показывают, что параллельные реализации метода сопряжённых градиентов демонстрируют различную эффективность в зависимости от используемой технологии и конфигурации.
В целом, как OpenMP, так и TBB показывают улучшение производительности по сравнению с последовательной версией, однако прирост не столь значителен. Использование STL threads также приводит к небольшому улучшению при увеличении числа потоков, хотя результаты не всегда стабильно превосходят последовательную версию.
Наиболее заметный прирост производительности достигается при использовании гибридной модели MPI + TBB, особенно при конфигурации с 4 процессами и 4 потоками. 
В этой конфигурации наблюдается значительное снижение времени выполнения как для PipelineRun, так и для TaskRun, что свидетельствует об эффективности комбинирования распределённых вычислений (MPI) и многопоточного распараллеливания внутри каждого узла (TBB). 

Гибридный подход позволяет оптимально использовать ресурсы вычислительной системы, обеспечивая масштабируемость и высокую производительность при решении задач, связанных с методами линейной алгебры. Важно отметить, что увеличение числа процессов и потоков не всегда приводит к линейному увеличению производительности,
что может быть связано с накладными расходами на синхронизацию и передачу данных между процессами.

\section{Вывод}
\hspace*{1.35em}В ходе работы были исследованы различные подходы к параллельной реализации метода сопряжённых градиентов, включая OpenMP, TBB, STL threads и гибридную модель MPI + TBB. Каждая из этих технологий была применена для распараллеливания наиболее вычислительно-

\section{Список литературы}

\begin{enumerate}
    \item OpenMP Architecture Review Board. \textit{OpenMP Specifications}. \url{https://www.openmp.org/specifications/}
    \item Сысоев А.В., Мееров И.Б., Сиднев А.А. \textit{Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks}. — Нижний Новгород, 2007.
    \item OpenMP Architecture Review Board. \textit{OpenMP Reference Guides}. \url{https://www.openmp.org/resources/refguides/}
    \item UXL Foundation. \textit{oneAPI Threading Building Blocks (oneTBB) Documentation}. \url{http://uxlfoundation.github.io/oneTBB/}
    \item cppreference.com. \textit{Concurrency support library (since C++11)}. \url{https://en.cppreference.com/w/cpp/thread.html}
\end{enumerate}
\newpage
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

В этом разделе приведены ключевые фрагменты реализации метода сопряжённых градиентов для различных технологий распараллеливания.

\subsection*{OpenMP}
\begin{lstlisting}[language=C++]
#include <omp.h>

#include <cmath>
#include <vector>

double fomin_v_conjugate_gradient::FominVConjugateGradientOmp::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
double result = 0.0;
#pragma omp parallel for reduction(+ : result)
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result += a[i] * b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
#pragma omp parallel for 
for (int i = 0; i < n; ++i) {
double sum = 0.0;
for (int j = 0; j < n; ++j) {
sum += a[(i * n) + j] * x[j];
}
result[i] = sum;
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
#pragma omp parallel for 
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result[i] = a[i] + b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
#pragma omp parallel for 
for (int i = 0; i < static_cast<int>(a.size()); ++i) {
result[i] = a[i] - b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientOmp::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
#pragma omp parallel for 
for (int i = 0; i < static_cast<int>(v.size()); ++i) {
result[i] = v[i] * scalar;
}
return result;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientOmp::PreProcessingImpl() {
unsigned int input_size = task_data->inputs_count[0];
auto* in_ptr = reinterpret_cast<double*>(task_data->inputs[0]);
std::vector<double> input(in_ptr, in_ptr + input_size);

n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
a_ = std::vector<double>(input.begin(), input.begin() + (n * n));
b_ = std::vector<double>(input.begin() + (n * n), input.end());
output_.resize(n, 0.0);

return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientOmp::ValidationImpl() {
unsigned int input_size = task_data->inputs_count[0];
const int calculated_n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
return (static_cast<unsigned int>(calculated_n * (calculated_n + 1)) == input_size) &&
(task_data->outputs_count[0] == static_cast<unsigned int>(calculated_n));
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientOmp::RunImpl() {
const double epsilon = 1e-6;
const int max_iter = 1000;
std::vector<double> x(n, 0.0);
std::vector<double> r = b_;
std::vector<double> p = r;
double rs_old = DotProduct(r, r);

for (int i = 0; i < max_iter; ++i) {
std::vector<double> ap = MatrixVectorMultiply(a_, p);
double p_ap = DotProduct(p, ap);

if (std::abs(p_ap) < 1e-12) {
  break;
}

double alpha = rs_old / p_ap;
x = VectorAdd(x, VectorScalarMultiply(p, alpha));
std::vector<double> r_new = VectorSub(r, VectorScalarMultiply(ap, alpha));

double rs_new = DotProduct(r_new, r_new);
if (std::sqrt(rs_new) < epsilon) {
  break;
}

double beta = rs_new / rs_old;
p = VectorAdd(r_new, VectorScalarMultiply(p, beta));
r = r_new;
rs_old = rs_new;
}

output_ = x;
return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientOmp::PostProcessingImpl() {
auto* out_ptr = reinterpret_cast<double*>(task_data->outputs[0]);
#pragma omp parallel for
for (int i = 0; i < static_cast<int>(output_.size()); ++i) {
out_ptr[i] = output_[i];
}
return true;
}
\end{lstlisting}
\newpage
\begin{lstlisting}[language=C++]

\end{lstlisting}

\subsection*{TBB}
\begin{lstlisting}[language=C++]
#include "tbb/fomin_v_conjugate_gradient/include/ops_tbb.hpp"

#include <oneapi/tbb/blocked_range.h>
#include <oneapi/tbb/parallel_for.h>
#include <oneapi/tbb/parallel_reduce.h>

#include <cmath>
#include <cstddef>
#include <functional>
#include <vector>

double fomin_v_conjugate_gradient::FominVConjugateGradientTbb::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
return tbb::parallel_reduce(
tbb::blocked_range<size_t>(0, a.size()), 0.0,
[&](const tbb::blocked_range<size_t>& r, double init) {
for (size_t i = r.begin(); i < r.end(); ++i) {
init += a[i] * b[i];
}
return init;
},
std::plus<>());
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
tbb::parallel_for(tbb::blocked_range<int>(0, n), [&](const tbb::blocked_range<int>& r) {
for (int i = r.begin(); i < r.end(); ++i) {
double temp = 0.0;
for (int j = 0; j < n; ++j) {
temp += a[(i * n) + j] * x[j];
}
result[i] = temp;
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, a.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = a[i] + b[i];
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, a.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = a[i] - b[i];
}
});
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientTbb::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
tbb::parallel_for(tbb::blocked_range<size_t>(0, v.size()), [&](const tbb::blocked_range<size_t>& r) {
for (size_t i = r.begin(); i < r.end(); ++i) {
result[i] = v[i] * scalar;
}
});
return result;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientTbb::PreProcessingImpl() {
unsigned int input_size = task_data->inputs_count[0];
auto* in_ptr = reinterpret_cast<double*>(task_data->inputs[0]);
std::vector<double> input(in_ptr, in_ptr + input_size);

n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
a_ = std::vector<double>(input.begin(), input.begin() + (n * n));
b_ = std::vector<double>(input.begin() + (n * n), input.end());
output_.resize(n, 0.0);

return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientTbb::ValidationImpl() {
unsigned int input_size = task_data->inputs_count[0];
const int calculated_n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
return (static_cast<unsigned int>(calculated_n * (calculated_n + 1)) == input_size) &&
(task_data->outputs_count[0] == static_cast<unsigned int>(calculated_n));
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientTbb::RunImpl() {
const double epsilon = 1e-6;
const int max_iter = 1000;
std::vector<double> x(n, 0.0);
std::vector<double> r = b_;
std::vector<double> p = r;
double rs_old = DotProduct(r, r);

for (int i = 0; i < max_iter; ++i) {
std::vector<double> ap = MatrixVectorMultiply(a_, p);
double p_ap = DotProduct(p, ap);

if (std::abs(p_ap) < 1e-12) {
break;
}

double alpha = rs_old / p_ap;
x = VectorAdd(x, VectorScalarMultiply(p, alpha));
std::vector<double> r_new = VectorSub(r, VectorScalarMultiply(ap, alpha));

double rs_new = DotProduct(r_new, r_new);
if (std::sqrt(rs_new) < epsilon) {
break;
}

double beta = rs_new / rs_old;
p = VectorAdd(r_new, VectorScalarMultiply(p, beta));
r = r_new;
rs_old = rs_new;
}

output_ = x;
return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientTbb::PostProcessingImpl() {
auto* out_ptr = reinterpret_cast<double*>(task_data->outputs[0]);
for (size_t i = 0; i < output_.size(); ++i) {
out_ptr[i] = output_[i];
}
return true;
}
\end{lstlisting}
\begin{lstlisting}[language=C++]

\end{lstlisting}
\newpage
\subsection*{STL (std::thread)}
\begin{lstlisting}[language=C++]
#include "stl/fomin_v_conjugate_gradient/include/ops_stl.hpp"

#include <cmath>
#include <cstddef>
#include <thread>
#include <vector>

#include "core/util/include/util.hpp"

double fomin_v_conjugate_gradient::FominVConjugateGradientStl::DotProduct(const std::vector<double>& a,
const std::vector<double>& b) {
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);
std::vector<double> partial_results(num_threads, 0.0);

const size_t chunk_size = a.size() / num_threads;

for (int i = 0; i < num_threads; ++i) {
const size_t start = i * chunk_size;
const size_t end = (i == num_threads - 1) ? a.size() : start + chunk_size;
threads[i] = std::thread(&, start, end, i {
for (size_t j = start; j < end; ++j) {
partial_results[i] += a[j] * b[j];
}
});
}

for (auto& thread : threads) {
thread.join();
}

double result = 0.0;
for (const auto& val : partial_results) {
result += val;
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::MatrixVectorMultiply(
const std::vector<double>& a, const std::vector<double>& x) const {
std::vector<double> result(n, 0.0);
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const int chunk_size = n / num_threads;

for (int t = 0; t < num_threads; ++t) {
const int start = t * chunk_size;
const int end = (t == num_threads - 1) ? n : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (int i = start; i < end; ++i) {
for (int j = 0; j < n; ++j) {
result[i] += a[(i * n) + j] * x[j];
}
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = a.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? a.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = a[i] + b[i];
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = a.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? a.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = a[i] - b[i];
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientStl::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
const int num_threads = ppc::util::GetPPCNumThreads();
std::vectorstd::thread threads(num_threads);

const size_t chunk_size = v.size() / num_threads;

for (int t = 0; t < num_threads; ++t) {
const size_t start = t * chunk_size;
const size_t end = (t == num_threads - 1) ? v.size() : start + chunk_size;
threads[t] = std::thread(&, start, end {
for (size_t i = start; i < end; ++i) {
result[i] = v[i] * scalar;
}
});
}

for (auto& thread : threads) {
thread.join();
}

return result;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientStl::PreProcessingImpl() {
unsigned int input_size = task_data->inputs_count[0];
auto* in_ptr = reinterpret_cast<double*>(task_data->inputs[0]);
std::vector<double> input(in_ptr, in_ptr + input_size);

n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
a_ = std::vector<double>(input.begin(), input.begin() + (n * n));
b_ = std::vector<double>(input.begin() + (n * n), input.end());
output_.resize(n, 0.0);

return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientStl::ValidationImpl() {
unsigned int input_size = task_data->inputs_count[0];
const int calculated_n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
return (static_cast<unsigned int>(calculated_n * (calculated_n + 1)) == input_size) &&
(task_data->outputs_count[0] == static_cast<unsigned int>(calculated_n));
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientStl::RunImpl() {
const double epsilon = (n < 100) ? 1e-4 : 1e-6;
const int max_iter = (n < 100) ? 200 : 1000;
std::vector<double> x(n, 0.0);
std::vector<double> r = b_;
std::vector<double> p = r;
double rs_old = DotProduct(r, r);

for (int i = 0; i < max_iter; ++i) {
std::vector<double> ap = MatrixVectorMultiply(a_, p);
double p_ap = DotProduct(p, ap);

if (std::abs(p_ap) < 1e-12) {
break;
}

double alpha = rs_old / p_ap;
x = VectorAdd(x, VectorScalarMultiply(p, alpha));
std::vector<double> r_new = VectorSub(r, VectorScalarMultiply(ap, alpha));

double rs_new = DotProduct(r_new, r_new);
if (std::sqrt(rs_new) < epsilon) {
break;
}

double beta = rs_new / rs_old;
p = VectorAdd(r_new, VectorScalarMultiply(p, beta));
r = r_new;
rs_old = rs_new;
}

output_ = x;
return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientStl::PostProcessingImpl() {
auto* out_ptr = reinterpret_cast<double*>(task_data->outputs[0]);
for (size_t i = 0; i < output_.size(); ++i) {
out_ptr[i] = output_[i];
}
return true;
}
\end{lstlisting}
  
\newpage
\subsection*{MPI + TBB}
\begin{lstlisting}[language=C++]
double fomin_v_conjugate_gradient::FominVConjugateGradientAll::DotProduct(const boost::mpi::communicator& world,
const std::vector<double>& a,
const std::vector<double>& b) {
double local_sum = tbb::parallel_reduce(
tbb::blocked_range<size_t>(0, a.size()), 0.0,
[&](const tbb::blocked_range<size_t>& r, double sum) {
for (size_t i = r.begin(); i < r.end(); ++i) {
sum += a[i] * b[i];
}
return sum;
},
std::plus<>());

double global_sum = 0.0;
all_reduce(world, local_sum, global_sum, std::plus<>());
return global_sum;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientAll::MatrixVectorMultiply(
const std::vector<double>& x) const {
std::vector<double> local_result(rows_per_proc_, 0.0);

tbb::parallel_for(tbb::blocked_range<int>(0, rows_per_proc_), [&](const tbb::blocked_range<int>& r) {
for (int i = r.begin(); i < r.end(); ++i) {
double sum = 0.0;
for (int j = 0; j < n; ++j) {
sum += local_a_[(i * n) + j] * x[j];
}
local_result[i] = sum;
}
});

std::vector<double> global_result;
if (world_.rank() == 0) {
global_result.resize(n);
}

std::vector<int> sizes(world_.size());
std::vector<int> displs(world_.size(), 0);
int remainder = n % world_.size();
for (int i = 0; i < world_.size(); ++i) {
sizes[i] = (n / world_.size()) + (i < remainder ? 1 : 0);
if (i > 0) {
displs[i] = displs[i - 1] + sizes[i - 1];
}
}
gatherv(world_, local_result.data(), rows_per_proc_, global_result.data(), sizes, displs, 0);
broadcast(world_, global_result, 0);
return global_result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientAll::VectorAdd(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
for (size_t i = 0; i < a.size(); ++i) {
result[i] = a[i] + b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientAll::VectorSub(const std::vector<double>& a,
const std::vector<double>& b) {
std::vector<double> result(a.size());
for (size_t i = 0; i < a.size(); ++i) {
result[i] = a[i] - b[i];
}
return result;
}

std::vector<double> fomin_v_conjugate_gradient::FominVConjugateGradientAll::VectorScalarMultiply(
const std::vector<double>& v, double scalar) {
std::vector<double> result(v.size());
for (size_t i = 0; i < v.size(); ++i) {
result[i] = v[i] * scalar;
}
return result;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientAll::PreProcessingImpl() {
if (world_.rank() == 0) {
unsigned input_size = task_data->inputs_count[0];
auto* in_ptr = reinterpret_cast<double*>(task_data->inputs[0]);
std::vector<double> input(in_ptr, in_ptr + input_size);

n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
a_ = std::vector<double>(input.begin(), input.begin() + (n * n));
b_ = std::vector<double>(input.begin() + (n * n), input.end());
}

broadcast(world_, n, 0);
broadcast(world_, b_, 0);

int remainder = n % world_.size();
rows_per_proc_ = n / world_.size();
if (world_.rank() < remainder) {
rows_per_proc_++;
}

std::vector<int> counts_a(world_.size());
std::vector<int> counts_b(world_.size());
std::vector<int> displs_a(world_.size(), 0);
std::vector<int> displs_b(world_.size(), 0);

for (int i = 0; i < world_.size(); ++i) {
int rows_i = (n / world_.size()) + (i < remainder ? 1 : 0);
counts_a[i] = rows_i * n;
counts_b[i] = rows_i;

if (i > 0) {
  displs_a[i] = displs_a[i - 1] + counts_a[i - 1];
  displs_b[i] = displs_b[i - 1] + counts_b[i - 1];
}
}

local_a_.resize(rows_per_proc_ * n);
local_b_.resize(rows_per_proc_);

if (world_.rank() == 0) {
boost::mpi::scatterv(world_, a_, counts_a, displs_a, local_a_.data(), (rows_per_proc_ * n), 0);
boost::mpi::scatterv(world_, b_, counts_b, displs_b, local_b_.data(), rows_per_proc_, 0);
} else {
boost::mpi::scatterv(world_, local_a_.data(), (rows_per_proc_ * n), 0);
boost::mpi::scatterv(world_, local_b_.data(), rows_per_proc_, 0);
}

return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientAll::ValidationImpl() {
unsigned int input_size = task_data->inputs_count[0];
const int calculated_n = static_cast<int>((-1.0 + std::sqrt(1 + (4 * input_size))) / 2);
return (static_cast<unsigned int>(calculated_n * (calculated_n + 1)) == input_size) &&
(task_data->outputs_count[0] == static_cast<unsigned int>(calculated_n));
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientAll::RunImpl() {
std::vector<double> x(n, 0.0);
std::vector<double> r = b_;
std::vector<double> p = r;

double rs_old = DotProduct(world_, r, r);

for (int iter = 0; iter < max_iter; ++iter) {
std::vector<double> ap = MatrixVectorMultiply(p);
double p_ap = DotProduct(world_, p, ap);

broadcast(world_, p_ap, 0);
if (std::abs(p_ap) < 1e-12) {
  break;
}
double alpha = rs_old / p_ap;
x = VectorAdd(x, VectorScalarMultiply(p, alpha));
r = VectorSub(r, VectorScalarMultiply(ap, alpha));

double rs_new = DotProduct(world_, r, r);
broadcast(world_, rs_new, 0);
if (std::sqrt(rs_new) < epsilon) {
  break;
}
double beta = rs_new / rs_old;
p = VectorAdd(r, VectorScalarMultiply(p, beta));
rs_old = rs_new;
}

output_ = x;

return true;
}

bool fomin_v_conjugate_gradient::FominVConjugateGradientAll::PostProcessingImpl() {
auto* out_ptr = reinterpret_cast<double*>(task_data->outputs[0]);
for (size_t i = 0; i < output_.size(); ++i) {
out_ptr[i] = output_[i];
}
return true;
}
\end{lstlisting}
\newpage
\begin{lstlisting}[language=C++]
  
\end{lstlisting}
\end{document}