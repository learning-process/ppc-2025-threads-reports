\documentclass[12pt,a4paper]{extarticle}
\renewcommand{\normalsize}{\fontsize{14pt}{16.8pt}\selectfont}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{framed}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}

\geometry{
  a4paper,
  left=30mm,
  right=15mm,
  top=20mm,
  bottom=20mm
}

\titleformat{\section}[block]
  {\normalfont\fontsize{14}{16}\bfseries\centering}
  {\thesection.}{0.5em}{}
\titleformat{\subsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}[block]
  {\normalfont\fontsize{14}{16}\bfseries\filcenter}
  {\thesubsubsection.}{0.5em}{}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    frame=single,
    tabsize=4,
    showstringspaces=false,
    breaklines=true
}

\sloppy

\onehalfspacing

\setlength{\parindent}{1.25cm}

\begin{document}

\begin{titlepage}
\begin{center}

\onehalfspacing

\begin{center}
    \textbf{МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ} \\
    Федеральное государственное автономное образовательное учреждение высшего образования \\
    «Национальный исследовательский Нижегородский университет им. Н.И. Лобачевского» (ННГУ) \\
    Институт информационных технологий, математики и механики
\end{center}

\vspace{4cm}

\begin{center}
    \textbf{ОТЧЕТ} \vspace{0.5cm}\\
    по курсу «ПАРАЛЛЕЛЬНОЕ ПРОГРАММИРОВАНИЕ» \vspace{0.5cm}\\
    Поиск кратчайших путей из одной вершины (алгоритм Дейкстры)
\end{center}

\vspace{4cm}

\begin{flushright}
    \textbf{ВЫПОЛНИЛ:} \\ 
    Студент 3 курса 
    группы 3822Б1ПР4 \\ 
    Трубин А.С. \\

    \vspace{1cm}

    \textbf{ПРОВЕРИЛ:} \\ 
    доцент кафедры ВВСП, к.т.н. \\ 
    Сысоев А.В.
\end{flushright}

\vspace{1cm}

\begin{center}
    Нижний Новгород\\
    2025\newpage
\end{center}

\end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Введение}

Многие задачи в области информационных технологий, логистики и телекоммуникаций требуют
поиска кратчайших путей в графах. Такие графы могут описывать дорожные сети, маршруты
передачи данных, зависимости между задачами и даже структуру социальных сетей.

Среди множества алгоритмов особое место занимает алгоритм Дейкстры. Он основан на жадной
стратегии и гарантирует корректный результат при отсутствии рёбер с отрицательным весом.
Алгоритм поэтапно находит минимальные расстояния от заданной вершины до всех остальных.

С увеличением объёма графов возрастает потребность в ускорении вычислений. Одним из
способов добиться этого является использование параллельных вычислений. Технологии OpenMP,
Intel TBB, стандартная библиотека потоков и MPI позволяют эффективно задействовать
ресурсы многопроцессорных систем.

Цель данной работы — реализовать и сравнить несколько версий алгоритма Дейкстры: от
последовательной до многопоточной, а также провести их экспериментальный анализ.

\section*{Постановка задачи}

В рамках данной работы необходимо:

\begin{itemize}
    \item Реализовать последовательную версию алгоритма Дейкстры на языке C++;
    \item Разработать параллельные версии с использованием:
    \begin{enumerate}
        \item OpenMP;
        \item Intel TBB;
        \item std::thread;
        \item MPI + TBB.
    \end{enumerate}
    \item Подготовить набор тестовых графов различной плотности и размера;
    \item Измерить время выполнения каждой реализации на одинаковых входных данных;
    \item Построить графики ускорения и провести сравнительный анализ производительности;
    \item Сделать выводы о масштабируемости и целесообразности параллельных решений.
\end{itemize}

Результатом работы станет отчёт, содержащий описание алгоритмов, особенности реализаций,
результаты экспериментов и графики производительности.

\section{Описание алгоритма}

Алгоритм Дейкстры предназначен для поиска кратчайших путей от одной вершины до всех остальных
в связном графе с неотрицательными весами рёбер. Он является жадным алгоритмом и работает
по принципу постепенного расширения множества уже обработанных вершин, для которых известно
наилучшее расстояние от начальной точки.

На старте всем вершинам приписывается бесконечное расстояние, за исключением начальной,
для которой оно равно нулю. Далее на каждом шаге выбирается непосещённая вершина с наименьшим
текущим расстоянием. Её соседи проверяются на возможность улучшения маршрута, и при
необходимости их расстояния обновляются.

Для ускорения поиска минимальной вершины используется очередь с приоритетом, обычно реализуемая
на базе минимальной кучи. Это позволяет выполнять выбор за логарифмическое время, сохраняя
общую сложность алгоритма на уровне $O((V + E)\log V)$, где $V$ — число вершин, $E$ — число рёбер.

Пошаговая схема работы алгоритма:

\begin{enumerate}
    \item Инициализировать массив расстояний и очередь.
    \item Добавить стартовую вершину в очередь с расстоянием 0.
    \item Пока очередь не пуста:
    \begin{enumerate}
        \item Извлечь вершину с минимальным расстоянием.
        \item Для всех её соседей проверить, можно ли улучшить путь.
        \item Если новое расстояние меньше текущего, обновить его и занести вершину в очередь.
    \end{enumerate}
    \item После завершения алгоритма массив расстояний содержит кратчайшие пути от стартовой
    вершины до остальных. Если вершина недостижима, расстояние считается равным $-1$.
\end{enumerate}

Данный алгоритм гарантирует корректность результатов только при условии отсутствия рёбер с
отрицательными весами. В противном случае его применение приводит к некорректным значениям
и требуется использовать алгоритмы Беллмана-Форда или Флойда-Уоршелла.

\section{Описание схемы параллельного алгоритма}

Алгоритм Дейкстры традиционно считается сложным для распараллеливания из-за своей
последовательной природы и зависимости от очереди с приоритетом. Однако на практике
существуют подходы, позволяющие адаптировать его для многопоточной обработки.

В данной реализации применяется модифицированный вариант алгоритма — \textit{«delta-stepping»},
который предполагает деление всех возможных расстояний на интервалы фиксированной ширины
$\delta$. Вершины, расстояние до которых попадает в один и тот же интервал, помещаются
в одну «корзину» (bucket) и обрабатываются параллельно.

На каждом шаге из очередной непустой корзины выбирается набор вершин, и для каждой
выполняется проверка и возможное обновление расстояний до соседей. Если расстояние
умещается в текущий диапазон, вершина попадает в следующий «лёгкий» набор. В противном
случае она распределяется по другим корзинам в зависимости от нового расстояния.

Такой подход позволяет обрабатывать несколько вершин одновременно и уменьшает
конфликт доступа к данным, особенно если использовать атомарные операции.

Схема организации параллельного выполнения:

\begin{itemize}
    \item Каждая корзина представляет собой список вершин с расстоянием, попавшим в один диапазон;
    \item Обработка текущей корзины делится между потоками с помощью OpenMP;
    \item Для минимизации гонок используются атомарные переменные при обновлении расстояний;
    \item Вспомогательные структуры — локальные массивы для новых вершин и тяжёлых переходов
    (по весу ребра) — также изолированы по потокам;
    \item После завершения обработки текущей корзины данные из локальных структур объединяются.
\end{itemize}

Подход не только хорошо масштабируется, но и позволяет избежать полной синхронизации при
каждом обновлении расстояния, что является основным узким местом классической реализации.

\section{Описание OpenMP-версии алгоритма}

Параллельная реализация алгоритма Дейкстры с использованием OpenMP основана на стратегии
обработки «корзин» (bucket-based), каждая из которых содержит вершины, расстояния до которых
находятся в определённом диапазоне.

В начале работы все расстояния инициализируются максимальным значением, за исключением
начальной вершины, которой присваивается ноль. Её индекс добавляется в первую корзину.

Алгоритм выполняется в цикле, в котором последовательно выбирается первая непустая корзина.
Для всех вершин внутри неё запускается параллельная обработка соседей. Если при пересчёте
обнаружено более короткое расстояние, сосед добавляется либо в текущую корзину, либо в
следующую, в зависимости от величины веса ребра.

Обработка корзины включает следующие этапы:

\begin{itemize}
    \item Потоки распределяют вершины между собой с помощью директивы \texttt{\#pragma omp for};
    \item Обновления расстояний происходят через атомарные операции с использованием
    \texttt{std::atomic};
    \item Каждому потоку предоставлены локальные буферы для накопления новых вершин,
    что исключает необходимость синхронизации при записи;
    \item После завершения всех потоков локальные данные объединяются и распределяются
    по соответствующим корзинам.
\end{itemize}

Ключевым параметром является значение $\delta$, определяющее ширину диапазонов для
разделения по корзинам. Его настройка влияет на производительность: при слишком маленьком
значении возникает много малых корзин, при слишком большом — теряется точность параллелизма.

Благодаря использованию OpenMP удалось организовать эффективную многопоточную обработку
без явного управления потоками и с минимальными накладными расходами на синхронизацию.

\section{Описание TBB-версии алгоритма}

В данной версии алгоритма Дейкстры использована библиотека Intel oneAPI Threading Building Blocks
(TBB), которая предоставляет высокоуровневые средства для организации параллельных вычислений.
Для реализации выбран подход с итеративным обновлением расстояний по всем вершинам, основанный
на идее «волнового» распространения значений с использованием атомарных операций.

Параллельная обработка построена по следующей схеме:

\begin{itemize}
    \item В начале всем вершинам присваивается значение расстояния, равное бесконечности,
    за исключением начальной вершины;
    \item Для хранения расстояний используется вектор \texttt{std::atomic<int>}, что позволяет
    безопасно обновлять значения в параллельных потоках без дополнительных блокировок;
    \item Основной цикл работает до тех пор, пока в графе продолжают происходить изменения;
    \item На каждой итерации выполняется параллельный проход по всем вершинам с помощью
    \texttt{tbb::parallel\_for};
    \item Для каждой вершины проверяются её соседи. Если найдено более короткое расстояние,
    оно обновляется с помощью \texttt{compare\_exchange\_strong}, а флаг изменений устанавливается в \texttt{true};
    \item После завершения итерации проверяется флаг. Если изменений не было, цикл завершается.
\end{itemize}

Для ограничения числа активных потоков и управления ресурсами используется объект
\texttt{tbb::task\_arena}, который инициализируется значением из функции
\texttt{GetPPCNumThreads()} и управляет количеством рабочих потоков в рамках вызова
\texttt{arena.execute(...)}.

Внутреннее обновление расстояний оформлено в виде функции
\texttt{UpdateDistancesInBlock}, принимающей диапазон вершин, массив расстояний и флаг
изменений. Это позволяет изолировать основную логику перерасчёта от кода управления потоками.

Особенности реализации:

\begin{itemize}
    \item Использование атомарных переменных исключает состояния гонки при обновлении
    расстояний;
    \item Стратегия «до сходимости» позволяет корректно обработать все вершины, несмотря
    на возможные циклы в графе;
    \item Размер блока в \texttt{tbb::blocked\_range} установлен равным 512, что является
    хорошим компромиссом между загрузкой потоков и балансировкой нагрузки;
    \item После завершения обработки результаты копируются в выходной буфер с заменой
    недостижимых вершин на $-1$.
\end{itemize}

Такой подход обеспечивает гибкость и масштабируемость без необходимости явного
управления потоками и блокировками. Он хорошо адаптируется под количество доступных ядер
и позволяет эффективно использовать ресурсы многоядерных процессоров.

\section{Описание STL-версии алгоритма}

В данной реализации алгоритм Дейкстры адаптирован для многопоточной обработки с
использованием стандартной библиотеки потоков C++ (STL threads). Такой подход требует
вручную управлять созданием, синхронизацией и завершением потоков, а также доступом к
общим структурам данных.

Ключевая особенность реализации заключается в параллельной обработке очереди с приоритетом
(минимальной кучи), в которой хранятся вершины, ожидающие обработки. Каждый поток
самостоятельно извлекает вершину с минимальным расстоянием, проверяет возможность улучшения
путей до соседей и, при необходимости, заносит их обратно в очередь.

Для корректной работы в многопоточном режиме используются следующие синхронизационные
механизмы:

\begin{itemize}
    \item \textbf{Очередь с приоритетом (\texttt{priority\_queue})} защищена мьютексом
    \texttt{pq\_mutex}, поскольку её обновление должно быть атомарным;
    \item \textbf{Условная переменная (\texttt{condition\_variable})} используется для уведомления
    потоков о появлении новых вершин в очереди;
    \item \textbf{Атомарные переменные} применяются для хранения расстояний до вершин, что позволяет
    безопасно производить обновления без блокировок;
    \item \textbf{Флаг завершения} (\texttt{terminate\_flag}) сигнализирует о завершении обработки
    всем потокам, когда работа полностью окончена;
    \item \textbf{Счётчик активных потоков} (\texttt{workers\_running}) необходим для корректной
    остановки при отсутствии элементов в очереди и завершении всех активных операций.
\end{itemize}

Работа алгоритма строится следующим образом:

\begin{enumerate}
    \item В начальную очередь помещается стартовая вершина с расстоянием 0;
    \item Запускается пул потоков, каждый из которых обрабатывает вершины из очереди;
    \item Для каждой извлечённой вершины поток проверяет всех её соседей:
    \begin{itemize}
        \item Если найдено более короткое расстояние, оно обновляется через
        \texttt{compare\_exchange\_weak};
        \item Обновлённая вершина добавляется обратно в очередь;
    \end{itemize}
    \item Потоки завершаются, когда очередь пуста, а другие потоки уже не активны.
\end{enumerate}

Данная реализация обеспечивает правильную многопоточную обработку, однако требует
внимательного управления синхронизацией и может иметь больший накладной расход по сравнению
с OpenMP или TBB. Это связано с тем, что очередь с приоритетом не может быть эффективно
разделена между потоками без использования специализированных структур (например,
локальных куч или relaxed-очередей).

Несмотря на это, подход позволяет наглядно продемонстрировать основные принципы
многопоточности в C++: защита общих ресурсов, синхронизация потоков и использование
атомарных операций для обеспечения согласованности данных.

\section{Описание MPI + TBB версии алгоритма}

В данной реализации используется гибридный подход, объединяющий распределённые вычисления
с помощью MPI и многопоточную обработку на каждом процессе с использованием библиотеки
Intel TBB. Это решение позволяет масштабировать алгоритм Дейкстры как на уровне кластеров,
так и в пределах каждого вычислительного узла.

Основная идея состоит в разбиении графа между MPI-процессами по вершинам. Каждый процесс
обрабатывает свою часть графа параллельно с помощью пула потоков TBB, выполняя пересчёт
кратчайших расстояний. После каждой итерации происходит синхронизация полученных значений
через коллективную операцию \texttt{all\_reduce}, чтобы обеспечить глобальное согласие
по расстояниям.

Пошагово алгоритм работает следующим образом:

\begin{enumerate}
    \item Все процессы получают размеры графа и самостоятельно инициализируют
    необходимые структуры;
    \item Расстояния до всех вершин задаются как бесконечные, кроме начальной;
    \item Граф разбивается на блоки по вершинам, и каждому процессу отводится
    соответствующий диапазон;
    \item Внутри каждого процесса запускается параллельный проход по локальному блоку
    с использованием \texttt{tbb::parallel\_for};
    \item При каждом улучшении расстояния оно обновляется через атомарную операцию;
    \item После завершения локальной итерации все процессы обмениваются своими результатами
    с помощью \texttt{boost::mpi::all\_reduce}, выбирая минимальное значение по каждой вершине;
    \item Алгоритм продолжается до тех пор, пока хотя бы один процесс зафиксирует изменения.
\end{enumerate}

Особенности реализации:

\begin{itemize}
    \item Использование \texttt{tbb::task\_arena} позволяет гибко управлять числом потоков;
    \item Атомарные переменные гарантируют корректную обработку без гонок;
    \item Распределение по блокам основано на равномерном делении: каждый процесс
    получает $\left\lceil \frac{n}{p} \right\rceil$ вершин;
    \item После каждой итерации расстояния синхронизируются между процессами,
    что обеспечивает консистентность данных на всех узлах.
\end{itemize}

Для повышения эффективности используется стратегия «до сходимости», при которой
алгоритм продолжается, пока хотя бы в одном процессе происходит изменение расстояний.
Коммуникации между процессами сведены к минимуму — используется только одна глобальная
операция синхронизации на шаг.

\section{Результаты экспериментов}

Для оценки производительности различных реализаций алгоритма Дейкстры были проведены
эксперименты на графе из 200\,000 вершин, каждая из которых имела до 50 случайных рёбер.
Запуск производился с разным числом потоков и процессов, в зависимости от технологии
параллелизации. Замеры времени проводились отдельно для полной цепочки исполнения
(\texttt{PipelineRun}) и для основного ядра вычислений (\texttt{TaskRun}).

Все реализации запускались по 5 раз с усреднением результатов. Далее представлены таблицы
с замерами и относительным ускорением по сравнению с базовой (однопоточной) реализацией
каждой технологии.

\subsection{OpenMP-версия}

\begin{table}[H]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Потоков} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Ускорение} \\ \midrule
1               & 3.97                     & 3.23                 & 1.00               \\
2               & 2.81                     & 1.93                 & 1.67               \\
4               & 1.98                     & 1.14                 & 2.83               \\
6               & 1.66                     & 0.87                 & 3.71               \\
\bottomrule
\end{tabular}
\caption{Результаты для OpenMP}
\end{table}

\subsection{TBB-версия}

\begin{table}[H]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Потоков} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Ускорение} \\ \midrule
1               & 1.72                     & 1.18                 & 1.00               \\
2               & 1.21                     & 0.77                 & 1.53               \\
4               & 1.03                     & 0.54                 & 2.19               \\
6               & 0.94                     & 0.44                 & 2.68               \\
\bottomrule
\end{tabular}
\caption{Результаты для TBB}
\end{table}

\subsection{STL (std::thread)-версия}

\begin{table}[H]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Потоков} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Ускорение} \\ \midrule
1               & 2.67                     & 0.18                 & 1.00               \\
2               & 2.72                     & 0.13                 & 1.38               \\
4               & 2.28                     & 0.10                 & 1.80               \\
6               & 1.78                     & 0.08                 & 2.25               \\
\bottomrule
\end{tabular}
\caption{Результаты для STL-версии}
\end{table}

\subsection{MPI + TBB (гибридная) версия}

\begin{table}[H]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Проц. $\times$ Потоки} & \textbf{PipelineRun (с)} & \textbf{TaskRun (с)} & \textbf{Ускорение} \\ \midrule
1 $\times$ 1                  & 1.108                    & 0.857               & 1.00               \\
1 $\times$ 2                  & 0.964                    & 0.626               & 1.37               \\
1 $\times$ 3                  & 0.970                    & 0.736               & 1.16               \\
2 $\times$ 1                  & 0.967                    & 0.666               & 1.29               \\
2 $\times$ 2                  & 0.936                    & 0.634               & 1.35               \\
2 $\times$ 3                  & 0.907                    & 0.604               & 1.42               \\
\bottomrule
\end{tabular}
\caption{Результаты для гибридной MPI + TBB реализации}
\end{table}

\subsection{Общий вывод}

\begin{itemize}
  \item Наилучший прирост наблюдается при использовании OpenMP и TBB на 4–6 потоках.
  \item STL-реализация показала улучшение, но требует аккуратного управления синхронизацией.
  \item Гибридная MPI + TBB версия даёт стабильный прирост, особенно в конфигурации 2 процесса × 3 потока.
  \item Ускорение ограничивается конкуренцией за общие ресурсы и затратами на синхронизацию (в STL)
  и межпроцессное взаимодействие (в MPI).
  \item Важно отметить, что сам алгоритм во многом является последовательным. Это может вызвать меньшее ускорение.
\end{itemize}

Таким образом, OpenMP и TBB — наиболее эффективные технологии на одной машине,
а гибридная модель MPI + TBB показывает потенциал масштабирования в распределённых системах.


\section{Заключение}

В рамках данной работы была реализована и сравнительно проанализирована серия версий
алгоритма Дейкстры для поиска кратчайших путей из одной вершины на взвешенном
ориентированном графе. Основной целью являлось исследование эффективности различных
подходов к параллельному программированию и оценка масштабируемости при использовании
современных многопоточных и многопроцессорных технологий.

Были рассмотрены следующие технологии:
\begin{itemize}
  \item \textbf{OpenMP} — простой способ распараллеливания с помощью директив;
  \item \textbf{TBB (oneAPI)} — динамическое распределение задач и автоматический
        контроль потоков;
  \item \textbf{std::thread (STL)} — ручное управление потоками без автоматизации;
  \item \textbf{MPI + TBB} — гибридная модель с обменом между процессами и
        многопоточностью внутри.
\end{itemize}

Проведённые замеры показали, что:
\begin{itemize}
  \item \textbf{OpenMP и TBB} дают наибольшее ускорение (до 3.7×) при 6 потоках;
  \item \textbf{STL-реализация} показала прирост, но проигрывает из-за ручной
        синхронизации и блокировок;
  \item \textbf{MPI + TBB} обеспечивает устойчивое ускорение, особенно при 2
        процессах × 3 потока.
\end{itemize}

Работа позволила изучить особенности различных моделей параллельного программирования,
их достоинства и ограничения. Полученные результаты демонстрируют практическую
применимость каждого подхода в зависимости от характера задачи и конфигурации системы.

\section*{Список литературы}

\begin{enumerate}
  \item Chandra R., Dagum L., Kohr D., Maydan D., McDonald J., Menon R.
        \textit{Parallel Programming in OpenMP}. Morgan Kaufmann, 2001.
  \item Reinders J. \textit{Intel Threading Building Blocks: Outfitting C++
        for Multi-core Processor Parallelism}. O'Reilly Media, 2007.
  \item Williams A. \textit{C++ Concurrency in Action: Practical Multithreading}.
        Manning Publications, 2019.
  \item Gropp W., Lusk E., Skjellum A. \textit{Using MPI: Portable Parallel Programming
        with the Message Passing Interface}. MIT Press, 2014.
  \item van der Pas R., Terboven C., Unat D. \textit{Using OpenMP – The Next Step:
        Affinity, Accelerators, Tasking, and SIMD}. MIT Press, 2017.
  \item Butenhof D. R. \textit{Programming with POSIX Threads}. Addison-Wesley, 1997.
  \item Mattson T. G., Sanders B. A., Massingill B. L. \textit{Patterns for Parallel
        Programming}. Addison-Wesley, 2004.
\end{enumerate}


\section{Приложение}

\subsection{Последовательная версия}

\begin{lstlisting}[language=C++, caption={RunImpl для последовательной версии}]
while (!min_heap.empty()) {
  auto [dist_u, u] = min_heap.top();
  min_heap.pop();
  if (dist_u > distances_[u]) continue;

  for (const auto& edge : adjacency_list_[u]) {
    int v = edge.to;
    int new_dist = distances_[u] + edge.weight;
    if (new_dist < distances_[v]) {
      distances_[v] = new_dist;
      min_heap.emplace(new_dist, v);
    }
  }
}
\end{lstlisting}

\subsection{OpenMP-версия}

\begin{lstlisting}[language=C++, caption={Параллельная обработка вершины}]
#pragma omp parallel
{
  int tid = omp_get_thread_num();
  auto& next = local_next[tid];
  auto& heavy = local_heavy[tid];

  #pragma omp for schedule(dynamic)
  for (int i = 0; i < current.size(); ++i) {
    int u = current[i];
    for (const auto& edge : adj[u]) {
      int v = edge.to, w = edge.weight;
      int cand = dist[u] + w;
      if (cand < dist[v]) {
        dist[v] = cand;
        (w <= delta ? next : heavy[cand / delta]).push_back(v);
      }
    }
  }
}
\end{lstlisting}

\subsection{TBB-версия}

\begin{lstlisting}[language=C++, caption={UpdateDistancesInBlock}]
for (size_t u = r.begin(); u < r.end(); ++u) {
  int u_dist = distances_atomic[u];
  if (u_dist == INT_MAX) continue;
  for (const auto& edge : adj[u]) {
    int new_dist = u_dist + edge.weight;
    int old = distances_atomic[edge.to];
    while (new_dist < old) {
      if (distances_atomic[edge.to]
          .compare_exchange_strong(old, new_dist)) {
        changed_flag = true;
        break;
      }
    }
  }
}
\end{lstlisting}

\subsection{STL-версия (std::thread)}

\begin{lstlisting}[language=C++, caption={Поток-работник для std::thread}]
while (!terminate_flag) {
  std::unique_lock lock(pq_mutex);
  if (pq.empty()) { cv.wait(lock); continue; }
  auto [dist_u, u] = pq.top(); pq.pop();

  for (const auto& edge : adj[u]) {
    int v = edge.to;
    int cand = dist_u + edge.weight;
    int old = distances[v];
    if (cand < old && distances[v].compare_exchange_weak(old, cand)) {
      pq.emplace(cand, v);
      cv.notify_one();
    }
  }
}
\end{lstlisting}

\subsection{MPI + TBB (гибридная версия)}

\begin{lstlisting}[language=C++, caption={SyncGlobalDistances}]
for (size_t i = 0; i < N; ++i)
  snapshot[i] = distances_atomic[i];
boost::mpi::all_reduce(world, snapshot.data(), N,
  result.data(), boost::mpi::minimum<int>());
for (size_t i = 0; i < N; ++i)
  distances_atomic[i] = result[i];
\end{lstlisting}

\end{document}